{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Environment(ABC):  \n",
    "\n",
    "  class State:\n",
    "      pass\n",
    "  InitialState: State \n",
    "  CurrentState: State \n",
    "\n",
    "  @abstractmethod\n",
    "  def TransitionModel(self, State: State, Action)-> State:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def RewardModel(self, State: State, Action, NextState: State, TerminalSignal: bool)-> float:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def StateTransition(self, State: State, Action)-> tuple[float, State, bool]:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def SampleTrajectory(self, RunDuration: float)-> list[State]:\n",
    "      ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "  AgentEnvironment: Environment\n",
    "\n",
    "  @abstractmethod\n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Observe(self)-> T.Tensor:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Learn(self):\n",
    "      'Improves  the agent by updating its models'\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def LearningAlgorithm(self):\n",
    "      ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__() \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims+n_actions, fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = T.cat([state, action], dim=-1)\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims , fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, n_actions))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = T.relu(self.bn1(self.fc1(state)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only= True)\n",
    "class DDPGAgent(Agent):\n",
    "  AgentEnvironment: Environment= NotImplemented\n",
    "  Actor: ActorNetwork = NotImplemented\n",
    "  Critic: CriticNetwork = NotImplemented\n",
    "  TargetActor: ActorNetwork = NotImplemented\n",
    "  TargetCritic: CriticNetwork= NotImplemented\n",
    "  ReplayBuffer: deque = NotImplemented\n",
    "  BatchSize: int = 64\n",
    "  SoftUpdateRate: float= 0.1\n",
    "  Noise= NotImplemented\n",
    "  DiscountRate: float = 0.67 \n",
    "  def __post_init__(self):\n",
    "    assert isinstance(self.AgentEnvironment, Environment), \"Must be an instance of Environment\"\n",
    "    assert isinstance(self.Actor, ActorNetwork), \"Must be an ActorNetwork\"\n",
    "    assert isinstance(self.Critic, CriticNetwork), \"Must be a CriticNetwork\"\n",
    "    assert isinstance(self.TargetActor, ActorNetwork), \"Must be an ActorNetwork\"\n",
    "    assert isinstance(self.TargetCritic, CriticNetwork), \"Must be a CriticNetwork\"\n",
    "    assert isinstance(self.ReplayBuffer, deque), \"Must be a deque\"\n",
    "    if self.Noise == NotImplemented: raise NotImplementedError(\"Must define AgentEnvironment\") \n",
    "\n",
    "  def Observe(self, State)-> T.Tensor:  \n",
    "    if isinstance(State, (tuple, list)):\n",
    "        Observation= [] \n",
    "        for i in State:\n",
    "          Observation.append(self.Observe(i)) \n",
    "        Observation= T.stack(Observation)\n",
    "    elif isinstance(State, self.AgentEnvironment.State):\n",
    "        Observation= T.cat([State.Position, State.Momentum])\n",
    "    return Observation\n",
    "  \n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "    self.Actor.eval()\n",
    "    Observation = T.tensor(Observation, dtype=T.float).to(self.Actor.device)\n",
    "    Action = self.Actor.forward(Observation).to(self.Actor.device)\n",
    "    NoisyAction = 1e-7* (Action + T.tensor(self.noise(), dtype=T.float).to(self.Actor.device))\n",
    "    self.Actor.train()\n",
    "    return NoisyAction.cpu().detach()\n",
    "\n",
    "  def Learn(self):\n",
    "  # Updates target network with online model parameters\n",
    "    if len(self.ReplayBuffer) < self.BatchSize:\n",
    "        return\n",
    "\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    states, actions, next_states, rewards, dones = zip(*batch)\n",
    "\n",
    "    state = T.tensor(states, dtype=T.float).to(self.Critic.device)\n",
    "    action = T.stack(actions).to(self.Critic.device)\n",
    "    reward = T.tensor(rewards, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    new_state =  T.tensor(next_states, dtype=T.float).to(self.Critic.device)\n",
    "    done = T.tensor(dones, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    \n",
    "    self.TargetActor.eval()\n",
    "    self.TargetCritic.eval()\n",
    "    self.Critic.eval()\n",
    "    \n",
    "    target_actions = self.TargetActor.forward(new_state)\n",
    "    Critic_value_ = self.TargetCritic.forward(new_state, target_actions) \n",
    "    q_expected = self.Critic.forward(state, action)\n",
    "    q_targets = reward + self.DiscountRate * Critic_value_ * (1 - done)\n",
    "\n",
    "    Critic_loss = nn.MSELoss()(q_expected, q_targets.detach())\n",
    "    self.Critic.train()\n",
    "    self.Critic.optimizer.zero_grad()\n",
    "    Critic_loss.backward()\n",
    "    self.Critic.optimizer.step()\n",
    "\n",
    "    self.Actor.eval()\n",
    "    self.Critic.eval()\n",
    "\n",
    "    mu = self.Actor.forward(state)\n",
    "    Actor_loss = -self.Critic.forward(state, mu)\n",
    "\n",
    "    Actor_loss = T.mean(Actor_loss)\n",
    "    self.Actor.train()\n",
    "    self.Actor.optimizer.zero_grad()\n",
    "    Actor_loss.backward()\n",
    "    self.Actor.optimizer.step()\n",
    "\n",
    "    self.update_network_parameters()\n",
    "  def RewardModel(self)-> float:\n",
    "      ...\n",
    "  def LearningAlgorithm(self):\n",
    "      ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
