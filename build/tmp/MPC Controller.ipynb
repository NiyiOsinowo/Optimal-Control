{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Environment(ABC):  \n",
    "\n",
    "  class State:\n",
    "      pass\n",
    "  InitialState: State \n",
    "  CurrentState: State \n",
    "\n",
    "  @abstractmethod\n",
    "  def TransitionModel(self, State: State, Action)-> State:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def RewardModel(self, State: State, Action, NextState: State, TerminalSignal: bool)-> float:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def StateTransition(self, State: State, Action)-> tuple[float, State, bool]:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def SampleTrajectory(self, RunDuration: float, Policy: Optional[Callable])-> list[State]:\n",
    "      ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MPController(ABC):\n",
    "  EnvironmentModel: Environment\n",
    "  InternalModel: Callable\n",
    "  Policy: Callable\n",
    "\n",
    "  @abstractmethod\n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "      'Produces an action based on the observation of the current state of the environment'\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Plan(self)-> tuple[list[EnvironmentModel.State], list[T.Tensor]]:\n",
    "      'Produces an sequence of actions and predicted states based on the observation of the current state of the environment'\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Observe(self)-> T.Tensor:\n",
    "      'Produces a vector encoding the observable information of the observation of the current state of the environment'\n",
    "      ... \n",
    "  @abstractmethod\n",
    "  def Learn(self):\n",
    "      'Improves the agent by updating its models'\n",
    "      ...  \n",
    "  @abstractmethod\n",
    "  def LearningAlgorithm(self):\n",
    "      ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
