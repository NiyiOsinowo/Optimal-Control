{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This module contains a detailed implementation of the backpropagation algorithm.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReLU:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.relu(x)\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "@dataclass\n",
    "class Tanh:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "@dataclass\n",
    "class Null:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.ones_like(x)\n",
    "def mse_grad(x, y):\n",
    "    return torch.mean(-2*torch.mean(y-x))\n",
    "def mse(x, y):\n",
    "    return torch.mean(torch.mean(((x-y)**2)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(8, 10)\n",
    "        self.hidden_layer = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        x = self.hidden_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork:\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accuately predict the expected value given a state-action pair.'''\n",
    "    def __init__(self, layer_sizes: list, \n",
    "                 layer_activations: list, \n",
    "                 layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [torch.randint(-10000, 10000, [layer_sizes[x], layer_sizes[x+1]])/10000  for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [torch.randint(-10000, 10000, [1, layer_sizes[x+1]])/10000 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "    def forward(self, StateInput: torch.Tensor, \n",
    "                ActionInput: torch.Tensor, \n",
    "                full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network'\n",
    "        InputData = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        LayerConnections=  [0]*len(self.weights)\n",
    "        ActivatedNeuronLayer= [InputData]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections[i]= torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]\n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return ActivatedNeuronLayer[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, \n",
    "                          ActionInput: torch.Tensor, \n",
    "                          OptimalReturn: torch.Tensor, \n",
    "                          loss_derivative: Callable):\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss functio, input data and target data'''\n",
    "        bias_grad = [torch.zeros_like(b) for b in self.bias]\n",
    "        weight_grad = [torch.zeros_like(w) for w in self.weights]\n",
    "        for x1, x2, y in zip(StateInput, ActionInput, OptimalReturn):\n",
    "            dEdb = [0]*len(self.bias)\n",
    "            dEdw = [0]*len(self.weights)\n",
    "            LayerConnections, ActivatedNeuronLayer= self.forward(x1, x2, full= True)\n",
    "            dEdA= torch.tensor([[loss_derivative(ActivatedNeuronLayer[-1], y)]])\n",
    "            if ActivatedNeuronLayer[0].ndim < 2:\n",
    "                ActivatedNeuronLayer[0]= ActivatedNeuronLayer[0].unsqueeze(dim=0)\n",
    "            for l in range(len(self.weights)):\n",
    "                z = LayerConnections[-l-1]     \n",
    "                dAdz= self.layer_activations_derivative[-l-1](z)\n",
    "                dEdz = torch.mul(dEdA, dAdz)\n",
    "                dzdw= ActivatedNeuronLayer[-l-2].t()\n",
    "                dEdb[-l-1] = dEdz\n",
    "                dEdw[-l-1] = torch.mul(dzdw, dEdz)\n",
    "                dzdA= self.weights[-l-1].t()\n",
    "                dEdA= torch.matmul(dEdz, dzdA)\n",
    "            bias_grad = [nb+dnb/len(StateInput) for nb, dnb in zip(bias_grad, dEdb)]\n",
    "            weight_grad = [nw+dnw/len(StateInput) for nw, dnw in zip(weight_grad, dEdw)]\n",
    "        return weight_grad, bias_grad\n",
    "    def update_model(self, weight_grad, bias_grad, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pytorch_gradients(input_data, target_data, model, loss_function):\n",
    "    model.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = loss_function(output, target_data)\n",
    "    loss.backward()\n",
    "    gradients = []\n",
    "    for param in model.parameters():\n",
    "        gradients.append(param.grad.clone())\n",
    "    for i, grad in enumerate(gradients):\n",
    "        print(f\"Gradient for parameter {i + 1}:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyCritic = NeuralNetwork()\n",
    "model = CriticNetwork([8, 10, 1], [torch.relu, f], [relu_derivative, f_grad])\n",
    "model.weights[0]= pyCritic.input_layer.weight.t()\n",
    "model.weights[1]= pyCritic.hidden_layer.weight.t()\n",
    "model.bias[0]= pyCritic.input_layer.bias.t()\n",
    "model.bias[1]= pyCritic.hidden_layer.bias.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 6)\n",
    "inputs1 = torch.randn(1, 2)\n",
    "input_data = torch.cat([inputs, inputs1], dim=1)\n",
    "target_data = torch.rand(1, 1)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for parameter 1:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.2823, -0.0956, -0.0077, -0.2742, -0.0097,  0.1919, -0.1223, -0.1113],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1426,  0.0483,  0.0039,  0.1385,  0.0049, -0.0970,  0.0618,  0.0562],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.2031,  0.0688,  0.0056,  0.1973,  0.0070, -0.1381,  0.0880,  0.0801]])\n",
      "Gradient for parameter 2:\n",
      "tensor([ 0.0000,  0.2280,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1152,\n",
      "         0.0000, -0.1641])\n",
      "Gradient for parameter 3:\n",
      "tensor([[ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892]])\n",
      "Gradient for parameter 4:\n",
      "tensor([-1.8363])\n",
      "[tensor([[ 0.0000, -0.2823,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1426,\n",
      "          0.0000,  0.2031],\n",
      "        [ 0.0000, -0.0956,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0483,\n",
      "          0.0000,  0.0688],\n",
      "        [ 0.0000, -0.0077,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0039,\n",
      "          0.0000,  0.0056],\n",
      "        [ 0.0000, -0.2742,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1385,\n",
      "          0.0000,  0.1973],\n",
      "        [ 0.0000, -0.0097,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0049,\n",
      "          0.0000,  0.0070],\n",
      "        [ 0.0000,  0.1919,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0970,\n",
      "          0.0000, -0.1381],\n",
      "        [ 0.0000, -0.1223,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0618,\n",
      "          0.0000,  0.0880],\n",
      "        [ 0.0000, -0.1113,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0562,\n",
      "          0.0000,  0.0801]], grad_fn=<AddBackward0>), tensor([[ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892],\n",
      "        [ 0.0000, -1.7567,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.2252,\n",
      "          0.0000, -1.6892]], grad_fn=<AddBackward0>)]\n",
      "[tensor([[ 0.0000,  0.2280,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1152,\n",
      "          0.0000, -0.1641]], grad_fn=<AddBackward0>), tensor([[-1.8363]])]\n"
     ]
    }
   ],
   "source": [
    "gradients = compute_pytorch_gradients(input_data, target_data, pyCritic, loss_function)\n",
    "a, b = model.compute_gradients(inputs, inputs1, target_data, mse_grad)\n",
    "print(a)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementLearningPractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
