{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch \n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def tanh_derivative(x: torch.Tensor|int|float)-> torch.Tensor|int|float:\n",
    "    return 1-x**2\n",
    "def neg_relu(x):\n",
    "    return min(0, x)\n",
    "def f(x):\n",
    "    return x\n",
    "def f_grad(x):\n",
    "    return torch.ones_like(x)\n",
    "def relu_derivative(tensor):\n",
    "    return torch.where(tensor > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "def mse_grad(x, y):\n",
    "    return (-2* (x-y))/len(x)\n",
    "\n",
    "@dataclass\n",
    "class Source(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Position: torch.Tensor # m\n",
    "    Charge: float #C\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def ElectricField(FieldSources: list, ObservationPosition: torch.Tensor)->torch.Tensor:\n",
    "    'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    for FieldSource in FieldSources:\n",
    "        if type(FieldSource) != Source:\n",
    "            raise TypeError(\"The input is not valid\")\n",
    "    if type(ObservationPosition[0]) != type(ObservationPosition[1]):\n",
    "         raise TypeError(\"Incompatible Reference point data types\")\n",
    "    elif type(ObservationPosition[0]) != torch.Tensor:\n",
    "        raise TypeError(\"Invalid Reference point data type\")\n",
    "    elif ObservationPosition[0].size()!=ObservationPosition[1].size():\n",
    "        raise TypeError(\"Incompatible Reference point dimensions\")\n",
    "    else: \n",
    "        ElectricFieldVector = torch.zeros_like(ObservationPosition)\n",
    "    for FieldSource in FieldSources:\n",
    "        PositionMatrices= torch.stack([torch.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                        torch.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "        DisplacemnetVector = ObservationPosition - PositionMatrices\n",
    "        DisplacementMagnitude = torch.sqrt(DisplacemnetVector[0]**2 +DisplacemnetVector[1]**2)  # Magnitude of the displacement vector\n",
    "        ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacemnetVector\n",
    "    return ElectricFieldVector #N/C or V/m\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def PlotField(Sources: list, ObservationPosition: torch.Tensor):\n",
    "    'This funtion plots the 2D electric vector field'\n",
    "    xd, yd = ElectricField(Sources, ObservationPosition)\n",
    "    xd = xd / torch.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / torch.sqrt(xd**2 + yd**2)\n",
    "    color_aara = torch.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n",
    "\n",
    "@dataclass \n",
    "class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "       These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: torch.Tensor # m\n",
    "    FieldStrength: torch.Tensor #N/C or V/m\n",
    "    Momentum: torch.Tensor #kg*m/s\n",
    "    def Unwrap(self)->torch.Tensor:\n",
    "        '''This function converts the state parameters to a single tensor for processing. '''\n",
    "        return torch.cat([self.Position, \n",
    "                          self.FieldStrength,\n",
    "                          self.Momentum])\n",
    "\n",
    "class CriticNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accuately predict the expected value given a state-action pair.'''\n",
    "    def __init__(self, layer_sizes: list, layer_activations: list, layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [2 * torch.rand(layer_sizes[x], layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [2 * torch.rand(1, layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "        self.layer_activations_derivative= layer_activations_derivative\n",
    "    def forward(self, StateInput: torch.Tensor, ActionInput: torch.Tensor, full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network'\n",
    "        InputData = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        LayerConnections= []\n",
    "        ActivatedNeuronLayer= [InputData]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections.append(torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]) \n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return ActivatedNeuronLayer[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, ActionInput: torch.Tensor, OptimalReturn: torch.Tensor, loss_derivative: Callable):\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss function and '''\n",
    "        BiasGradient = [torch.zeros_like(b) for b in self.bias]\n",
    "        WeightGradient = [torch.zeros_like(w) for w in self.weights]\n",
    "        zs , ActivatedNeuronLayer  = self.forward(StateInput, ActionInput, full= True)\n",
    "        LayerError = loss_derivative(ActivatedNeuronLayer[-1], OptimalReturn) * self.layer_activations_derivative[-1](zs[-1])\n",
    "        BiasGradient[-1] = LayerError\n",
    "        WeightGradient[-1] = torch.matmul(ActivatedNeuronLayer[-2].t(), LayerError)\n",
    "        if ActivatedNeuronLayer[0].ndim < 2:\n",
    "            ActivatedNeuronLayer[0]= ActivatedNeuronLayer[0].unsqueeze(dim=0)\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            LayerError = torch.matmul(LayerError, self.weights[-l+1].t()) * self.layer_activations_derivative[-l](z)\n",
    "            BiasGradient[-l] = LayerError\n",
    "            WeightGradient[-l] = torch.matmul(ActivatedNeuronLayer[-l-1].t(), LayerError)\n",
    "        return  WeightGradient, BiasGradient\n",
    "    def update_model(self, weight_grad: list, bias_grad: list, learning_rate: float):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n",
    "\n",
    "class ActorNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Policy Function(Actor) used to predict the best action to take at any given a state.\n",
    "       This policy function is a neural network that will learn to predict actions that lead to better rewards.'''\n",
    "    def __init__(self, layer_sizes: list, layer_activations: list, layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [2 * torch.rand(layer_sizes[x], layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [2 * torch.rand(1, layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "        self.layer_activations_derivative= layer_activations_derivative\n",
    "    def forward(self, StateInput: torch.Tensor, full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters to outputs Action parameters(Force applied on the x and y axis by the agent/controller) predicted by the Main actor network'\n",
    "        LayerConnections= []\n",
    "        ActivatedNeuronLayer= [StateInput]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections.append(torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]) \n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return torch.squeeze(ActivatedNeuronLayer[-1])\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateBatch: torch.Tensor, CriticModel: CriticNetwork)-> torch.Tensor:\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss function, a batch of inputs and targets'''\n",
    "        BiasGradient = [torch.zeros_like(b) for b in self.bias]\n",
    "        WeightGradient = [torch.zeros_like(w) for w in self.weights]\n",
    "        zs , ActivatedNeuronLayer  = self.forward(StateBatch, full= True)\n",
    "        ActorLoss = -torch.mean(CriticModel.forward(StateBatch, ActivatedNeuronLayer[-1].squeeze()))\n",
    "        LayerError = ActorLoss * self.layer_activations_derivative[-1](zs[-1])\n",
    "        BiasGradient[-1] = LayerError\n",
    "        WeightGradient[-1] = torch.matmul(ActivatedNeuronLayer[-2].t(), LayerError)\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            sp = self.layer_activations_derivative[-l](z)\n",
    "            LayerError = torch.matmul(LayerError, self.weights[-l+1].t()) * sp\n",
    "            BiasGradient[-l] = LayerError\n",
    "            WeightGradient[-l] = torch.matmul((ActivatedNeuronLayer[-l-1].unsqueeze(dim=1)), LayerError)\n",
    "        return  WeightGradient, BiasGradient\n",
    "    def update_model(self, weight_grad, bias_grad, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n",
    "\n",
    "@dataclass \n",
    "class ReplayBuffer(EnforceClassTyping):\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    @EnforceMethodTyping\n",
    "    def AddExperience(self, State: State, Action: torch.Tensor, NextState: State, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "    @EnforceMethodTyping\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [torch.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [torch.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= torch.stack(SampledStates)\n",
    "            ActionBatch= torch.stack(SampledActions)\n",
    "            NextStateBatch= torch.stack(SampledNextStates)\n",
    "            RewardsBatch= torch.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= torch.stack(SampledTerminalSignals)\n",
    "        else:\n",
    "            raise ValueError('BatchSize too big')\n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch\n",
    "\n",
    "@dataclass\n",
    "class Agent(EnforceClassTyping):\n",
    "    '''This class represents the agent which will interact with the environment to create state state transitions which it will use to learn a good policy and value function.\n",
    "\n",
    "    The Mass and Charge parameters deteremine how the interacts with its environment.\n",
    "    The LearningRate, LossFunction, HiddenLayerSize, and MemorySize parameters determine its learning behaviour.'''\n",
    "    Charge: float\n",
    "    Mass: float\n",
    "    LearningRate: float\n",
    "    MemorySize: int\n",
    "    ActorHiddenLayerSize: list\n",
    "    ActorLayerActivations: list\n",
    "    ActorLayerActivationDerivatives: list\n",
    "    CriticHiddenLayerSize: list\n",
    "    CriticLayerActivations: list\n",
    "    CriticLayerActivationDerivatives: list\n",
    "    CurrentState: State \n",
    "    Memory: ReplayBuffer = field(init=False) \n",
    "    ActorModel: ActorNetwork = field(init=False)\n",
    "    CriticModel: CriticNetwork = field(init=False)\n",
    "    ActorTargetModel: ActorNetwork = field(init=False)\n",
    "    CriticTargetModel: CriticNetwork = field(init=False)\n",
    "    def __post_init__(self):\n",
    "        self.Memory= ReplayBuffer(self.MemorySize)\n",
    "        self.ActorModel= ActorNetwork(self.ActorHiddenLayerSize, self.ActorLayerActivations, self.ActorLayerActivationDerivatives)\n",
    "        self.ActorTargetModel= self.ActorModel\n",
    "        self.CriticModel= CriticNetwork(self.CriticHiddenLayerSize, self.CriticLayerActivations, self.CriticLayerActivationDerivatives)\n",
    "        self.CriticTargetModel= self.CriticModel\n",
    "    def ForceGenerator(self, Action: torch.Tensor)-> torch.Tensor:\n",
    "        ForceVector= Action* 20\n",
    "        return ForceVector\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateCritic(self, StateBatch: torch.Tensor, ActionBatch: torch.Tensor, NextStateBatch: torch.Tensor, RewardBatch: torch.Tensor, TerminalSignalsbatch: torch.Tensor, DiscountRate: float):\n",
    "        'Updates the main critic network parameters by minimizing the difference between the bellman optimal expected return and the expected return predicted by the main critic network'\n",
    "        NextAction= self.ActorTargetModel.forward(NextStateBatch)\n",
    "        BellmanOptimalReturn= RewardBatch+ (1-TerminalSignalsbatch)*DiscountRate*self.CriticTargetModel.forward(NextStateBatch, NextAction)\n",
    "        for i in range(len(StateBatch)):\n",
    "            WeightGradient, BiasGradient= self.CriticModel.compute_gradients(StateBatch[i], ActionBatch[i], BellmanOptimalReturn[i], mse_grad)\n",
    "            self.CriticModel.update_model(WeightGradient, BiasGradient, self.LearningRate)\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateActor(self, StateBatch: torch.Tensor):\n",
    "        'Updates the main actor network parameters by maximizing the Expected Q-value predicted by the main critic network'\n",
    "        for i in range(len(StateBatch)):\n",
    "            WeightGradient, BiasGradient= self.ActorModel.compute_gradients(StateBatch[i], self.CriticModel)\n",
    "            self.ActorModel.update_model(WeightGradient, BiasGradient, self.LearningRate)\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateTargetCritic(self, SoftUpdateRate: float):\n",
    "        'Updates the target critic network parameters by making in it lag behind the main critic network updates'\n",
    "        for i in range(len(self.CriticTargetModel.weights)):\n",
    "            self.CriticTargetModel.weights[i]= self.CriticModel.weights[i] * SoftUpdateRate + self.CriticTargetModel.weights[i] * (1.0 - SoftUpdateRate)\n",
    "    @EnforceMethodTyping \n",
    "    def UpdateTargetActor(self, SoftUpdateRate: float):\n",
    "        'Updates the target actor network parameters by making in it lag behind the main actor network updates'\n",
    "        for i in range(len(self.ActorTargetModel.weights)):\n",
    "            self.ActorTargetModel.weights[i]= self.ActorModel.weights[i] * SoftUpdateRate + self.ActorTargetModel.weights[i] * (1.0 - SoftUpdateRate)\n",
    "\n",
    "@dataclass\n",
    "class Environment(EnforceClassTyping):\n",
    "    '''This class represents the environment(i.e. the Space and Physics) the agent will learn from. \n",
    "    \n",
    "    The UppperBoundX, LowerBoundX, UpperBoundY, and LowerBoundY determine the dimensions of the viable learning region of the environment.\n",
    "    The FieldType determines the physics/dynamics of the environment\n",
    "    The FieldSources shape the field '''\n",
    "    UppperBoundX: float\n",
    "    LowerBoundX: float\n",
    "    UpperBoundY: float\n",
    "    LowerBoundY: float\n",
    "    FieldSources: list\n",
    "    FieldType: Callable #[[list, torch.Tensor], torch.Tensor]\n",
    "    def KineticEnergy(self, Mass: float, Velocity: float)-> float:\n",
    "        return 0.5* Mass* Velocity**2\n",
    "    @EnforceMethodTyping\n",
    "    def ForceFieldStrength(self, Position: torch.Tensor)-> torch.Tensor:\n",
    "        '''This method determines the field strength at any given position based the field type and field sources'''\n",
    "        FieldStrengthVector = self.FieldType(self.FieldSources, Position)\n",
    "        return FieldStrengthVector\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: torch.Tensor, FinalPosition: torch.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ForceFieldStrength(torch.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def TransitionModel(self, LearningAgent: Agent, CurrentState: State, Action: torch.Tensor, TimeStep:float)-> State:\n",
    "        '''This function determines how the state of the agent changes after a given period given the agents state and parameters'''\n",
    "        InitialVelocity= CurrentState.Momentum/LearningAgent.Mass\n",
    "        Acceleration= (Action- CurrentState.FieldStrength*LearningAgent.Charge)/LearningAgent.Mass\n",
    "        FinalVelocity= InitialVelocity+ Acceleration*TimeStep\n",
    "        NewPosition= InitialVelocity*TimeStep- (Acceleration*TimeStep**2)/2\n",
    "        NewFieldForce= self.ForceFieldStrength(NewPosition)\n",
    "        ResultantMomemntum= FinalVelocity*LearningAgent.Mass\n",
    "        NewState= State(NewPosition, NewFieldForce, ResultantMomemntum)\n",
    "        return NewState\n",
    "    @EnforceMethodTyping\n",
    "    def RewardModel(self, CurrentState: State, Action: torch.Tensor, NextState: State, Target: torch.Tensor, TerminalSignal: bool, DistanceSignificance: float, EnergySignificance: float, TerminalSignalSignificance: float, Resolution: int= 5000)-> float:\n",
    "        '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "        DistanceGainedFromTarget= torch.norm(CurrentState.Position-Target)- torch.norm(NextState.Position-Target) \n",
    "        EnergyConsumed= self.WorkDoneAgainstField(CurrentState.Position, NextState.Position, Resolution)\n",
    "        Cost= DistanceSignificance* DistanceGainedFromTarget+ EnergySignificance* EnergyConsumed+ TerminalSignalSignificance* TerminalSignal\n",
    "        return -Cost.item()\n",
    "    @EnforceMethodTyping\n",
    "    def IsTerminalCondition(self, Position: torch.Tensor)-> bool:\n",
    "        '''This method determines if a position is within the viable learning region of the environment'''\n",
    "        if self.LowerBoundX <= Position[0] <= self.UppperBoundX or self.LowerBoundY <= Position[1] <= self.UpperBoundY:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    @EnforceMethodTyping\n",
    "    def RandomState(self)->State:\n",
    "        '''This method generates a random state within the viable learning region'''\n",
    "        RandomPosition= torch.Tensor([random.uniform(self.LowerBoundX, self.UppperBoundX), random.uniform(self.LowerBoundY, self.UpperBoundY)])\n",
    "        RandomFieldStrength= self.ForceFieldStrength(RandomPosition)\n",
    "        RandomMomentum= torch.squeeze(torch.rand((1, 2)))\n",
    "        return State(RandomPosition, RandomFieldStrength, RandomMomentum)\n",
    "\n",
    "@dataclass\n",
    "class DDPG(EnforceClassTyping):\n",
    "    LearningAgent: Agent\n",
    "    AgentEnvironment: Environment\n",
    "    Target: torch.Tensor\n",
    "    NumberOfEpisodes: int\n",
    "    EpisodeDuration: int\n",
    "    BatchSize: int\n",
    "    SoftUpdateRate: float\n",
    "    LearningRate: float\n",
    "    DiscountRate: float\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    gamma: float\n",
    "    TimeStep: float\n",
    "    def __post_init__(self):\n",
    "        if self.TimeStep< 0:\n",
    "            raise ValueError('Time step cant be Negative')\n",
    "    def CreateExperience(self):\n",
    "        AgentAction= self.LearningAgent.ActorModel.forward(self.LearningAgent.CurrentState.Unwrap())\n",
    "        NewState= self.AgentEnvironment.TransitionModel(self.LearningAgent, \n",
    "                                                      self.LearningAgent.CurrentState, \n",
    "                                                      AgentAction, \n",
    "                                                      self.TimeStep)\n",
    "        TerminalSignal= self.AgentEnvironment.IsTerminalCondition(NewState.Position)\n",
    "        Reward= self.AgentEnvironment.RewardModel(self.LearningAgent.CurrentState,\n",
    "                                                    AgentAction,\n",
    "                                                    NewState, \n",
    "                                                    self.Target, \n",
    "                                                    TerminalSignal, \n",
    "                                                    self.alpha,\n",
    "                                                    self.beta, \n",
    "                                                    self.gamma) \n",
    "        return self.LearningAgent.CurrentState, AgentAction, NewState, Reward, TerminalSignal\n",
    "    def ActionNoiseGenerator(self, Action: torch.Tensor, theta:float= 0.5, Mean: float= 0)-> torch.Tensor:\n",
    "        OUNoise= -theta*Action+ Mean*torch.rand_like(Action)#np.random.randn\n",
    "        NoisyAction= Action+ OUNoise*self.TimeStep\n",
    "        return NoisyAction\n",
    "    def TrainModel(self):\n",
    "        '''This method runs the DDPG algorithm by letting it learn from the environment over the episodes'''\n",
    "        StatePath= []\n",
    "        for _ in range(self.EpisodeDuration):\n",
    "            StatePath.append(self.LearningAgent.CurrentState)\n",
    "            NoAction= torch.Tensor([[0.0, 0.0]])\n",
    "            NewState = self.AgentEnvironment.TransitionModel(self.LearningAgent, self.LearningAgent.CurrentState, NoAction, self.TimeStep)   \n",
    "            self.LearningAgent.CurrentState = NewState\n",
    "\n",
    "# Charge1= Source(torch.tensor([-1, 0]), -1e-9)\n",
    "# Charge2= Source(torch.tensor([1, 0]), 1e-9)\n",
    "# ChargeSources= [Charge1, Charge2]\n",
    "# TestEnvironment= Environment(25.0, \n",
    "#                              -25.0, \n",
    "#                              25.0, \n",
    "#                              -25.0, \n",
    "#                              ChargeSources, \n",
    "#                              ElectricField)\n",
    "# TestState= TestEnvironment.RandomState()\n",
    "# TestNextState= TestEnvironment.RandomState()\n",
    "# TestActor= ActorNetwork([6, 10, 2], [torch.tanh, torch.tanh], [tanh_derivative, tanh_derivative])\n",
    "# TestCritic= CriticNetwork([8, 10, 1], [torch.relu, f], [relu_derivative, f_grad])\n",
    "# TestAgent= Agent(2.0, 2.0, 0.2, 64, \n",
    "#                  [6, 10, 2], [torch.tanh, torch.tanh], [tanh_derivative, tanh_derivative], \n",
    "#                  [8, 15, 10, 5, 1], [torch.relu, torch.relu, torch.relu, f], [relu_derivative, relu_derivative, relu_derivative, f_grad], \n",
    "#                  TestState)\n",
    "# TestDDPG= DDPG(TestAgent, \n",
    "#                TestEnvironment, \n",
    "#                torch.Tensor([-10, 10]),\n",
    "#                 10, \n",
    "#                 20, \n",
    "#                 10, \n",
    "#                 0.2,\n",
    "#                 0.01,                                                         \n",
    "#                 0.1, \n",
    "#                 10.5, \n",
    "#                 10.2, \n",
    "#                 -100.0,\n",
    "#                 0.5)\n",
    "# TestDDPG.TrainModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shoul obey newtons laws in Homogenous vector field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
