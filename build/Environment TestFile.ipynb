{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple\n",
    "T.Tensor.ndim = property(lambda self: len(self.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ornstein Uhlenbeck Noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
    "                                                            self.mu, self.sigma)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Particle(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Mass: float # kg\n",
    "    Charge: float #C\n",
    "    Position: T.Tensor # m\n",
    "    Velocity: T.Tensor # m/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particle Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electron= Particle(Mass=9.11e-8, Charge= -1.6e-9, Position=T.tensor([1.0, 0.0]), Velocity=T.tensor([0.0, 0.0]))\n",
    "Proton= Particle(Mass=9.11e-8, Charge= 1.6e-9, Position=T.tensor([-1.0, 0.0]), Velocity=T.tensor([0.0, 0.0]))\n",
    "Source= [Electron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Field:\n",
    "    Dimensions: int\n",
    "    FieldHighBound: list[float]\n",
    "    FieldLowBound: list[float]\n",
    "    def __post_init__(self):\n",
    "        assert  len(self.FieldHighBound) == self.Dimensions| 1, \"Length of high bound and dimensions do not match\"\n",
    "        assert  len(self.FieldLowBound) == self.Dimensions| 1, \"Length of low bound and dimensions do not match\"\n",
    "    @abstractmethod\n",
    "    def FieldStrength(self, ObservationPosition: T.Tensor)-> T.Tensor:\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def FieldPotential(self, ObservationPosition: T.Tensor)-> float:\n",
    "        pass\n",
    "\n",
    "class HomogenousField(Field):\n",
    "    def FieldStrength(self, ObservationPosition: T.Tensor)-> T.Tensor:\n",
    "        return  T.zeros((ObservationPosition.shape[0], self.Dimensions), dtype=T.float64)\n",
    "    def FieldPotential(self, ObservationPosition: T.Tensor)-> float:\n",
    "        return  0.0\n",
    "\n",
    "@dataclass\n",
    "class LJField:\n",
    "    FieldSources: list[Particle]\n",
    "    FieldHighBound: float\n",
    "    FieldLowBound: float\n",
    "    def __call__(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        return self.ElectricFieldStrength(ObservationPosition)\n",
    "    @EnforceMethodTyping\n",
    "    def ElectricFieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "        CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "        for FieldSource in self.FieldSources:\n",
    "            if type(FieldSource) != Particle:\n",
    "                raise TypeError(\"The input is not valid\")\n",
    "        assert type(ObservationPosition) == T.Tensor, \"Invalid Reference point data type\"\n",
    "        ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "        for FieldSource in self.FieldSources:\n",
    "            PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                            T.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "            DisplacementVector = ObservationPosition - PositionMatrices\n",
    "            DisplacementMagnitude = T.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "            ElectricFieldVector += ((FieldSource.Charge) / DisplacementMagnitude**3 * DisplacementVector) - ((FieldSource.Charge) / DisplacementMagnitude**6 * DisplacementVector)\n",
    "        ElectricFieldVector= CoulombConstant *ElectricFieldVector\n",
    "        return ElectricFieldVector #N/C or V/m\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ForceFieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def PlotField(self):\n",
    "        'This funtion plots the 2D electric vector field'\n",
    "        ObservationPosition= T.meshgrid(T.linspace(self.FieldLowBound, self.FieldHighBound, 40), \n",
    "                                        T.linspace(self.FieldLowBound, self.FieldHighBound, 40))\n",
    "        ObservationPosition= T.stack(ObservationPosition)\n",
    "        xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "        xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "        yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "        color_aara = T.sqrt(xd**2+ yd**2)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "        fig.colorbar(cp)\n",
    "        plt.rcParams['figure.dpi'] = 250\n",
    "        plt.show()\n",
    "\n",
    "@dataclass\n",
    "class ElectricField:\n",
    "    FieldSources: list[Particle]\n",
    "    FieldHighBound: float\n",
    "    FieldLowBound: float\n",
    "    def __call__(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        return self.ElectricFieldStrength(ObservationPosition)\n",
    "    @EnforceMethodTyping\n",
    "    def ElectricFieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "        CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "        for FieldSource in self.FieldSources:\n",
    "            if type(FieldSource) != Particle:\n",
    "                raise TypeError(\"The input is not valid\")\n",
    "        assert type(ObservationPosition) == T.Tensor, \"Invalid Reference point data type\"\n",
    "        ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "        for FieldSource in self.FieldSources:\n",
    "            PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                            T.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "            DisplacementVector = ObservationPosition - PositionMatrices\n",
    "            DisplacementMagnitude = T.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "            ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacementVector\n",
    "        return ElectricFieldVector #N/C or V/m\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ElectricFieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def PlotField(self):\n",
    "        'This funtion plots the 2D electric vector field'\n",
    "        ObservationPosition= T.meshgrid(T.linspace(self.FieldLowBound, self.FieldHighBound, 50), \n",
    "                                        T.linspace(self.FieldLowBound, self.FieldHighBound, 50))\n",
    "        ObservationPosition= T.stack(ObservationPosition)\n",
    "        xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "        xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "        yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "        color_aara = T.sqrt(xd**2+ yd**2)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "        fig.colorbar(cp)\n",
    "        plt.rcParams['figure.dpi'] = 250\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElectricField1= ElectricField(Source, 10.0, -10.0)\n",
    "# ElectricField1.PlotField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should obey newtons laws in Homogenous vector field \n",
    "@dataclass\n",
    "class Environment:\n",
    "    @dataclass\n",
    "    class State:\n",
    "        pass\n",
    "    InitialState: State \n",
    "    CurrentState: State \n",
    " \n",
    "    def __post_init__(self):\n",
    "        pass\n",
    " \n",
    "    @abstractmethod\n",
    "    def StateDynamics(self, State: State, Action)-> State:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def RewardModel(self, State: State, Action, NextState: State, TerminalSignal: bool)-> float:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def IsTerminalCondition(self, State: State)-> bool:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def StateTransition(self, State: State, Action)-> tuple[float, State, bool]:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def Run(self, RunDuration: float)-> list[State]:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "    \n",
    "@dataclass\n",
    "class ParticleInField(EnforceClassTyping):\n",
    "    '''This class represents the environment the agent will learn from. \n",
    "    \n",
    "    The UppperBoundX, LowerBoundX, UpperBoundY, and LowerBoundY determine the dimensions of the viable learning region of the environment.\n",
    "    The FieldType determines the physics/dynamics of the environment\n",
    "    The FieldSources shape the field '''\n",
    "    Field: ElectricField\n",
    "    ChargedParticle: Particle\n",
    "    \n",
    "    @dataclass \n",
    "    class State(EnforceClassTyping):\n",
    "        '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "        These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "        Position: T.Tensor # m\n",
    "        Momentum: T.Tensor #kg*m/s\n",
    "        Time: float # s\n",
    "\n",
    "        # def __add__(self, other):\n",
    "        #     Position = self.Position + other.Position\n",
    "        #     Momentum = self.Momentum + other.Momentum\n",
    "        #     Time = self.Time + other.Time\n",
    "        #     return self(Position, Momentum, Time)\n",
    "    InitialState: State = None\n",
    "    CurrentState: State = None\n",
    "    def __post_init__(self):\n",
    "        if self.InitialState is None:\n",
    "            self.InitialState= self.RandomState()\n",
    "        self.CurrentState= self.InitialState\n",
    "\n",
    "    # def StateDynamics(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0])):\n",
    "    #     PositionDynamics= State.Momentum/ self.ChargedParticle.Mass\n",
    "    #     MomentumDynamics= (self.ChargedParticle.Charge* self.Field(State.Position))+Action\n",
    "    #     TimeDynamics= 1.0\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def TransitionModel(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval:float= 1, Resolution: int=30)-> State:\n",
    "        '''This function determines how the state of the system changes after a given period given the agents state and parameters'''\n",
    "        CurrentMomentum= State.Momentum\n",
    "        CurrrentPosition= State.Position\n",
    "        TimeTaken= 0\n",
    "        for _ in range(Resolution):\n",
    "            CurrentMomentum, CurrrentPosition= CurrentMomentum + ((self.ChargedParticle.Charge* self.Field(CurrrentPosition))+Action)*(TimeInterval/Resolution), CurrrentPosition+ (CurrentMomentum/ self.ChargedParticle.Mass)*(TimeInterval/Resolution)\n",
    "            TimeTaken+= (TimeInterval/Resolution)\n",
    "        CurrentTime= State.Time+ TimeTaken\n",
    "        return self.State(CurrrentPosition, CurrentMomentum, CurrentTime)\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def IsTerminalCondition(self, State: State)-> bool:\n",
    "        '''This method determines if a position is within the viable learning region of the environment'''\n",
    "        WithinXBound= self.Field.FieldLowBound <= State.Position[0] <= self.Field.FieldHighBound\n",
    "        WithinYBound= self.Field.FieldLowBound <= State.Position[1] <= self.Field.FieldHighBound\n",
    "        if WithinXBound or WithinYBound:\n",
    "            return False    \n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "\n",
    "    def Step(self, State: State= CurrentState, Action: T.Tensor= T.tensor([0.0, 0.0])):\n",
    "        NextState= self.TransitionModel(State, Action)\n",
    "        TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "        return NextState, TerminalSignal\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def RandomState(self)->State:\n",
    "        '''This method generates a random state within the viable learning region'''\n",
    "        RandomPosition= T.Tensor([np.random.uniform(self.Field.FieldLowBound, self.Field.FieldHighBound), \n",
    "                                  np.random.uniform(self.Field.FieldLowBound, self.Field.FieldHighBound)])\n",
    "        RandomMomentum= T.zeros_like(RandomPosition)\n",
    "        return self.State(RandomPosition, RandomMomentum, 0.0)\n",
    "\n",
    "    def Run(self, RunDuration: float, Resolution: int=30):\n",
    "        Path= []\n",
    "        State= self.CurrentState\n",
    "        Time= 0\n",
    "        for _ in range(Resolution):\n",
    "            Path.append(State.Position)\n",
    "            State= self.TransitionModel(State, 1e-7* T.randn(2))\n",
    "            Time += (RunDuration/Resolution)\n",
    "        return Path\n",
    "    \n",
    "    def Lagrangian(self):\n",
    "        pass\n",
    "    \n",
    "    def PlotRun(self, RunDuration: float):\n",
    "        Path= self.Run(RunDuration)\n",
    "        Path= T.stack(Path)\n",
    "        Path= Path.transpose(dim0=0, dim1=1)\n",
    "        # print(Path)\n",
    "        t=  T.arange(0, RunDuration)\n",
    "        plt.plot(Path[0], Path[1])\n",
    "        plt.plot(Path[0][0], Path[1][0], 'ko')\n",
    "        plt.plot(Path[0][-1], Path[1][-1], 'r*')\n",
    "        plt.xlim(-10,10)\n",
    "        plt.ylim(-10,10)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def Reset(self):\n",
    "        self.CurrentState= self.Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Environment1= ParticleInField(ElectricField1, Proton)\n",
    "Environment1.PlotRun(20)\n",
    "# -(Environment.ChargedParticle.Charge* Environment.Field(Environment.CurrentState.Position))\n",
    "# Environment.Step(Environment.CurrentState,T.tensor([1e-9, 1e-9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__() \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims+n_actions, fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = T.cat([state, action], dim=-1)\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic Network Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.4355, -0.7806,  0.3042,  1.1601, -0.1184]]),\n",
       " tensor([[0.8233, 0.8126]]),\n",
       " tensor([[0.6215]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 1135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N= CriticNetwork(0.02, 5, 10, 15, 2, name='tb')\n",
    "\n",
    "bt= T.tensor([[-1.4355, -0.7806,  0.3042,  1.1601, -0.1184]])\n",
    "at= T.tensor([[0.8233, 0.8126]])\n",
    "# for target_param in N.parameters():\n",
    "#     print(target_param) \n",
    "# bt, m(bt), N(bt)\n",
    "N.load_checkpoint()\n",
    "bt, at, N(bt, at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims , fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, n_actions))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = T.relu(self.bn1(self.fc1(state)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor Network Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading checkpoint ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.6552,  0.0852,  2.0087, -0.6352,  0.4445]]),\n",
       " tensor([[-0.5010,  0.0301]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 1137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nt= ActorNetwork(0.02, 5, 10, 15, 2, name='btt')\n",
    "\n",
    "btt= T.tensor([[-0.6552,  0.0852,  2.0087, -0.6352,  0.4445]])\n",
    "att= T.randn(1, 2)\n",
    "# for target_param in N.parameters():\n",
    "#     print(target_param) \n",
    "# bt, m(bt), N(bt)\n",
    "Nt.load_checkpoint()\n",
    "btt, Nt(btt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ReplayBuffer:\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    def AddExperience(self, State, Action, NextState, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append({'State': State, 'Action': Action, 'NextState': NextState, 'Reward': Reward, 'TerminalState': TerminalState})\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append({'State': State, 'Action': Action, 'NextState': NextState, 'Reward': Reward, 'TerminalState': TerminalState})\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [T.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [T.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= T.stack(SampledStates)\n",
    "            ActionBatch= T.stack(SampledActions)\n",
    "            NextStateBatch= T.stack(SampledNextStates)\n",
    "            RewardsBatch= T.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= T.stack(SampledTerminalSignals)\n",
    " \n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch\n",
    "    def __len__(self):\n",
    "        return len(self.Buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt=ReplayBuffer(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qw/w54k7dcj4mlftnk6g2k3ys1w0000gn/T/ipykernel_17191/3012710907.py:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  observation = T.tensor(observation, dtype=T.float).to(self.actor.device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'Target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1140], line 208\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[38;5;28minput\u001b[39m()\n\u001b[1;32m    207\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(\u001b[38;5;241m0.000025\u001b[39m, \u001b[38;5;241m0.00025\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, Environment1, T\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]))\n\u001b[0;32m--> 208\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDDPGAlgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1140], line 81\u001b[0m, in \u001b[0;36mAgent.DDPGAlgorithm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m Action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(observation)\n\u001b[1;32m     80\u001b[0m new_state, done\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mCurrentState, Action)\n\u001b[0;32m---> 81\u001b[0m reward\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRewardModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCurrentState\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mCurrentState, Action, new_state, reward, \u001b[38;5;28mint\u001b[39m(done)))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# self.memory.AddExperience(CurrentState, Action, new_state, reward, int(done)) \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1140], line 66\u001b[0m, in \u001b[0;36mAgent.RewardModel\u001b[0;34m(self, State, Action, NextState, TerminalSignal)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRewardModel\u001b[39m(\u001b[38;5;28mself\u001b[39m, State , Action , NextState , TerminalSignal )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     DistanceGainedFromTarget\u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mnorm(State\u001b[38;5;241m.\u001b[39mPosition\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTarget\u001b[49m)\u001b[38;5;241m-\u001b[39m T\u001b[38;5;241m.\u001b[39mnorm(NextState\u001b[38;5;241m.\u001b[39mPosition\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTarget) \n\u001b[1;32m     67\u001b[0m     EnergyConsumed\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mField\u001b[38;5;241m.\u001b[39mWorkDoneAgainstField(State\u001b[38;5;241m.\u001b[39mPosition, NextState\u001b[38;5;241m.\u001b[39mPosition)\n\u001b[1;32m     68\u001b[0m     Cost\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDistanceWeight\u001b[38;5;241m*\u001b[39m DistanceGainedFromTarget\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEnergyWeight\u001b[38;5;241m*\u001b[39m EnergyConsumed\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTerminalSignalWeight\u001b[38;5;241m*\u001b[39m TerminalSignal\u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'Target'"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Agent:\n",
    "    AgentEnvironment: Environment\n",
    "\n",
    "    def __post_init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "        NotImplementedError (\"Subclasses must implement the `Act` method\")\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def Observe(self)-> T.Tensor:\n",
    "        NotImplementedError (\"Subclasses must implement the `Act` method\")\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Learn(self):\n",
    "        'Improves  the agent by updating its models'\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def LearningAlgorithm(self):\n",
    "        pass\n",
    "    \n",
    "class Agent(object):\n",
    "    def __init__(self, lr_actor, lr_critic, input_dims, tau, env, Target,\n",
    "                 gamma=0.99, n_actions=2, max_size=1000, layer1_size=20, layer2_size=15, batch_size=16, time_step= 0.5):\n",
    "        self.actor = ActorNetwork(lr_actor, input_dims, layer1_size, layer2_size, n_actions=n_actions, name='Actor')\n",
    "        self.critic = CriticNetwork(lr_critic, input_dims, layer1_size, layer2_size, n_actions=n_actions, name='Critic')\n",
    "        self.target_actor = ActorNetwork(lr_actor, input_dims, layer1_size, layer2_size, n_actions=n_actions, name='TargetActor')\n",
    "        self.target_critic = CriticNetwork(lr_critic, input_dims, layer1_size, layer2_size, n_actions=n_actions, name='TargetCritic')\n",
    "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        # self.memory = ReplayBuffer(max_size)\n",
    "        self.memory = deque(maxlen=max_size)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.env: ParticleInField= env\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "        self.time_step= time_step\n",
    "        Target: T.Tensor= Target\n",
    "        DistanceWeight: float= 0.5\n",
    "        EnergyWeight: float= 0.5\n",
    "        TerminalSignalWeight: float= 0.5\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(self.noise(), dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach()\n",
    "\n",
    "    def Observe(self):\n",
    "        State= self.env.CurrentState \n",
    "        Observation= T.cat([State.Position,\n",
    "                          State.Momentum,\n",
    "                          T.tensor([State.Time])])\n",
    "        return Observation\n",
    "    \n",
    "    def RewardModel(self, State , Action , NextState , TerminalSignal )-> float:\n",
    "        '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "        DistanceGainedFromTarget= T.norm(State.Position-self.Target)- T.norm(NextState.Position-self.Target) \n",
    "        EnergyConsumed= self.Field.WorkDoneAgainstField(State.Position, NextState.Position)\n",
    "        Cost= self.DistanceWeight* DistanceGainedFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal+ 1.0\n",
    "        return -Cost.item()\n",
    "    \n",
    "    def DDPGAlgorithm(self):\n",
    "        score_history = []\n",
    "        for i in range(100):\n",
    "            self.env.CurrentState = self.env.InitialState\n",
    "            done = False\n",
    "            score = 0\n",
    "            for _ in range(100):\n",
    "                observation= self.Observe()\n",
    "                Action = self.choose_action(observation)\n",
    "                new_state, done= self.env.Step(self.env.CurrentState, Action)\n",
    "                reward= self.RewardModel(self.env.CurrentState, Action, new_state, done)\n",
    "                self.memory.append((self.env.CurrentState, Action, new_state, reward, int(done)))\n",
    "                # self.memory.AddExperience(CurrentState, Action, new_state, reward, int(done)) \n",
    "                self.learn()\n",
    "                score += reward\n",
    "                # print(reward)\n",
    "                self.env.CurrentState = new_state\n",
    "            score_history.append(score)\n",
    "        plt.plot(score_history)\n",
    "        return score_history\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        # state, action, new_state, reward, done = self.memory.SampleBuffer(self.batch_size)\n",
    "\n",
    "        # reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        # done = T.tensor(done).to(self.critic.device)\n",
    "        # new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        # action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        # state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, next_states, rewards, dones = zip(*batch)\n",
    "        # print(states)\n",
    "        # print(actions)\n",
    "        # print(next_states)\n",
    "        # print(T.tensor(rewards, dtype=T.float).unsqueeze(1) )\n",
    "        # print(T.tensor(dones, dtype=T.float).unsqueeze(1) )\n",
    "        state = T.stack(states).to(self.critic.device)\n",
    "        action = T.stack(actions).to(self.critic.device)\n",
    "        reward = T.tensor(rewards, dtype=T.float).unsqueeze(1).to(self.critic.device)\n",
    "        new_state = T.stack(next_states).to(self.critic.device)\n",
    "        done = T.tensor(dones, dtype=T.float).unsqueeze(1).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "        \n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
    "\n",
    "        q_expected = self.critic.forward(state, action)\n",
    "        q_targets = reward + self.gamma * critic_value_ * (1 - done)\n",
    "\n",
    "        critic_loss = nn.MSELoss()(q_expected, q_targets.detach())\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "    \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + (1-tau)*target_critic_dict[name].clone()\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(target_critic_params)\n",
    "        actor_state_dict = dict(target_actor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.actor.named_parameters():\n",
    "            print(name, T.equal(param, actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.critic.named_parameters():\n",
    "            print(name, T.equal(param, critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def check_actor_params(self):\n",
    "        current_actor_params = self.actor.named_parameters()\n",
    "        current_actor_dict = dict(current_actor_params)\n",
    "        original_actor_dict = dict(self.original_actor.named_parameters())\n",
    "        original_critic_dict = dict(self.original_critic.named_parameters())\n",
    "        current_critic_params = self.critic.named_parameters()\n",
    "        current_critic_dict = dict(current_critic_params)\n",
    "        print('Checking Actor parameters')\n",
    "\n",
    "        for param in current_actor_dict:\n",
    "            print(param, T.equal(original_actor_dict[param], current_actor_dict[param]))\n",
    "        print('Checking critic parameters')\n",
    "        for param in current_critic_dict:\n",
    "            print(param, T.equal(original_critic_dict[param], current_critic_dict[param]))\n",
    "        input()\n",
    "agent = Agent(0.000025, 0.00025, 5, 0.001, Environment1, T.tensor([1.0, 1.0]))\n",
    "agent.DDPGAlgorithm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
