{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "# shoul obey newtons laws in Homogenous vector field\n",
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    " \n",
    "@dataclass\n",
    "class Particle(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Mass: float # m\n",
    "    Charge: float #C\n",
    "    Position: T.Tensor # m\n",
    "    Velocity: T.Tensor #kg*m/s\n",
    "\n",
    "Electron= Particle(Mass=1.0, Charge= -1e-9, Position=T.tensor([1.0, 2.0]), Velocity=T.tensor([0.0, 0.0]))\n",
    "\n",
    "@dataclass\n",
    "class ElectricField:\n",
    "    FieldSources: list\n",
    "    FieldHighBound: float\n",
    "    FieldLowBound: float\n",
    "    def __call__(self, ObservationPosition: T.Tensor):\n",
    "        return self.ElectricFieldStrength(ObservationPosition)\n",
    "    def ElectricFieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "        CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "        for FieldSource in self.FieldSources:\n",
    "            if type(FieldSource) != Particle:\n",
    "                raise TypeError(\"The input is not valid\")\n",
    "        if type(ObservationPosition[0]) != type(ObservationPosition[1]):\n",
    "            raise TypeError(\"Incompatible Reference point data types\")\n",
    "        elif type(ObservationPosition[0]) != T.Tensor:\n",
    "            raise TypeError(\"Invalid Reference point data type\")\n",
    "        elif ObservationPosition[0].size()!=ObservationPosition[1].size():\n",
    "            raise TypeError(\"Incompatible Reference point dimensions\")\n",
    "        else: \n",
    "            ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "        for FieldSource in self.FieldSources:\n",
    "            PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                            T.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "            DisplacemnetVector = ObservationPosition - PositionMatrices\n",
    "            DisplacementMagnitude = T.sqrt(DisplacemnetVector[0]**2 +DisplacemnetVector[1]**2)  # Magnitude of the displacement vector\n",
    "            ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacemnetVector\n",
    "        return ElectricFieldVector #N/C or V/m\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ForceFieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    def KineticEnergy(self, Mass: float, Velocity: float)-> float:\n",
    "        return 0.5* Mass* Velocity**2\n",
    "\n",
    "    def PlotField(self, ObservationPosition: T.Tensor):\n",
    "        'This funtion plots the 2D electric vector field'\n",
    "        xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "        xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "        yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "        color_aara = T.sqrt(xd**2+ yd**2)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "        fig.colorbar(cp)\n",
    "        plt.rcParams['figure.dpi'] = 150\n",
    "        plt.show()\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        #f2 = 0.002\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        f3 = 0.003\n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        T.nn.init.uniform_(self.q.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.q.bias.data, -f3, f3)\n",
    "        #self.q.weight.data.uniform_(-f3, f3)\n",
    "        #self.q.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "\n",
    "        action_value = F.relu(self.action_value(action))\n",
    "        state_action_value = F.relu(T.add(state_value, action_value))\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
    "        T.nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
    "        #self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        #self.fc1.bias.data.uniform_(-f1, f1)\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "        #f2 = 0.002\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        T.nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
    "        T.nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
    "        #self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        #self.fc2.bias.data.uniform_(-f2, f2)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        #f3 = 0.004\n",
    "        f3 = 0.003\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "        T.nn.init.uniform_(self.mu.weight.data, -f3, f3)\n",
    "        T.nn.init.uniform_(self.mu.bias.data, -f3, f3)\n",
    "        #self.mu.weight.data.uniform_(-f3, f3)\n",
    "        #self.mu.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "@dataclass \n",
    "class ReplayBuffer(EnforceClassTyping):\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    @EnforceMethodTyping\n",
    "    def AddExperience(self, State: State, Action: T.Tensor, NextState: State, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "    @EnforceMethodTyping\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [T.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [T.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= T.stack(SampledStates)\n",
    "            ActionBatch= T.stack(SampledActions)\n",
    "            NextStateBatch= T.stack(SampledNextStates)\n",
    "            RewardsBatch= T.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= T.stack(SampledTerminalSignals)\n",
    "        else:\n",
    "            raise ValueError('BatchSize too big')\n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch\n",
    "\n",
    "@dataclass\n",
    "class ParticleInField(EnforceClassTyping):\n",
    "    '''This class represents the environment(i.e. the Space and Physics) the agent will learn from. \n",
    "    \n",
    "    The UppperBoundX, LowerBoundX, UpperBoundY, and LowerBoundY determine the dimensions of the viable learning region of the environment.\n",
    "    The FieldType determines the physics/dynamics of the environment\n",
    "    The FieldSources shape the field '''\n",
    "    Field: ElectricField\n",
    "    ChargedParticle: Particle\n",
    "    @dataclass \n",
    "    class State(EnforceClassTyping):\n",
    "        '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "        These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "        Position: T.Tensor # m\n",
    "        Momentum: T.Tensor #kg*m/s\n",
    "        Time: float  # s\n",
    "        def Unwrap(self)->T.Tensor:\n",
    "            '''This function converts the state parameters to a single tensor for processing. '''\n",
    "            return T.cat([self.Position, \n",
    "                            self.Momentum])\n",
    "    Target: T.Tensor\n",
    "    DistanceWeight: float\n",
    "    EnergyWeight: float\n",
    "    TerminalSignalWeight: float\n",
    "    CurrentState: State = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.CurrentState is None:\n",
    "            self.CurrentState= self.RandomState()\n",
    "\n",
    "    @EnforceMethodTyping\n",
    "    def TransitionModel(self, State: State, Action: T.Tensor, TimeStep:float)-> State:\n",
    "        '''This function determines how the state of the agent changes after a given period given the agents state and parameters'''\n",
    "        InitialVelocity= State.Momentum/ self.ChargedParticle.Mass\n",
    "        Acceleration= (Action- self.CurrentState.FieldStrength* self.ChargedParticle.Charge)/self.ChargedParticle.Mass\n",
    "        FinalVelocity= InitialVelocity+ Acceleration*TimeStep\n",
    "        NewPosition= InitialVelocity*TimeStep- (Acceleration*TimeStep**2)/2\n",
    "        NewFieldForce= self.ForceFieldStrength(NewPosition)\n",
    "        ResultantMomemntum= FinalVelocity* self.ChargedParticle.Mass\n",
    "        NewState= self.State(NewPosition, NewFieldForce, ResultantMomemntum)\n",
    "        return NewState\n",
    "    @EnforceMethodTyping\n",
    "    def IsTerminalCondition(self, State: State)-> bool:\n",
    "        '''This method determines if a position is within the viable learning region of the environment'''\n",
    "        if self.LowerBoundX <= Position[0] <= self.UppperBoundX or self.LowerBoundY <= Position[1] <= self.UpperBoundY:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    @EnforceMethodTyping\n",
    "    def RewardModel(self, State: State, Action: T.Tensor, NextState: State, TerminalSignal: bool, Resolution: int= 5000)-> float:\n",
    "        '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "        DistanceGainedFromTarget= T.norm(self.CurrentState.Position-Target)- T.norm(NextState.Position-Target) \n",
    "        EnergyConsumed= self.WorkDoneAgainstField(self.CurrentState.Position, NextState.Position, Resolution)\n",
    "        Cost= self.DistanceWeight* DistanceGainedFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal\n",
    "        return -Cost.item()\n",
    "    @EnforceMethodTyping\n",
    "    def RandomState(self)->State:\n",
    "        '''This method generates a random state within the viable learning region'''\n",
    "        RandomPosition= T.Tensor([random.uniform(self.LowerBoundX, self.UppperBoundX), random.uniform(self.LowerBoundY, self.UpperBoundY)])\n",
    "        RandomFieldStrength= self.ForceFieldStrength(RandomPosition)\n",
    "        RandomMomentum= T.squeeze(T.rand((1, 2)))\n",
    "        return State(RandomPosition, RandomFieldStrength, RandomMomentum)\n",
    "    \n",
    "    def Render(self):\n",
    "        pass\n",
    "\n",
    "    def Run(self, RunDuration: float):\n",
    "        pass\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, \n",
    "                 alpha, \n",
    "                 beta, \n",
    "                 input_dims, \n",
    "                 tau, \n",
    "                 env, \n",
    "                 gamma=0.99,\n",
    "                 n_actions=2, \n",
    "                 max_size=1000000, \n",
    "                 layer1_size=400,\n",
    "                 layer2_size=300, \n",
    "                 batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                  layer2_size, n_actions=n_actions,\n",
    "                                  name='Actor')\n",
    "        self.critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                    layer2_size, n_actions=n_actions,\n",
    "                                    name='Critic')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, layer1_size,\n",
    "                                         layer2_size, n_actions=n_actions,\n",
    "                                         name='TargetActor')\n",
    "        self.target_critic = CriticNetwork(beta, input_dims, layer1_size,\n",
    "                                           layer2_size, n_actions=n_actions,\n",
    "                                           name='TargetCritic')\n",
    "        self.env= env\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.actor.eval()\n",
    "        observation = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(observation).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(self.noise(),\n",
    "                                 dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_prime.cpu().detach().numpy()\n",
    "\n",
    "    def DDPGAlgorithm(self):\n",
    "        score_history = []\n",
    "        for i in range(1000):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                act = agent.choose_action(obs)\n",
    "                new_state, reward, done, info = self.env.step(act)\n",
    "                agent.remember(obs, act, reward, new_state, int(done))\n",
    "                agent.learn()\n",
    "                score += reward\n",
    "                obs = new_state\n",
    "                #env.render()\n",
    "            score_history.append(score)\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size:\n",
    "            return\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                      self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.critic.device)\n",
    "        done = T.tensor(done).to(self.critic.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.critic.device)\n",
    "        action = T.tensor(action, dtype=T.float).to(self.critic.device)\n",
    "        state = T.tensor(state, dtype=T.float).to(self.critic.device)\n",
    "\n",
    "        self.target_actor.eval()\n",
    "        self.target_critic.eval()\n",
    "        self.critic.eval()\n",
    "        target_actions = self.target_actor.forward(new_state)\n",
    "        critic_value_ = self.target_critic.forward(new_state, target_actions)\n",
    "        critic_value = self.critic.forward(state, action)\n",
    "\n",
    "        target = []\n",
    "        for j in range(self.batch_size):\n",
    "            target.append(reward[j] + self.gamma*critic_value_[j]*done[j])\n",
    "        target = T.tensor(target).to(self.critic.device)\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.train()\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.critic.eval()\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        mu = self.actor.forward(state)\n",
    "        self.actor.train()\n",
    "        actor_loss = -self.critic.forward(state, mu)\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_critic_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "            actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                      (1-tau)*target_actor_dict[name].clone()\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(target_critic_params)\n",
    "        actor_state_dict = dict(target_actor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.actor.named_parameters():\n",
    "            print(name, T.equal(param, actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.critic.named_parameters():\n",
    "            print(name, T.equal(param, critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def check_actor_params(self):\n",
    "        current_actor_params = self.actor.named_parameters()\n",
    "        current_actor_dict = dict(current_actor_params)\n",
    "        original_actor_dict = dict(self.original_actor.named_parameters())\n",
    "        original_critic_dict = dict(self.original_critic.named_parameters())\n",
    "        current_critic_params = self.critic.named_parameters()\n",
    "        current_critic_dict = dict(current_critic_params)\n",
    "        print('Checking Actor parameters')\n",
    "\n",
    "        for param in current_actor_dict:\n",
    "            print(param, T.equal(original_actor_dict[param], current_actor_dict[param]))\n",
    "        print('Checking critic parameters')\n",
    "        for param in current_critic_dict:\n",
    "            print(param, T.equal(original_critic_dict[param], current_critic_dict[param]))\n",
    "        input()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
