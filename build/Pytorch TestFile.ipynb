{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An agent trying to move through a field using the least amount of energy\n",
    "'''This module contains a detailed implementation of the Deep Deterministic Policy Gradient (DDPG) algorithm, a model-free off-policy actor-critic reinforcement learning algorithm using pytorch's neural network tools.'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "import torch.autograd\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def tanh_derivative(x: torch.Tensor|int|float)-> torch.Tensor|int|float:\n",
    "    return 1-x**2\n",
    "def neg_relu(x):\n",
    "    return min(0, x)\n",
    "def f(x):\n",
    "    return x\n",
    "def f_grad(x):\n",
    "    return torch.ones_like(x)\n",
    "def relu_derivative(tensor):\n",
    "    return torch.where(tensor > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "def mse_grad(x, y):\n",
    "    return (-2* (x-y))/len(x)\n",
    "\n",
    "@dataclass\n",
    "class Source(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Position: torch.Tensor # m\n",
    "    Charge: float #C\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def ElectricField(FieldSources: list, ObservationPosition: torch.Tensor)->torch.Tensor:\n",
    "    'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    for FieldSource in FieldSources:\n",
    "        if type(FieldSource) != Source:\n",
    "            raise TypeError(\"The input is not valid\")\n",
    "    if type(ObservationPosition[0]) != type(ObservationPosition[1]):\n",
    "         raise TypeError(\"Incompatible Reference point data types\")\n",
    "    elif type(ObservationPosition[0]) != torch.Tensor:\n",
    "        raise TypeError(\"Invalid Reference point data type\")\n",
    "    elif ObservationPosition[0].size()!=ObservationPosition[1].size():\n",
    "        raise TypeError(\"Incompatible Reference point dimensions\")\n",
    "    else: \n",
    "        ElectricFieldVector = torch.zeros_like(ObservationPosition)\n",
    "    for FieldSource in FieldSources:\n",
    "        PositionMatrices= torch.stack([torch.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                        torch.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "        DisplacemnetVector = ObservationPosition - PositionMatrices\n",
    "        DisplacementMagnitude = torch.sqrt(DisplacemnetVector[0]**2 +DisplacemnetVector[1]**2)  # Magnitude of the displacement vector\n",
    "        ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacemnetVector\n",
    "    return ElectricFieldVector #N/C or V/m\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def PlotField(Sources: list, ObservationPosition: torch.Tensor):\n",
    "    'This funtion plots the 2D electric vector field'\n",
    "    xd, yd = ElectricField(Sources, ObservationPosition)\n",
    "    xd = xd / torch.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / torch.sqrt(xd**2 + yd**2)\n",
    "    color_aara = torch.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n",
    "\n",
    "@dataclass \n",
    "class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "       These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: torch.Tensor # m\n",
    "    FieldStrength: torch.Tensor #N/C or V/m\n",
    "    Momentum: torch.Tensor #kg*m/s\n",
    "    def Unwrap(self)->torch.Tensor:\n",
    "        '''This function converts the state parameters to a single tensor for processing. '''\n",
    "        return torch.cat([self.Position, \n",
    "                          self.FieldStrength,\n",
    "                          self.Momentum])\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        return x\n",
    "\n",
    "@dataclass \n",
    "class ReplayBuffer(EnforceClassTyping):\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    @EnforceMethodTyping\n",
    "    def AddExperience(self, State: State, Action: torch.Tensor, NextState: State, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "    @EnforceMethodTyping\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [torch.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [torch.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= torch.stack(SampledStates)\n",
    "            ActionBatch= torch.stack(SampledActions)\n",
    "            NextStateBatch= torch.stack(SampledNextStates)\n",
    "            RewardsBatch= torch.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= torch.stack(SampledTerminalSignals)\n",
    "        else:\n",
    "            raise ValueError('BatchSize too big')\n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch\n",
    "\n",
    "@dataclass\n",
    "class Agent(EnforceClassTyping):\n",
    "    '''This class represents the agent which will interact with the environment to create state state transitions which it will use to learn a good policy and value function.\n",
    "\n",
    "    The Mass and Charge parameters deteremine how the interacts with its environment.\n",
    "    The LearningRate, LossFunction, HiddenLayerSize, and MemorySize parameters determine its learning behaviour.'''\n",
    "    Charge: float\n",
    "    Mass: float\n",
    "    LearningRate: float\n",
    "    SoftUpdateRate: float\n",
    "    DiscountRate: float\n",
    "    MemorySize: int\n",
    "    CurrentState: State \n",
    "    Memory: ReplayBuffer = field(init=False) \n",
    "    ActorModel: ActorNetwork = field(init=False)\n",
    "    CriticModel: CriticNetwork = field(init=False)\n",
    "    ActorTargetModel: ActorNetwork = field(init=False)\n",
    "    CriticTargetModel: CriticNetwork = field(init=False)\n",
    "    def __post_init__(self):\n",
    "        self.Memory= ReplayBuffer(self.MemorySize)\n",
    "        self.ActorModel= ActorNetwork(6, 10, 2)\n",
    "        self.ActorTargetModel= self.ActorModel\n",
    "        self.CriticModel= CriticNetwork(8, 10, 2)\n",
    "        self.CriticTargetModel= self.CriticModel\n",
    "        self.actor_optimizer  = optim.Adam(self.ActorModel.parameters(), lr=self.LearningRate)\n",
    "        self.critic_optimizer = optim.Adam(self.CriticModel.parameters(), lr=self.LearningRate)\n",
    "        self.critic_criterion= torch.nn.MSELoss()\n",
    "    def ForceGenerator(self, Action: torch.Tensor)-> torch.Tensor:\n",
    "        ForceVector= Action* 20\n",
    "        return ForceVector\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateCritic(self, StateBatch: torch.Tensor, ActionBatch: torch.Tensor, NextStateBatch: torch.Tensor, RewardBatch: torch.Tensor, TerminalSignalsbatch: torch.Tensor):\n",
    "        'Updates the main critic network parameters by minimizing the difference between the bellman optimal expected return and the expected return predicted by the main critic network'\n",
    "        ExpectedReturn= self.CriticModel.forward(StateBatch, ActionBatch)\n",
    "        NextAction= self.ActorTargetModel.forward(NextStateBatch)\n",
    "        BellmanOptimalReturn= RewardBatch+ (1-TerminalSignalsbatch)*self.DiscountRate*self.CriticTargetModel.forward(NextStateBatch, NextAction)\n",
    "        critic_loss= self.critic_criterion(BellmanOptimalReturn, ExpectedReturn)\n",
    "\n",
    "        self.CriticModel.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward(retain_graph=True) \n",
    "        self.critic_optimizer.step()\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateActor(self, StateBatch: torch.Tensor):\n",
    "        'Updates the main actor network parameters by maximizing the Expected Q-value predicted by the main critic network'\n",
    "        policy_loss = -self.CriticModel.forward(StateBatch, self.ActorModel.forward(StateBatch)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        policy_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateTargetCritic(self):\n",
    "        'Updates the target critic network parameters by making in it lag behind the main critic network updates'\n",
    "        for target_param, param in zip(self.CriticTargetModel.parameters(), self.CriticModel.parameters()):\n",
    "            target_param.data.copy_(param.data * self.SoftUpdateRate + target_param.data * (1.0 - self.SoftUpdateRate))\n",
    "    @EnforceMethodTyping \n",
    "    def UpdateTargetActor(self):\n",
    "        'Updates the target actor network parameters by making in it lag behind the main actor network updates'\n",
    "        for target_param, param in zip(self.ActorTargetModel.parameters(), self.ActorModel.parameters()):\n",
    "            target_param.data.copy_(param.data * self.SoftUpdateRate + target_param.data * (1.0 - self.SoftUpdateRate))\n",
    "\n",
    "@dataclass\n",
    "class Environment(EnforceClassTyping):\n",
    "    '''This class represents the environment(i.e. the Space and Physics) the agent will learn from. \n",
    "    \n",
    "    The UppperBoundX, LowerBoundX, UpperBoundY, and LowerBoundY determine the dimensions of the viable learning region of the environment.\n",
    "    The FieldType determines the physics/dynamics of the environment\n",
    "    The FieldSources shape the field '''\n",
    "    UppperBoundX: float\n",
    "    LowerBoundX: float\n",
    "    UpperBoundY: float\n",
    "    LowerBoundY: float\n",
    "    FieldSources: list\n",
    "    FieldType: Callable #[[list, torch.Tensor], torch.Tensor]\n",
    "    def KineticEnergy(self, Mass: float, Velocity: float)-> float:\n",
    "        return 0.5* Mass* Velocity**2\n",
    "    @EnforceMethodTyping\n",
    "    def ForceFieldStrength(self, Position: torch.Tensor)-> torch.Tensor:\n",
    "        '''This method determines the field strength at any given position based the field type and field sources'''\n",
    "        FieldStrengthVector = self.FieldType(self.FieldSources, Position)\n",
    "        return FieldStrengthVector\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: torch.Tensor, FinalPosition: torch.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ForceFieldStrength(torch.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def TransitionModel(self, LearningAgent: Agent, CurrentState: State, Action: torch.Tensor, TimeStep:float)-> State:\n",
    "        '''This function determines how the state of the agent changes after a given period given the agents state and parameters'''\n",
    "        InitialVelocity= CurrentState.Momentum/LearningAgent.Mass\n",
    "        Acceleration= (Action- CurrentState.FieldStrength*LearningAgent.Charge)/LearningAgent.Mass\n",
    "        FinalVelocity= InitialVelocity+ Acceleration*TimeStep\n",
    "        NewPosition= InitialVelocity*TimeStep- (Acceleration*TimeStep**2)/2\n",
    "        NewFieldForce= self.ForceFieldStrength(NewPosition)\n",
    "        ResultantMomemntum= FinalVelocity*LearningAgent.Mass\n",
    "        NewState= State(NewPosition, NewFieldForce, ResultantMomemntum)\n",
    "        return NewState\n",
    "    @EnforceMethodTyping\n",
    "    def RewardModel(self, CurrentState: State, Action: torch.Tensor, NextState: State, Target: torch.Tensor, TerminalSignal: bool, DistanceSignificance: float, EnergySignificance: float, TerminalSignalSignificance: float, Resolution: int= 5000)-> float:\n",
    "        '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "        DistanceGainedFromTarget= torch.norm(CurrentState.Position-Target)- torch.norm(NextState.Position-Target) \n",
    "        EnergyConsumed= self.WorkDoneAgainstField(CurrentState.Position, NextState.Position, Resolution)\n",
    "        Cost= DistanceSignificance* DistanceGainedFromTarget+ EnergySignificance* EnergyConsumed+ TerminalSignalSignificance* TerminalSignal\n",
    "        return -Cost.item()\n",
    "    @EnforceMethodTyping\n",
    "    def IsTerminalCondition(self, Position: torch.Tensor)-> bool:\n",
    "        '''This method determines if a position is within the viable learning region of the environment'''\n",
    "        if self.LowerBoundX <= Position[0] <= self.UppperBoundX or self.LowerBoundY <= Position[1] <= self.UpperBoundY:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    @EnforceMethodTyping\n",
    "    def RandomState(self)->State:\n",
    "        '''This method generates a random state within the viable learning region'''\n",
    "        RandomPosition= torch.Tensor([random.uniform(self.LowerBoundX, self.UppperBoundX), random.uniform(self.LowerBoundY, self.UpperBoundY)])\n",
    "        RandomFieldStrength= self.ForceFieldStrength(RandomPosition)\n",
    "        RandomMomentum= torch.squeeze(torch.rand((1, 2)))\n",
    "        return State(RandomPosition, RandomFieldStrength, RandomMomentum)\n",
    "\n",
    "@dataclass\n",
    "class DDPG(EnforceClassTyping):\n",
    "    LearningAgent: Agent\n",
    "    AgentEnvironment: Environment\n",
    "    Target: torch.Tensor\n",
    "    NumberOfEpisodes: int\n",
    "    EpisodeDuration: int\n",
    "    BatchSize: int\n",
    "    alpha: float\n",
    "    beta: float\n",
    "    gamma: float\n",
    "    TimeStep: float\n",
    "    def __post_init__(self):\n",
    "        if self.TimeStep< 0:\n",
    "            raise ValueError('Time step cant be Negative')\n",
    "    def CreateExperience(self):\n",
    "        AgentAction= self.LearningAgent.ActorModel.forward(self.LearningAgent.CurrentState.Unwrap())\n",
    "        NewState= self.AgentEnvironment.TransitionModel(self.LearningAgent, \n",
    "                                                      self.LearningAgent.CurrentState, \n",
    "                                                      AgentAction, \n",
    "                                                      self.TimeStep)\n",
    "        TerminalSignal= self.AgentEnvironment.IsTerminalCondition(NewState.Position)\n",
    "        Reward= self.AgentEnvironment.RewardModel(self.LearningAgent.CurrentState,\n",
    "                                                    AgentAction,\n",
    "                                                    NewState, \n",
    "                                                    self.Target, \n",
    "                                                    TerminalSignal, \n",
    "                                                    self.alpha,\n",
    "                                                    self.beta, \n",
    "                                                    self.gamma) \n",
    "        return self.LearningAgent.CurrentState, AgentAction, NewState, Reward, TerminalSignal\n",
    "    def ActionNoiseGenerator(self, Action: torch.Tensor, theta:float= 0.5, Mean: float= 0)-> torch.Tensor:\n",
    "        OUNoise= -theta*Action+ Mean*torch.rand_like(Action)#np.random.randn\n",
    "        NoisyAction= Action+ OUNoise*self.TimeStep\n",
    "        return NoisyAction\n",
    "    def TrainModel(self):\n",
    "        '''This method runs the DDPG algorithm by letting it learn from the environment over the episodes'''\n",
    "        InitialState= self.LearningAgent.CurrentState\n",
    "        for _ in range(self.NumberOfEpisodes):\n",
    "            self.LearningAgent.CurrentState = InitialState\n",
    "            EpisodeReward = 0\n",
    "            ReturnValues= []\n",
    "            for _ in range(self.EpisodeDuration):\n",
    "                Action = self.LearningAgent.ForceGenerator(self.LearningAgent.ActorModel.forward(self.LearningAgent.CurrentState.Unwrap()))\n",
    "                NoisyAction = self.ActionNoiseGenerator(Action)\n",
    "                NewState = self.AgentEnvironment.TransitionModel(self.LearningAgent, self.LearningAgent.CurrentState, NoisyAction, self.TimeStep) \n",
    "                TerminalSignal= self.AgentEnvironment.IsTerminalCondition(NewState.Position)\n",
    "                Cost= self.AgentEnvironment.RewardModel(self.LearningAgent.CurrentState,\n",
    "                                                    Action,\n",
    "                                                    NewState, \n",
    "                                                    self.Target, \n",
    "                                                    TerminalSignal, \n",
    "                                                    self.alpha,\n",
    "                                                    self.beta, \n",
    "                                                    self.gamma) \n",
    "                self.LearningAgent.Memory.AddExperience(self.LearningAgent.CurrentState, Action, NewState, Cost, TerminalSignal)\n",
    "                if len(self.LearningAgent.Memory.Buffer) > self.BatchSize:\n",
    "                    StateBatch, ActionBatch, NextStateBatch, RewardBatch, TerminalSignalsbatch= self.LearningAgent.Memory.SampleBuffer(self.BatchSize)\n",
    "                    self.LearningAgent.UpdateCritic(StateBatch, ActionBatch, NextStateBatch, RewardBatch, TerminalSignalsbatch)\n",
    "                    self.LearningAgent.UpdateActor(StateBatch)   \n",
    "                    self.LearningAgent.UpdateTargetCritic() \n",
    "                    self.LearningAgent.UpdateTargetActor() \n",
    "                self.LearningAgent.CurrentState = NewState\n",
    "                ReturnValues.append(Cost)\n",
    "                # EpisodeReward  += Reward \n",
    "            plt.plot(ReturnValues)\n",
    "            plt.show()\n",
    "\n",
    "Charge1= Source(torch.tensor([-1, 0]), -1e-9)\n",
    "Charge2= Source(torch.tensor([1, 0]), 1e-9)\n",
    "ChargeSources= [Charge1, Charge2]\n",
    "TestEnvironment= Environment(25.0, \n",
    "                             -25.0, \n",
    "                             25.0, \n",
    "                             -25.0, \n",
    "                             ChargeSources, \n",
    "                             ElectricField)\n",
    "TestState= TestEnvironment.RandomState()\n",
    "TestNextState= TestEnvironment.RandomState()\n",
    "TestAgent= Agent(2.0, 2.0, 0.2, 0.2, 0.2, 64, TestState)\n",
    "TestDDPG= DDPG(TestAgent, \n",
    "               TestEnvironment, \n",
    "               torch.Tensor([-10, 10]),\n",
    "                10, \n",
    "                20, \n",
    "                10, \n",
    "                10.5, \n",
    "                10.2, \n",
    "                -100.0,\n",
    "                0.5)\n",
    "TestDDPG.TrainModel()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
