{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    " \n",
    "def Euler(func, X0, t):\n",
    "  \"\"\"\n",
    "  Euler integrator.\n",
    "  \"\"\"\n",
    "  dt = t[1] - t[0]\n",
    "  nt = len(t)\n",
    "  X  = np.zeros([nt, len(X0)])\n",
    "  X[0] = X0\n",
    "  for i in range(nt-1):\n",
    "      X[i+1] = X[i] + func(X[i], t[i]) * dt\n",
    "  return X\n",
    "\n",
    "def RK4(func, X0, t):\n",
    "  \"\"\" Runge and Kutta 4 integrator. \"\"\"\n",
    "  dt = t[1] - t[0]\n",
    "  nt = len(t)\n",
    "  X  = np.zeros([nt, len(X0)])\n",
    "  X[0] = X0\n",
    "  for i in range(nt-1):\n",
    "      k1 = func(X[i], t[i])\n",
    "      k2 = func(X[i] + dt/2. * k1, t[i] + dt/2.)\n",
    "      k3 = func(X[i] + dt/2. * k2, t[i] + dt/2.)\n",
    "      k4 = func(X[i] + dt    * k3, t[i] + dt)\n",
    "      X[i+1] = X[i] + dt / 6. * (k1 + 2. * k2 + 2. * k3 + k4)\n",
    "  return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
    "                                                            self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Particle(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Mass: float # kg\n",
    "    Charge: float #C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "@dataclass\n",
    "class Field(ABC):\n",
    "  @abstractmethod\n",
    "  def FieldStrength(self, ObservationPosition: T.Tensor)-> T.Tensor:\n",
    "    ...\n",
    "  @abstractmethod\n",
    "  def FieldPotential(self, ObservationPosition: T.Tensor)-> float:\n",
    "    ...\n",
    "  def PotentialDifference(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor) -> float:\n",
    "    ...\n",
    "\n",
    "@dataclass(kw_only= True)\n",
    "class ElectricField(Field):\n",
    "  FieldSources: Dict\n",
    "\n",
    "  def __call__(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "      return self.FieldStrength(ObservationPosition)\n",
    "  @EnforceMethodTyping\n",
    "  def FieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "    'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    assert len(self.FieldSources[\"Particle\"]) == len(self.FieldSources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "    for FieldSource, _ in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      assert isinstance(FieldSource, Particle),  \"The FieldSource is not a Particle\"\n",
    "    ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "    for FieldSource, SourcePosition in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* SourcePosition[0].item(), \n",
    "                                T.ones_like(ObservationPosition[1])* SourcePosition[1].item()])\n",
    "      DisplacementVector = ObservationPosition - PositionMatrices\n",
    "      DisplacementMagnitude = T.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "      ElectricFieldVector += (DisplacementVector * FieldSource.Charge) / DisplacementMagnitude**2\n",
    "    return CoulombConstant * ElectricFieldVector #N/C or V/m\n",
    "  @EnforceMethodTyping\n",
    "  def FieldPotential(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "      '''This method determines the amount of work required to get one position to another in the field'''\n",
    "      XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "      YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "      XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "      YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "      WorkDone = 0\n",
    "      for i in range(resolution):\n",
    "          PositionFieldStrength = self.FieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "          WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "      return WorkDone\n",
    "  @EnforceMethodTyping\n",
    "  def PlotField(self):\n",
    "      'This funtion plots the 2D electric vector field'\n",
    "      ObservationPosition= T.meshgrid(T.linspace(self.FieldLowBound, self.FieldHighBound, 50), \n",
    "                                      T.linspace(self.FieldLowBound, self.FieldHighBound, 50))\n",
    "      ObservationPosition= T.stack(ObservationPosition)\n",
    "      xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "      xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "      yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "      color_aara = T.sqrt(xd**2+ yd**2)\n",
    "      fig, ax = plt.subplots(1,1)\n",
    "      cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "      fig.colorbar(cp)\n",
    "      plt.rcParams['figure.dpi'] = 250\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class Environment(ABC):  \n",
    "\n",
    "  class State:\n",
    "      pass\n",
    "  InitialState: State \n",
    "  CurrentState: State \n",
    "\n",
    "  @abstractmethod\n",
    "  def TransitionModel(self, State: State, Action)-> State:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def RewardModel(self, State: State, Action, NextState: State, TerminalSignal: bool)-> float:\n",
    "      '''This is a scalar performance metric.'''\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def StateTransition(self, State: State, Action)-> tuple[float, State, bool]:\n",
    "      ...\n",
    "\n",
    "  @abstractmethod\n",
    "  def SampleTrajectory(self, RunDuration: float)-> list[State]:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def TrajectoryValue(self, Trajectory: list[State])-> float:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Reset(self):\n",
    "      ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(Environment): \n",
    "  Field: Field\n",
    "  ChargedParticle: Particle\n",
    "  Target: T.Tensor\n",
    "  DistanceWeight: float= 1.0\n",
    "  EnergyWeight: float= -1.0\n",
    "  TerminalSignalWeight: float= -1000.0\n",
    "  CurrentTime: float = 0.0# s\n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Velocity and the Field Strength if experiences at its Position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: T.Tensor # m\n",
    "    Velocity: T.Tensor #kg*m/s\n",
    "    \n",
    "    def StateDynamics(self):\n",
    "      pass\n",
    "    def Vector(self):\n",
    "      return T.cat([self.Position, self.Velocity])\n",
    "  InitialState: State = None\n",
    "  CurrentState: State = None\n",
    "  def __post_init__(self):\n",
    "    if self.InitialState is None:\n",
    "        self.InitialState= self.RandomState()\n",
    "    self.CurrentState= self.InitialState\n",
    "\n",
    "  @EnforceMethodTyping\n",
    "  def TransitionModel(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval:float= 1.0, Resolution: int=100)-> State:\n",
    "    '''Outputs the state of the system after taking an action(applying a constant force for *TimeInterval* seconds)'''\n",
    "    CurrentVelocity= State.Velocity\n",
    "    CurrrentPosition= State.Position\n",
    "    TimeTaken= 0\n",
    "    while TimeTaken< TimeInterval:\n",
    "        CurrentVelocity = CurrentVelocity + ((self.ChargedParticle.Charge* self.Field(CurrrentPosition))+Action)/self.ChargedParticle.Mass*(TimeInterval/Resolution)\n",
    "        CurrrentPosition= CurrrentPosition+ (CurrentVelocity)*(TimeInterval/Resolution)\n",
    "        TimeTaken+= (TimeInterval/Resolution)\n",
    "    return self.State(CurrrentPosition, CurrentVelocity)\n",
    "  \n",
    "  def RewardModel(self, State: State, Action: T.Tensor, TerminalSignal: bool)-> float:\n",
    "      '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "      DistanceFromTarget= T.norm(State.Position-self.Target)\n",
    "      EnergyConsumed= T.norm(Action)\n",
    "      Cost= self.DistanceWeight* DistanceFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal\n",
    "      return Cost.item()\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      '''This method determines if the state is within the viable learning region of the environment: Constraints'''\n",
    "      WithinXBound= -10. <= State.Position[0] <= 10.\n",
    "      WithinYBound= -10. <= State.Position[1] <= 10. \n",
    "      WithinVelocityBound= T.norm(State.Velocity) < 10. \n",
    "      GoalAchieved= T.equal(State.Position, self.Target)\n",
    "      if WithinXBound and WithinYBound and WithinVelocityBound and not GoalAchieved: \n",
    "          return False    \n",
    "      else:\n",
    "          return True\n",
    "  \n",
    "  def StateTransition(self, State: State= CurrentState, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval: float= 1.0):\n",
    "      'Outputs the state of the system after taking an action, the reward ocurring from the transition and the terminal signal'\n",
    "      NextState= self.TransitionModel(State, Action, TimeInterval=TimeInterval)\n",
    "      TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "      Reward= self.RewardModel(State, Action,TerminalSignal)\n",
    "      return NextState, Reward, TerminalSignal\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def RandomState(self)->State:\n",
    "      '''This method generates a random state within the viable learning region'''\n",
    "      RandomPosition= T.tensor([np.random.uniform(-10., 10.), \n",
    "                                np.random.uniform(-10., 10.)])\n",
    "      RandomVelocity= T.zeros_like(RandomPosition)\n",
    "      return self.State(RandomPosition, RandomVelocity)\n",
    "\n",
    "  def SampleTrajectory(self, RunDuration: float, Policy: Optional[Callable]= None, TimeStep: int=0.1):\n",
    "    Time= [0]\n",
    "    State= self.CurrentState\n",
    "    StateTrajectory= []\n",
    "    ActionTrajectory= []\n",
    "    while Time[-1]<RunDuration: \n",
    "        StateTrajectory.append(State)\n",
    "        if Policy is Callable:\n",
    "          Action = Policy(State)\n",
    "        else:\n",
    "           Action = T.randn(2)\n",
    "        ActionTrajectory.append(Action)\n",
    "        State= self.TransitionModel(State, Action, TimeInterval= TimeStep) \n",
    "        Time.append(Time[-1]+TimeStep) \n",
    "    return StateTrajectory, ActionTrajectory, Time\n",
    "\n",
    "  def PlotTrajectory(self, StateTrajectory, Time): \n",
    "      PositionPath= [State.Position for State in StateTrajectory]\n",
    "      VelocityPath= [State.Velocity for State in StateTrajectory]\n",
    "      PositionTrajectory= T.stack(PositionPath).transpose(dim0=0, dim1=1)\n",
    "      VelocityTrajectory= T.stack(VelocityPath).transpose(dim0=0, dim1=1)\n",
    "      plt.plot(PositionTrajectory[0], PositionTrajectory[1])\n",
    "      plt.plot(PositionTrajectory[0][0], PositionTrajectory[1][0], 'ko')\n",
    "      plt.plot(PositionTrajectory[0][-1], PositionTrajectory[1][-1], 'r*')\n",
    "      plt.xlim(-100,100)\n",
    "      plt.ylim(-100,100)\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "  def TrajectoryValue(self, StateTrajectory: list[State], ActionTrajectory, Time)-> float:\n",
    "      Value= 0\n",
    "      TimeInterval= (Time[-1]-Time[0])/len(Time)\n",
    "      for State, Action in zip(StateTrajectory, ActionTrajectory):\n",
    "         Value= Value+ (T.norm(State.Position-self.Target)+T.norm(Action))* TimeInterval\n",
    "      return Value\n",
    "  def Reset(self):\n",
    "      self.CurrentState= self.InitialState\n",
    "      self.CurrentTime= 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(ABC):\n",
    "  AgentEnvironment: Environment\n",
    "  ControlFrequency: float\n",
    "  @abstractmethod\n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Observe(self)-> T.Tensor:\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def Learn(self):\n",
    "      'Improves  the agent by updating its models'\n",
    "      ...\n",
    "  @abstractmethod\n",
    "  def LearningAlgorithm(self):\n",
    "      ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__() \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims+n_actions, fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = T.cat([state, action], dim=-1)\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims , fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, n_actions))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = T.relu(self.bn1(self.fc1(state)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only= True)\n",
    "class DDPGAgent(Agent):\n",
    "  ObservationDimensions: int\n",
    "  ActionDimensions: int\n",
    "  AgentEnvironment: Environment\n",
    "  Layer1Size: int= 100\n",
    "  Layer2Size: int= 50\n",
    "  ActorLearningRate= 0.000025\n",
    "  CriticLearningRate= 0.00025\n",
    "  BufferSize: int= 128\n",
    "  BatchSize: int = 64\n",
    "  EpisodeDuration: int= 20\n",
    "  NumberOfEpisodes: int= 50\n",
    "  ControlFrequency: float= 1.0\n",
    "  DiscountRate: float = 0.99\n",
    "  SoftUpdateRate: float= 0.001\n",
    "  Actor: ActorNetwork = NotImplemented\n",
    "  Critic: CriticNetwork = NotImplemented\n",
    "  TargetActor: ActorNetwork = NotImplemented\n",
    "  TargetCritic: CriticNetwork = NotImplemented\n",
    "  ReplayBuffer: deque = NotImplemented\n",
    "  Noise= NotImplemented\n",
    "  \n",
    "  def __post_init__(self):\n",
    "    self.Actor = ActorNetwork(self.ActorLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='Actor')\n",
    "    self.Critic = CriticNetwork(self.CriticLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='Critic')\n",
    "    self.TargetActor = ActorNetwork(self.ActorLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='TargetActor')\n",
    "    self.TargetCritic = CriticNetwork(self.CriticLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='TargetCritic')\n",
    "    for target_param, param in zip(self.TargetActor.parameters(), self.Actor.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "    for target_param, param in zip(self.TargetCritic.parameters(), self.Critic.parameters()):\n",
    "        target_param.data.copy_(param.data) \n",
    "    self.ReplayBuffer = deque(maxlen=self.BufferSize)\n",
    "    self.Noise = OUActionNoise(mu=np.zeros(self.ActionDimensions))\n",
    "\n",
    "  def Observe(self, State)-> T.Tensor:  \n",
    "    if isinstance(State, (tuple, list)):\n",
    "        Observation= [] \n",
    "        for i in State:\n",
    "          Observation.append(self.Observe(i)) \n",
    "        Observation= T.stack(Observation)\n",
    "    elif isinstance(State, self.AgentEnvironment.State):\n",
    "        Observation= T.cat([State.Position, State.Velocity])\n",
    "    return Observation\n",
    "  \n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "    self.Actor.eval()\n",
    "    Observation = T.tensor(Observation, dtype=T.float).to(self.Actor.device)\n",
    "    Action = self.Actor.forward(Observation).to(self.Actor.device)\n",
    "    NoisyAction = 1e-3 *(Action + T.tensor(self.Noise(), dtype=T.float).to(self.Actor.device))\n",
    "    self.Actor.train()\n",
    "    return NoisyAction.cpu().detach()\n",
    "\n",
    "  def Learn(self):\n",
    "    \"  Updates target network with online model parameters\"\n",
    "    if len(self.ReplayBuffer) < self.BatchSize:\n",
    "        return\n",
    "\n",
    "    Batch = random.sample(self.ReplayBuffer, self.BatchSize)\n",
    "    States, Actions, NextStates, Rewards, TerminalSignals = zip(*Batch)\n",
    "\n",
    "    States = T.stack(States).to(self.Critic.device)\n",
    "    Actions = T.stack(Actions).to(self.Critic.device)\n",
    "    Rewards = T.tensor(Rewards, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    NextStates =  T.stack(NextStates).to(self.Critic.device)\n",
    "    TerminalSignals = T.tensor(TerminalSignals, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    \n",
    "    self.TargetActor.eval()\n",
    "    self.TargetCritic.eval()\n",
    "    self.Critic.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      TargetActions = self.TargetActor.forward(NextStates)\n",
    "      CriticValue = self.TargetCritic.forward(NextStates, TargetActions)\n",
    "      TargetQValue = Rewards + self.DiscountRate * CriticValue * (1 - TerminalSignals)\n",
    "    \n",
    "    ExpectedQValue = self.Critic.forward(States, Actions)\n",
    "    CriticLoss = nn.MSELoss()(ExpectedQValue, TargetQValue.detach())\n",
    "    self.Critic.train()\n",
    "    self.Critic.optimizer.zero_grad()\n",
    "    CriticLoss.backward()\n",
    "    self.Critic.optimizer.step()\n",
    "\n",
    "    self.Actor.eval()\n",
    "    self.Critic.eval()\n",
    "\n",
    "    PredictedAction = self.Actor.forward(States)\n",
    "    PredictedReward = -self.Critic.forward(States, PredictedAction)\n",
    "\n",
    "    ActorLoss = -T.mean(PredictedReward)\n",
    "    self.Actor.train()\n",
    "    self.Actor.optimizer.zero_grad()\n",
    "    ActorLoss.backward()\n",
    "    self.Actor.optimizer.step()\n",
    "    \n",
    "    self.Actor.eval()\n",
    "    for target_param, param in zip(self.TargetActor.parameters(), self.Actor.parameters()):\n",
    "        target_param.data.copy_(self.SoftUpdateRate * param.data + (1 - self.SoftUpdateRate) * target_param.data)\n",
    "\n",
    "    for target_param, param in zip(self.TargetCritic.parameters(), self.Critic.parameters()):\n",
    "        target_param.data.copy_(self.SoftUpdateRate* param.data + (1 - self.SoftUpdateRate) * target_param.data)\n",
    "\n",
    "  def LearningAlgorithm(self):\n",
    "    # self.Actor.load_checkpoint()\n",
    "    # self.Critic.load_checkpoint()\n",
    "    # self.TargetActor.load_checkpoint()\n",
    "    # self.TargetCritic.load_checkpoint()\n",
    "    ReturnHistory = []\n",
    "    for _ in range(self.NumberOfEpisodes):\n",
    "        self.AgentEnvironment.Reset()\n",
    "        IsDone = False\n",
    "        Return = 0\n",
    "        for _ in range(self.EpisodeDuration):\n",
    "          print('CurrentState:', self.AgentEnvironment.CurrentState)\n",
    "          Observation= self.Observe(self.AgentEnvironment.CurrentState)\n",
    "          print('Observation:', Observation)\n",
    "          Action = self.Act(Observation) \n",
    "          print('Action:', Action)\n",
    "          NextState, Reward, IsDone= self.AgentEnvironment.StateTransition(self.AgentEnvironment.CurrentState, Action, TimeInterval=self.ControlFrequency) \n",
    "          print('NextState:', NextState)\n",
    "          print('Reward:', Reward)\n",
    "          print('IsDone:', IsDone)\n",
    "          self.ReplayBuffer.append((self.AgentEnvironment.CurrentState.Vector(), Action, NextState.Vector(), Reward, int(IsDone)))\n",
    "          print('ReplayBuffer:', self.ReplayBuffer)\n",
    "          self.Learn()\n",
    "          Return += Reward\n",
    "          self.AgentEnvironment.CurrentState = NextState\n",
    "        ReturnHistory.append(Return)\n",
    "    plt.plot(ReturnHistory)\n",
    "    self.Actor.save_checkpoint()\n",
    "    self.Critic.save_checkpoint()\n",
    "    self.TargetActor.save_checkpoint()\n",
    "    self.TargetCritic.save_checkpoint()\n",
    "    return ReturnHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObservationDimensions, layer1_size, layer2_size, n_actions= 4, 10, 5, 2\n",
    "NegativeCharge= Particle(Mass=1.0, Charge= -1e-6)\n",
    "PositiveCharge= Particle(Mass=1.0, Charge= 1e-6)\n",
    "Sources = {\"Particle\": [NegativeCharge],\n",
    "          \"Position\": [T.tensor([10.0, 0.0])]}\n",
    "TestElectricField= ElectricField(FieldSources=Sources)\n",
    "PositiveChargeInElectricField= ParticleInField(Field=TestElectricField, ChargedParticle=PositiveCharge, Target=T.tensor([50.0, 50.0]))\n",
    "TestDDPGAgent= DDPGAgent(ObservationDimensions=ObservationDimensions, ActionDimensions=n_actions, AgentEnvironment=PositiveChargeInElectricField)\n",
    "obs= TestDDPGAgent.Observe([PositiveChargeInElectricField.CurrentState])\n",
    "TestDDPGAgent.LearningAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateTrajectory, ActionTrajectory, Time= PositiveChargeInElectricField.SampleTrajectory(100, TestDDPGAgent.Actor)\n",
    "PositiveChargeInElectricField.PlotTrajectory(StateTrajectory, Time)\n",
    "StateTrajectory[0], PositiveChargeInElectricField.TrajectoryValue(StateTrajectory, ActionTrajectory, Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(9, dtype= torch.float) - 4\n",
    "b = a.reshape((3, 3))\n",
    "a, b, torch.norm(a), torch.norm(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
