{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Replay Buffer'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch \n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Source(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Position: torch.Tensor # m\n",
    "    Charge: float #C\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def ElectricField(FieldSources: list, ObservationPosition: torch.Tensor)->torch.Tensor:\n",
    "    'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    for FieldSource in FieldSources:\n",
    "        if type(FieldSource) != Source:\n",
    "            raise TypeError(\"The input is not valid\")\n",
    "    if type(ObservationPosition[0]) != type(ObservationPosition[1]):\n",
    "         raise TypeError(\"Incompatible Reference point data types\")\n",
    "    elif type(ObservationPosition[0]) != torch.Tensor:\n",
    "        raise TypeError(\"Invalid Reference point data type\")\n",
    "    elif ObservationPosition[0].size()!=ObservationPosition[1].size():\n",
    "        raise TypeError(\"Incompatible Reference point dimensions\")\n",
    "    else: \n",
    "        ElectricFieldVector = torch.zeros_like(ObservationPosition)\n",
    "    for FieldSource in FieldSources:\n",
    "        PositionMatrices= torch.stack([torch.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                        torch.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "        DisplacemnetVector = ObservationPosition - PositionMatrices\n",
    "        DisplacementMagnitude = torch.sqrt(DisplacemnetVector[0]**2 +DisplacemnetVector[1]**2)  # Magnitude of the displacement vector\n",
    "        ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacemnetVector\n",
    "    return ElectricFieldVector #N/C or V/m\n",
    "\n",
    "@EnforceFunctionTyping\n",
    "def PlotField(Sources: list, ObservationPosition: torch.Tensor):\n",
    "    'This funtion plots the 2D electric vector field'\n",
    "    xd, yd = ElectricField(Sources, ObservationPosition)\n",
    "    xd = xd / torch.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / torch.sqrt(xd**2 + yd**2)\n",
    "    color_aara = torch.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n",
    "\n",
    "@dataclass \n",
    "class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "       These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: torch.Tensor # m\n",
    "    FieldStrength: torch.Tensor #N/C or V/m\n",
    "    Momentum: torch.Tensor #kg*m/s\n",
    "    def Unwrap(self)->torch.Tensor:\n",
    "        '''This function converts the state parameters to a single tensor for processing. '''\n",
    "        return torch.cat([self.Position, \n",
    "                          self.FieldStrength,\n",
    "                          self.Momentum])\n",
    "\n",
    "@dataclass \n",
    "class ReplayBuffer(EnforceClassTyping):\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    @EnforceMethodTyping\n",
    "    def AddExperience(self, State: State, Action: torch.Tensor, NextState: State, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "    @EnforceMethodTyping\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [torch.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [torch.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= torch.stack(SampledStates)\n",
    "            ActionBatch= torch.stack(SampledActions)\n",
    "            NextStateBatch= torch.stack(SampledNextStates)\n",
    "            RewardsBatch= torch.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= torch.stack(SampledTerminalSignals)\n",
    "        else:\n",
    "            raise ValueError('BatchSize too big')\n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
