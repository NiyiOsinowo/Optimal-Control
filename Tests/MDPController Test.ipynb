{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/niyi/Documents/GitHub/Optimal-Control/Tools')\n",
    "from EnforceTyping import EnforceClassTyping, EnforceMethodTyping\n",
    "from ParticlesandFields import Particle, Field\n",
    "from Environment import Environment\n",
    "from MDPController import MDPController\n",
    "import OUNoise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only= True)\n",
    "class ElectricField(Field):\n",
    "  FieldSources: Dict\n",
    "\n",
    "  def __call__(self, ObservationPosition: np.ndarray)->np.ndarray:\n",
    "      return self.FieldStrength(ObservationPosition)\n",
    "  @EnforceMethodTyping\n",
    "  def FieldStrength(self, ObservationPosition: np.ndarray)->np.ndarray:\n",
    "    'This function outputs the field strength due to Field Sources experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    assert len(self.FieldSources[\"Particle\"]) == len(self.FieldSources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "    for FieldSource, _ in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      assert isinstance(FieldSource, Particle),  \"The FieldSource is not a Particle\"\n",
    "    ElectricFieldVector = np.zeros_like(ObservationPosition)\n",
    "    for FieldSource, SourcePosition in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      PositionMatrices= np.stack([np.ones_like(ObservationPosition[0])* SourcePosition[0].item(), \n",
    "                                np.ones_like(ObservationPosition[1])* SourcePosition[1].item()])\n",
    "      DisplacementVector = ObservationPosition - PositionMatrices\n",
    "      DisplacementMagnitude = np.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "      ElectricFieldVector += (DisplacementVector * FieldSource.Charge) / DisplacementMagnitude**2\n",
    "    return CoulombConstant * ElectricFieldVector #N/C or V/m\n",
    "  @EnforceMethodTyping\n",
    "  def FieldPotential(self, InitialPosition: np.ndarray, FinalPosition: np.ndarray, resolution: int= 5000)-> float:\n",
    "      '''This method determines the amount of work required to get one position to another in the field'''\n",
    "      XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "      YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "      XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "      YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "      WorkDone = 0\n",
    "      for i in range(resolution):\n",
    "          PositionFieldStrength = self.FieldStrength(np.array([XPositions[i], YPositions[i]]))\n",
    "          WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "      return WorkDone\n",
    "  @EnforceMethodTyping\n",
    "  def PlotField(self, LowBound= -20, HighBound= 20):\n",
    "      'This funtion plots the 2D electric vector field'\n",
    "      ObservationPosition= np.meshgrid(np.linspace(LowBound, HighBound, 50), \n",
    "                                      np.linspace(LowBound, HighBound, 50))\n",
    "      ObservationPosition= np.stack(ObservationPosition)\n",
    "      xd, yd = self.FieldStrength(ObservationPosition)\n",
    "      xd = xd / np.sqrt(xd**2 + yd**2)\n",
    "      yd = yd / np.sqrt(xd**2 + yd**2)\n",
    "      color_aara = np.sqrt(xd**2+ yd**2)\n",
    "      fig, ax = plt.subplots(1,1)\n",
    "      cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "      fig.colorbar(cp)\n",
    "      plt.rcParams['figure.dpi'] = 250\n",
    "      plt.show()\n",
    "  def Derivative(self, ObservationPosition):\n",
    "    'This function returns the derivative of the field at a given point'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    assert len(self.FieldSources[\"Particle\"]) == len(self.FieldSources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "    for FieldSource, _ in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      assert isinstance(FieldSource, Particle),  \"The FieldSource is not a Particle\"\n",
    "    ElectricFieldVector = np.zeros_like(ObservationPosition)\n",
    "    for FieldSource, SourcePosition in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      PositionMatrices= np.stack([np.ones_like(ObservationPosition[0])* SourcePosition[0].item(), \n",
    "                                np.ones_like(ObservationPosition[1])* SourcePosition[1].item()])\n",
    "      DisplacementVector = ObservationPosition - PositionMatrices\n",
    "      DisplacementMagnitude = np.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "      ElectricFieldVector += (DisplacementVector * FieldSource.Charge) / DisplacementMagnitude**1.5\n",
    "    return -CoulombConstant * ElectricFieldVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(Environment): \n",
    "  Field: Field\n",
    "  ChargedParticle: Particle\n",
    "  Target: T.Tensor\n",
    "  DistanceWeight: float= 1.0\n",
    "  EnergyWeight: float= -1.0\n",
    "  TerminalSignalWeight: float= -1000.0\n",
    "  CurrentTime: float = 0.0# s\n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Velocity and the Field Strength if experiences at its Position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: T.Tensor # m\n",
    "    Velocity: T.Tensor #kg*m/s\n",
    "    \n",
    "    def Vector(self):\n",
    "      return T.cat([self.Position, self.Velocity])\n",
    "  InitialState: State = None\n",
    "  CurrentState: State = None\n",
    "  def __post_init__(self):\n",
    "    if self.InitialState is None:\n",
    "        self.InitialState= self.RandomState()\n",
    "    self.CurrentState= self.InitialState\n",
    "\n",
    "  def StateDynamics(self, State: T.Tensor, Time: float, ControlForce: T.Tensor):\n",
    "    dxPosition, dyPosition = State[2], State[3]\n",
    "    Position= T.tensor([State[0], State[1]])\n",
    "    dxVelocity, dyVelocity = ((self.ChargedParticle.Charge* self.Field(Position))+ControlForce)/self.ChargedParticle.Mass\n",
    "\n",
    "    return np.array([dxPosition, dyPosition, dxVelocity, dyVelocity])\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def TransitionModel(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval:float= 1.0)-> State:\n",
    "    '''Outputs the state of the system after taking an action(applying a constant force for *TimeInterval* seconds)'''\n",
    "    Posx, Posy, Velx, Vely= integrate.odeint(self.StateDynamics, State.Vector(), [self.CurrentTime, self.CurrentTime+ TimeInterval], args=(Action,))[-1]\n",
    "    CurrrentPosition= T.tensor([Posx, Posy])\n",
    "    CurrentVelocity= T.tensor([Velx, Vely])\n",
    "    return self.State(CurrrentPosition, CurrentVelocity)\n",
    "  \n",
    "  def RewardModel(self, State: State, Action: T.Tensor, NextState: State, TerminalSignal: bool)-> float:\n",
    "      '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "      DistanceGainedFromTarget= T.norm(State.Position-self.Target)- T.norm(NextState.Position-self.Target) \n",
    "      EnergyConsumed= self.ChargedParticle.Charge* self.Field.FieldPotential(State.Position, NextState.Position)\n",
    "      Cost= self.DistanceWeight* DistanceGainedFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal\n",
    "      return Cost.item()\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      '''This method determines if the state is within the viable learning region of the environment: Constraints'''\n",
    "      WithinXBound= -10. <= State.Position[0] <= 10.\n",
    "      WithinYBound= -10. <= State.Position[1] <= 10. \n",
    "      WithinVelocityBound= T.norm(State.Velocity) < 10. \n",
    "      if WithinXBound and WithinYBound and WithinVelocityBound: \n",
    "          return False    \n",
    "      else:\n",
    "          return True\n",
    "  \n",
    "  def StateTransition(self, State: State= CurrentState, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval: float= 1.0):\n",
    "      'Outputs the state of the system after taking an action, the reward ocurring from the transition and the terminal signal'\n",
    "      NextState= self.TransitionModel(State, Action, TimeInterval=TimeInterval)\n",
    "      TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "      Reward= self.RewardModel(State, Action, NextState, TerminalSignal)\n",
    "      return NextState, Reward, TerminalSignal\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def RandomState(self)->State:\n",
    "      '''This method generates a random state within the viable learning region'''\n",
    "      RandomPosition= T.tensor([np.random.uniform(-10., 10.), \n",
    "                                np.random.uniform(-10., 10.)])\n",
    "      RandomVelocity= T.zeros_like(RandomPosition)\n",
    "      return self.State(RandomPosition, RandomVelocity)\n",
    "\n",
    "  def SampleTrajectory(self, RunDuration: float, Policy: Optional[Callable]= None, TimeStep: int=0.1):\n",
    "    Time= [0]\n",
    "    State= self.CurrentState\n",
    "    StateTrajectory= []\n",
    "    ActionTrajectory= []\n",
    "    while Time[-1]<RunDuration: \n",
    "      StateTrajectory.append(State)\n",
    "      if Policy is Callable:\n",
    "        Action = Policy(State)\n",
    "      else:\n",
    "          Action = T.randn(2)\n",
    "      ActionTrajectory.append(Action)\n",
    "      State= self.TransitionModel(State, Action, TimeInterval= TimeStep) \n",
    "      Time.append(Time[-1]+TimeStep) \n",
    "    return StateTrajectory, ActionTrajectory, Time\n",
    "\n",
    "  def PlotTrajectory(self, StateTrajectory, Time): \n",
    "      PositionPath= [State.Position for State in StateTrajectory]\n",
    "      VelocityPath= [State.Velocity for State in StateTrajectory]\n",
    "      PositionTrajectory= T.stack(PositionPath).transpose(dim0=0, dim1=1)\n",
    "      VelocityTrajectory= T.stack(VelocityPath).transpose(dim0=0, dim1=1)\n",
    "      plt.plot(PositionTrajectory[0], PositionTrajectory[1])\n",
    "      plt.plot(PositionTrajectory[0][0], PositionTrajectory[1][0], 'ko')\n",
    "      plt.plot(PositionTrajectory[0][-1], PositionTrajectory[1][-1], 'r*')\n",
    "      plt.xlim(-10,10)\n",
    "      plt.ylim(-10,10)\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "  def TrajectoryValue(self, StateTrajectory: list[State], ActionTrajectory, Time)-> float:\n",
    "      Value= 0\n",
    "      TimeInterval= (Time[-1]-Time[0])/len(Time)\n",
    "      for State, Action in zip(StateTrajectory, ActionTrajectory):\n",
    "         Value= Value+ (T.norm(State.Position-self.Target)+T.norm(Action))* TimeInterval\n",
    "      return Value\n",
    "  \n",
    "  def Reset(self):\n",
    "      self.CurrentState= self.InitialState\n",
    "      self.CurrentTime= 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__() \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims+n_actions, fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = T.cat([state, action], dim=-1)\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims , fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, n_actions))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = T.relu(self.bn1(self.fc1(state)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only= True)\n",
    "class DDPGAgent(MDPController):\n",
    "  ObservationDimensions: int\n",
    "  ActionDimensions: int\n",
    "  MDPEnvironment: Environment\n",
    "  Layer1Size: int= 100\n",
    "  Layer2Size: int= 50\n",
    "  ActorLearningRate= 0.000025\n",
    "  CriticLearningRate= 0.00025\n",
    "  BufferSize: int= 128\n",
    "  BatchSize: int = 64\n",
    "  EpisodeDuration: int= 20\n",
    "  NumberOfEpisodes: int= 50\n",
    "  ControlFrequency: float= 1.0\n",
    "  DiscountRate: float = 0.99\n",
    "  SoftUpdateRate: float= 0.001\n",
    "  Actor: ActorNetwork = NotImplemented\n",
    "  Critic: CriticNetwork = NotImplemented\n",
    "  TargetActor: ActorNetwork = NotImplemented\n",
    "  TargetCritic: CriticNetwork = NotImplemented\n",
    "  ReplayBuffer: deque = NotImplemented\n",
    "  Noise= NotImplemented\n",
    "  \n",
    "  def __post_init__(self):\n",
    "    self.Actor = ActorNetwork(self.ActorLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='Actor')\n",
    "    self.Critic = CriticNetwork(self.CriticLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='Critic')\n",
    "    self.TargetActor = ActorNetwork(self.ActorLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='TargetActor')\n",
    "    self.TargetCritic = CriticNetwork(self.CriticLearningRate, self.ObservationDimensions, self.Layer1Size, self.Layer2Size, n_actions=self.ActionDimensions, name='TargetCritic')\n",
    "    for target_param, param in zip(self.TargetActor.parameters(), self.Actor.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "    for target_param, param in zip(self.TargetCritic.parameters(), self.Critic.parameters()):\n",
    "        target_param.data.copy_(param.data) \n",
    "    self.ReplayBuffer = deque(maxlen=self.BufferSize)\n",
    "    self.Noise = OUNoise.OUNoise(mu=np.zeros(self.ActionDimensions))\n",
    "\n",
    "  def Observe(self, State)-> T.Tensor:  \n",
    "    if isinstance(State, (tuple, list)):\n",
    "        Observation= [] \n",
    "        for i in State:\n",
    "          Observation.append(self.Observe(i)) \n",
    "        Observation= T.stack(Observation)\n",
    "    elif isinstance(State, self.MDPEnvironment.State):\n",
    "        Observation= T.cat([State.Position, State.Velocity])\n",
    "    return Observation\n",
    "  \n",
    "  def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "    self.Actor.eval()\n",
    "    Observation = T.tensor(Observation, dtype=T.float).to(self.Actor.device)\n",
    "    Action = self.Actor.forward(Observation).to(self.Actor.device)\n",
    "    NoisyAction = 1e-3 *(Action + T.tensor(self.Noise(), dtype=T.float).to(self.Actor.device))\n",
    "    self.Actor.train()\n",
    "    return NoisyAction.cpu().detach()\n",
    "\n",
    "  def Learn(self):\n",
    "    \"  Updates target network with online model parameters\"\n",
    "    if len(self.ReplayBuffer) < self.BatchSize:\n",
    "        return\n",
    "\n",
    "    Batch = random.sample(self.ReplayBuffer, self.BatchSize)\n",
    "    States, Actions, NextStates, Rewards, TerminalSignals = zip(*Batch)\n",
    "\n",
    "    States = T.stack(States).to(self.Critic.device)\n",
    "    Actions = T.stack(Actions).to(self.Critic.device)\n",
    "    Rewards = T.tensor(Rewards, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    NextStates =  T.stack(NextStates).to(self.Critic.device)\n",
    "    TerminalSignals = T.tensor(TerminalSignals, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "    \n",
    "    self.TargetActor.eval()\n",
    "    self.TargetCritic.eval()\n",
    "    self.Critic.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      TargetActions = self.TargetActor.forward(NextStates)\n",
    "      CriticValue = self.TargetCritic.forward(NextStates, TargetActions)\n",
    "      TargetQValue = Rewards + self.DiscountRate * CriticValue * (1 - TerminalSignals)\n",
    "    \n",
    "    ExpectedQValue = self.Critic.forward(States, Actions)\n",
    "    CriticLoss = nn.MSELoss()(ExpectedQValue, TargetQValue.detach())\n",
    "    self.Critic.train()\n",
    "    self.Critic.optimizer.zero_grad()\n",
    "    CriticLoss.backward()\n",
    "    self.Critic.optimizer.step()\n",
    "\n",
    "    self.Actor.eval()\n",
    "    self.Critic.eval()\n",
    "\n",
    "    PredictedAction = self.Actor.forward(States)\n",
    "    PredictedReward = -self.Critic.forward(States, PredictedAction)\n",
    "\n",
    "    ActorLoss = -T.mean(PredictedReward)\n",
    "    self.Actor.train()\n",
    "    self.Actor.optimizer.zero_grad()\n",
    "    ActorLoss.backward()\n",
    "    self.Actor.optimizer.step()\n",
    "    \n",
    "    self.Actor.eval()\n",
    "    for target_param, param in zip(self.TargetActor.parameters(), self.Actor.parameters()):\n",
    "        target_param.data.copy_(self.SoftUpdateRate * param.data + (1 - self.SoftUpdateRate) * target_param.data)\n",
    "\n",
    "    for target_param, param in zip(self.TargetCritic.parameters(), self.Critic.parameters()):\n",
    "        target_param.data.copy_(self.SoftUpdateRate* param.data + (1 - self.SoftUpdateRate) * target_param.data)\n",
    "\n",
    "  def LearningAlgorithm(self):\n",
    "    # self.Actor.load_checkpoint()\n",
    "    # self.Critic.load_checkpoint()\n",
    "    # self.TargetActor.load_checkpoint()\n",
    "    # self.TargetCritic.load_checkpoint()\n",
    "    ReturnHistory = []\n",
    "    for _ in range(self.NumberOfEpisodes):\n",
    "        self.MDPEnvironment.Reset()\n",
    "        IsDone = False\n",
    "        Return = 0\n",
    "        for _ in range(self.EpisodeDuration):\n",
    "          print('CurrentState:', self.MDPEnvironment.CurrentState)\n",
    "          Observation= self.Observe(self.MDPEnvironment.CurrentState)\n",
    "          print('Observation:', Observation)\n",
    "          Action = self.Act(Observation) \n",
    "          print('Action:', Action)\n",
    "          NextState, Reward, IsDone= self.MDPEnvironment.StateTransition(self.MDPEnvironment.CurrentState, Action, TimeInterval=self.ControlFrequency) \n",
    "          print('NextState:', NextState)\n",
    "          print('Reward:', Reward)\n",
    "          print('IsDone:', IsDone)\n",
    "          self.ReplayBuffer.append((self.MDPEnvironment.CurrentState.Vector(), Action, NextState.Vector(), Reward, int(IsDone)))\n",
    "          print('ReplayBuffer:', self.ReplayBuffer)\n",
    "          self.Learn()\n",
    "          Return += Reward\n",
    "          self.MDPEnvironment.CurrentState = NextState\n",
    "        ReturnHistory.append(Return)\n",
    "    plt.plot(ReturnHistory)\n",
    "    self.Actor.save_checkpoint()\n",
    "    self.Critic.save_checkpoint()\n",
    "    self.TargetActor.save_checkpoint()\n",
    "    self.TargetCritic.save_checkpoint()\n",
    "    return ReturnHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'OUNoise' has no attribute 'OUNoise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m TestElectricField\u001b[38;5;241m=\u001b[39m ElectricField(FieldSources\u001b[38;5;241m=\u001b[39mSources)\n\u001b[1;32m      7\u001b[0m PositiveChargeInElectricField\u001b[38;5;241m=\u001b[39m ParticleInField(Field\u001b[38;5;241m=\u001b[39mTestElectricField, ChargedParticle\u001b[38;5;241m=\u001b[39mPositiveCharge, Target\u001b[38;5;241m=\u001b[39mT\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m50.0\u001b[39m, \u001b[38;5;241m50.0\u001b[39m]))\n\u001b[0;32m----> 8\u001b[0m TestDDPGAgent\u001b[38;5;241m=\u001b[39m \u001b[43mDDPGAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mObservationDimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mObservationDimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mActionDimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMDPEnvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPositiveChargeInElectricField\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m obs\u001b[38;5;241m=\u001b[39m TestDDPGAgent\u001b[38;5;241m.\u001b[39mObserve([PositiveChargeInElectricField\u001b[38;5;241m.\u001b[39mCurrentState])\n\u001b[1;32m     10\u001b[0m TestDDPGAgent\u001b[38;5;241m.\u001b[39mLearningAlgorithm()\n",
      "File \u001b[0;32m<string>:20\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, MDPEnvironment, ObservationDimensions, ActionDimensions, Layer1Size, Layer2Size, BufferSize, BatchSize, EpisodeDuration, NumberOfEpisodes, ControlFrequency, DiscountRate, SoftUpdateRate, Actor, Critic, TargetActor, TargetCritic, ReplayBuffer)\u001b[0m\n",
      "Cell \u001b[0;32mIn[49], line 34\u001b[0m, in \u001b[0;36mDDPGAgent.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m     target_param\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(param\u001b[38;5;241m.\u001b[39mdata) \n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mReplayBuffer \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBufferSize)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNoise \u001b[38;5;241m=\u001b[39m \u001b[43mOUNoise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUNoise\u001b[49m(mu\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mActionDimensions))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'OUNoise' has no attribute 'OUNoise'"
     ]
    }
   ],
   "source": [
    "ObservationDimensions, layer1_size, layer2_size, n_actions= 4, 10, 5, 2\n",
    "NegativeCharge= Particle(Mass=1.0, Charge= -1e-6)\n",
    "PositiveCharge= Particle(Mass=1.0, Charge= 1e-6)\n",
    "Sources = {\"Particle\": [NegativeCharge],\n",
    "          \"Position\": [T.tensor([10.0, 0.0])]}\n",
    "TestElectricField= ElectricField(FieldSources=Sources)\n",
    "PositiveChargeInElectricField= ParticleInField(Field=TestElectricField, ChargedParticle=PositiveCharge, Target=T.tensor([50.0, 50.0]))\n",
    "TestDDPGAgent= DDPGAgent(ObservationDimensions=ObservationDimensions, ActionDimensions=n_actions, MDPEnvironment=PositiveChargeInElectricField)\n",
    "obs= TestDDPGAgent.Observe([PositiveChargeInElectricField.CurrentState])\n",
    "TestDDPGAgent.LearningAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateTrajectory, ActionTrajectory, Time= PositiveChargeInElectricField.SampleTrajectory(100, TestDDPGAgent.Actor)\n",
    "PositiveChargeInElectricField.PlotTrajectory(StateTrajectory, Time)\n",
    "StateTrajectory[0], PositiveChargeInElectricField.TrajectoryValue(StateTrajectory, ActionTrajectory, Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(9, dtype= torch.float) - 4\n",
    "b = a.reshape((3, 3))\n",
    "a, b, torch.norm(a), torch.norm(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
