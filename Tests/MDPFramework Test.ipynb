{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Tuple, Union, Optional\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrate\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mintegrate\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mT\u001b[49m\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mproperty\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/niyi/Documents/GitHub/Optimal-Control/Tools\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "import scipy.integrate as integrate\n",
    "T.Tensor.ndim = property(lambda self: len(self.shape))\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/niyi/Documents/GitHub/Optimal-Control/Tools')\n",
    "from EnforceTyping import EnforceClassTyping, EnforceMethodTyping\n",
    "from ParticlesandFields import *\n",
    "from MDPFramework import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only= True)\n",
    "class ElectroStaticField2D(ClassicalField):\n",
    "  \"\"\"\n",
    "  A class used to represent a 2D Electrostatic Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  FieldSources: Dict\n",
    "      a formatted string to print out what the animal says\n",
    "  Dimensionality: tuple\n",
    "      a tuple of the dimensionality of the field  \n",
    "\n",
    "  Methods\n",
    "  -------\n",
    "  Dynamics(self, ObservationPosition: np.ndarray, Time: float)-> np.ndarray:\n",
    "      Represents the value of the field at any given point(s) or time. \n",
    "  Potential(self, ObservationPosition: np.ndarray, Time: float)-> float:\n",
    "      Represents the potential dure to the field at a given position and/or time  \n",
    "  PotentialDifference(self, InitialPosition: np.ndarray, FinalPosition: np.ndarray, Time: float)-> float:\n",
    "      Represents the potential difference between two positions at a given time in the vector field   \n",
    "  Gradient(self, ObservationPosition: np.ndarray, Time: float)-> float:\n",
    "      Represents the gradient at a given position and/or time in the vector field \n",
    "  Curl(self, ObservationPosition: np.ndarray, Time: float)-> float:\n",
    "      Represents the curl at a given position and/or time in the vector field \n",
    "  Divergence(self, ObservationPosition: np.ndarray, Time: float)-> float:\n",
    "      Represents the divergence at a given position and/or time in the vector field\n",
    "  \"\"\"\n",
    "  FieldSources: Dict\n",
    "  Dimensionality: tuple = (2, )\n",
    "  def __call__(self, ObservationPosition: np.ndarray)->np.ndarray:\n",
    "      return self.Dynamics(ObservationPosition)\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def Dynamics(self, ObservationPosition: np.ndarray)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function outputs the field strength due to Field Sources experienced at any given point(s) or time. \n",
    "    This determines the physics of the field(a 2D Electricstatic Field in this case)\n",
    "\n",
    "    Args:\n",
    "        ObservationPosition (np.ndarray): The position.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The electric field strength vector at the given position.\n",
    "    \"\"\"\n",
    "    assert len(self.FieldSources[\"Particle\"]) == len(self.FieldSources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "    for FieldSource, _ in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      assert isinstance(FieldSource, ClassicalParticle),  \"The FieldSource is not a Particle\"\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    ElectricFieldVector = np.zeros_like(ObservationPosition)\n",
    "    for FieldSource, SourcePosition in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      PositionMatrices= np.broadcast_to(SourcePosition, reversed(ObservationPosition.shape)).T\n",
    "      DisplacementVector = ObservationPosition - PositionMatrices\n",
    "      DisplacementMagnitude = np.linalg.norm(DisplacementVector, axis=0)\n",
    "      ElectricFieldVector += (DisplacementVector * FieldSource.Charge) / DisplacementMagnitude**3\n",
    "    ElectricFieldVector= CoulombConstant * ElectricFieldVector\n",
    "    return np.round(ElectricFieldVector, 3) #N/C or V/m\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def Potential(self, ObservationPosition: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    Calculate the Potential(Voltage) at a position in the field.\n",
    "\n",
    "    Args:\n",
    "        ObservationPosition (np.ndarray): The position.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The Electric Potental at the given position.\n",
    "    \"\"\"\n",
    "    assert len(self.FieldSources[\"Particle\"]) == len(self.FieldSources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "    for FieldSource, _ in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      assert isinstance(FieldSource, ClassicalParticle),  \"The FieldSource is not a Particle\"\n",
    "\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    ElectricPotential = 0.0\n",
    "\n",
    "    for FieldSource, SourcePosition in zip(self.FieldSources[\"Particle\"], self.FieldSources[\"Position\"]):\n",
    "      PositionMatrices= np.broadcast_to(SourcePosition, reversed(ObservationPosition.shape)).T\n",
    "      DisplacementVector = ObservationPosition - PositionMatrices\n",
    "      DisplacementMagnitude = np.linalg.norm(DisplacementVector, axis=0)\n",
    "      ElectricPotential += FieldSource.Charge / DisplacementMagnitude\n",
    "\n",
    "    ElectricPotential= CoulombConstant * ElectricPotential\n",
    "    return np.round(ElectricPotential, 3) #V\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def PotentialDifference(self, InitialPosition: np.ndarray, FinalPosition: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    Calculate the potental difference between the initial position and the final position in the field.\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    assert InitialPosition.shape == self.Dimensionality, \"InitialPosition has the wrong dimensions\"\n",
    "    assert FinalPosition.shape == self.Dimensionality, \"FinalPosition has the wrong dimensions\"\n",
    "    PorentialDifference= self.Potential(InitialPosition)- self.Potential(FinalPosition)\n",
    "    return PorentialDifference\n",
    "\n",
    "  def Gradient(self, ObservationPosition: np.ndarray, Delta: float= 0.001)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the derivative of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    Gradient= np.zeros_like(ObservationPosition)\n",
    "    for i in range(len(ObservationPosition)):\n",
    "      di= np.zeros_like(ObservationPosition)\n",
    "      di[i, ] = di[i, ]+Delta\n",
    "      plusdi= ObservationPosition+ di\n",
    "      minusdi= ObservationPosition- di\n",
    "      Gradient[i]= (self.Dynamics(plusdi)- self.Dynamics(minusdi))[i]/ (2* Delta)\n",
    "    return Gradient\n",
    "  \n",
    "  def Curl(self, ObservationPosition: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the Curl of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  def Divergence(self, ObservationPosition: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the Divergence of the field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  @EnforceMethodTyping\n",
    "  def PlotField(self, LowBound= -20, HighBound= 20):\n",
    "    \"\"\"\n",
    "    This funtion plots the 2D electric vector field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    ObservationPosition= np.meshgrid(np.linspace(LowBound, HighBound, 25), \n",
    "                                    np.linspace(LowBound, HighBound, 25))\n",
    "    ObservationPosition= np.stack(ObservationPosition)\n",
    "    xd, yd = self.Dynamics(ObservationPosition)\n",
    "    xd = xd / np.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / np.sqrt(xd**2 + yd**2)\n",
    "    color_aara = np.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Particle in Field Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(MDPEnvironment): \n",
    "  Field: ClassicalField\n",
    "  ChargedParticle: ClassicalParticle\n",
    "  Target: np.ndarray # m\n",
    "  DistanceWeight: float= 1.0\n",
    "  EnergyWeight: float= -1.0\n",
    "  TerminalSignalWeight: float= -1000.0\n",
    "  CurrentTime: float = 0.0# s\n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Velocity and the Field Strength if experiences at its Position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: np.ndarray # m\n",
    "    Velocity: np.ndarray #m/s\n",
    "    \n",
    "    def Vector(self):\n",
    "      return np.concatenate([self.Position, self.Velocity])\n",
    "  InitialState: State = None\n",
    "  CurrentState: State = None\n",
    "  def __post_init__(self):\n",
    "    if self.InitialState is None:\n",
    "        self.InitialState= self.RandomState()\n",
    "    self.CurrentState= self.InitialState\n",
    "\n",
    "  def StateDynamics(self, State: np.ndarray, Time: float, ControlForce: np.ndarray):\n",
    "    dxPosition, dyPosition = State[2], State[3]\n",
    "    Position= np.array([State[0], State[1]])\n",
    "    dxVelocity, dyVelocity = ((self.ChargedParticle.Charge* self.Field(Position))+ControlForce)/self.ChargedParticle.Mass\n",
    "\n",
    "    return np.array([dxPosition, dyPosition, dxVelocity, dyVelocity])\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def TransitionModel(self, State: State, Action: np.ndarray= np.array([0.0, 0.0]), TimeInterval:float= 1.0)-> State:\n",
    "    '''Outputs the state of the system after taking an action(applying a constant force for *TimeInterval* seconds)'''\n",
    "    Posx, Posy, Velx, Vely= integrate.odeint(self.StateDynamics, State.Vector(), [self.CurrentTime, self.CurrentTime+ TimeInterval], args=(Action,))[-1]\n",
    "    CurrrentPosition= np.array([Posx, Posy])\n",
    "    CurrentVelocity= np.array([Velx, Vely])\n",
    "    return self.State(CurrrentPosition, CurrentVelocity)\n",
    "  \n",
    "  def RewardModel(self, State: State, Action: np.ndarray, NextState: State, TerminalSignal: bool)-> float:\n",
    "      '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "      DistanceGainedFromTarget= np.linalg.norm(State.Position-self.Target)- np.linalg.norm(NextState.Position-self.Target) \n",
    "      EnergyConsumed= self.ChargedParticle.Charge* self.Field.FieldPotential(State.Position, NextState.Position)\n",
    "      Cost= self.DistanceWeight* DistanceGainedFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal\n",
    "      return Cost.item()\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      '''This method determines if the state is within the viable learning region of the environment: Constraints'''\n",
    "      WithinXBound= -10. <= State.Position[0] <= 10.\n",
    "      WithinYBound= -10. <= State.Position[1] <= 10. \n",
    "      WithinVelocityBound= np.linalg.norm(State.Velocity) < 10. \n",
    "      if WithinXBound and WithinYBound and WithinVelocityBound: \n",
    "          return False    \n",
    "      else:\n",
    "          return True\n",
    "  \n",
    "  def StateTransition(self, State: State= CurrentState, Action: np.ndarray= np.array([0.0, 0.0]), TimeInterval: float= 1.0):\n",
    "      'Outputs the state of the system after taking an action, the reward ocurring from the transition and the terminal signal'\n",
    "      NextState= self.TransitionModel(State, Action, TimeInterval=TimeInterval)\n",
    "      TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "      Reward= self.RewardModel(State, Action, NextState, TerminalSignal)\n",
    "      return NextState, Reward, TerminalSignal\n",
    "  \n",
    "  @EnforceMethodTyping\n",
    "  def RandomState(self)->State:\n",
    "      '''This method generates a random state within the viable learning region'''\n",
    "      RandomPosition= np.array([np.random.uniform(-10., 10.), \n",
    "                                np.random.uniform(-10., 10.)])\n",
    "      RandomVelocity= np.zeros_like(RandomPosition)\n",
    "      return self.State(RandomPosition, RandomVelocity)\n",
    "\n",
    "  def SampleTrajectory(self, RunDuration: float, Policy: Optional[Callable]= None, TimeStep: int=0.1):\n",
    "    Time= [0]\n",
    "    State= self.CurrentState\n",
    "    StateTrajectory= []\n",
    "    ActionTrajectory= []\n",
    "    while Time[-1]<RunDuration: \n",
    "      StateTrajectory.append(State)\n",
    "      if Policy is Callable:\n",
    "        Action = Policy(State)\n",
    "      else:\n",
    "          Action = np.random.randn(2)\n",
    "      ActionTrajectory.append(Action)\n",
    "      State= self.TransitionModel(State, Action, TimeInterval= TimeStep) \n",
    "      Time.append(Time[-1]+TimeStep) \n",
    "    return StateTrajectory, ActionTrajectory, Time\n",
    "\n",
    "  def PlotTrajectory(self, StateTrajectory, Time): \n",
    "      PositionPath= [State.Position for State in StateTrajectory]\n",
    "      VelocityPath= [State.Velocity for State in StateTrajectory]\n",
    "      PositionTrajectory= np.stack(PositionPath).transpose(dim0=0, dim1=1)\n",
    "      VelocityTrajectory= np.stack(VelocityPath).transpose(dim0=0, dim1=1)\n",
    "      plt.plot(PositionTrajectory[0], PositionTrajectory[1])\n",
    "      plt.plot(PositionTrajectory[0][0], PositionTrajectory[1][0], 'ko')\n",
    "      plt.plot(PositionTrajectory[0][-1], PositionTrajectory[1][-1], 'r*')\n",
    "      plt.xlim(-10,10)\n",
    "      plt.ylim(-10,10)\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "  def TrajectoryValue(self, StateTrajectory: list[State], ActionTrajectory, Time)-> float:\n",
    "      Value= 0\n",
    "      TimeInterval= (Time[-1]-Time[0])/len(Time)\n",
    "      for State, Action in zip(StateTrajectory, ActionTrajectory):\n",
    "         Value= Value+ (np.linalg.norm(State.Position-self.Target)+np.linalg.norm(Action))* TimeInterval\n",
    "      return Value\n",
    "  \n",
    "  def Reset(self):\n",
    "      self.CurrentState= self.InitialState\n",
    "      self.CurrentTime= 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ndarray.transpose() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m StateTrajectory, ActionTrajectory, Time\u001b[38;5;241m=\u001b[39m PositiveChargeInElectricField\u001b[38;5;241m.\u001b[39mSampleTrajectory(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      9\u001b[0m Return\u001b[38;5;241m=\u001b[39m PositiveChargeInElectricField\u001b[38;5;241m.\u001b[39mTrajectoryValue(StateTrajectory, ActionTrajectory, Time)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mPositiveChargeInElectricField\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPlotTrajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStateTrajectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m Return\n",
      "Cell \u001b[0;32mIn[39], line 93\u001b[0m, in \u001b[0;36mParticleInField.PlotTrajectory\u001b[0;34m(self, StateTrajectory, Time)\u001b[0m\n\u001b[1;32m     91\u001b[0m PositionPath\u001b[38;5;241m=\u001b[39m [State\u001b[38;5;241m.\u001b[39mPosition \u001b[38;5;28;01mfor\u001b[39;00m State \u001b[38;5;129;01min\u001b[39;00m StateTrajectory]\n\u001b[1;32m     92\u001b[0m VelocityPath\u001b[38;5;241m=\u001b[39m [State\u001b[38;5;241m.\u001b[39mVelocity \u001b[38;5;28;01mfor\u001b[39;00m State \u001b[38;5;129;01min\u001b[39;00m StateTrajectory]\n\u001b[0;32m---> 93\u001b[0m PositionTrajectory\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPositionPath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m VelocityTrajectory\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(VelocityPath)\u001b[38;5;241m.\u001b[39mtranspose(dim0\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(PositionTrajectory[\u001b[38;5;241m0\u001b[39m], PositionTrajectory[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: ndarray.transpose() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "NegativeCharge= ClassicalParticle(Mass=1.0, Charge= -1e-9)\n",
    "PositiveCharge= ClassicalParticle(Mass=1.0, Charge= 1e-9)\n",
    "Sources = {\"Particle\": [NegativeCharge, PositiveCharge],\n",
    "           \"Position\": [np.array([1.0, 1.0]), np.array([-1.0, 1.0])]} \n",
    "TestElectricField= ElectroStaticField2D(FieldSources=Sources)\n",
    "\n",
    "PositiveChargeInElectricField= ParticleInField(Field=TestElectricField, ChargedParticle=PositiveCharge, Target=np.array([0.0, 0.0]))\n",
    "StateTrajectory, ActionTrajectory, Time= PositiveChargeInElectricField.SampleTrajectory(50)\n",
    "Return= PositiveChargeInElectricField.TrajectoryValue(StateTrajectory, ActionTrajectory, Time)\n",
    "PositiveChargeInElectricField.PlotTrajectory(StateTrajectory, Time)\n",
    "Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAgent(MDPController):\n",
    "  MDPEnvironment: MDPEnvironment\n",
    "  Policy: Callable\n",
    "\n",
    "  def Act(self, Observation: np.ndarray)-> np.ndarray:\n",
    "    ...\n",
    "      \n",
    "  def Observe(self)-> np.ndarray:\n",
    "    ...\n",
    "\n",
    "  def Learn(self):\n",
    "    'Improves  the MDPController by updating its models'\n",
    "    ...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
