{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "import scipy.integrate as integrate\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/niyi/Documents/GitHub/Optimal-Control/Tools')\n",
    "from EnforceTyping import EnforceClassTyping, enforce_method_typing, enforce_function_typing\n",
    "from ParticlesandFields import ClassicalField, ClassicalParticle\n",
    "from MDPFramework import MDPEnvironment, MDPController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ElectrostaticField2D(ClassicalField):\n",
    "  \"\"\"\n",
    "  A class used to represent a 2D Electrostatic Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  field_sources: dict\n",
    "      a formatted string to print out what the animal says\n",
    "  dimensionality: tuple\n",
    "      a tuple of the dimensionality of the field  \n",
    "\n",
    "  Methods\n",
    "  -------\n",
    "  dynamics(self, observation_position: np.ndarray, time: float) -> np.ndarray:\n",
    "      Represents the value of the field at any given point(s) or time. \n",
    "  potential(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential due to the field at a given position and/or time  \n",
    "  potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential difference between two positions at a given time in the vector field   \n",
    "  gradient(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the gradient at a given position and/or time in the vector field \n",
    "  curl(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the curl at a given position and/or time in the vector field \n",
    "  divergence(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the divergence at a given position and/or time in the vector field\n",
    "  \"\"\"\n",
    "  field_sources: dict\n",
    "  dimensionality: tuple = (2,)\n",
    "\n",
    "  def __call__(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      return self.dynamics(observation_position)\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def dynamics(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      \"\"\"\n",
    "      This function outputs the field strength due to field sources experienced at any given point(s) or time. \n",
    "      This determines the physics of the field (a 2D Electricstatic Field in this case)\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric field strength vector at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_field_vector = np.zeros_like(observation_position)\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_field_vector += (displacement_vector * field_source.charge) / displacement_magnitude**3\n",
    "      electric_field_vector = coulomb_constant * electric_field_vector\n",
    "      return np.round(electric_field_vector, 3)  # N/C or V/m\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential(self, observation_position: np.ndarray) -> float:\n",
    "      \"\"\"\n",
    "      Calculate the potential (voltage) at a position in the field.\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric potential at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "  \n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_potential = 0.0\n",
    "\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_potential += field_source.charge / displacement_magnitude\n",
    "\n",
    "      electric_potential = coulomb_constant * electric_potential\n",
    "      return np.round(electric_potential, 3)  # V\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the potential difference between the initial position and the final position in the field.\n",
    "\n",
    "    Args:\n",
    "        initial_position (np.ndarray): The starting position.\n",
    "        final_position (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    assert initial_position.shape == self.dimensionality, \"initial_position has the wrong dimensions\"\n",
    "    assert final_position.shape == self.dimensionality, \"final_position has the wrong dimensions\"\n",
    "    PorentialDifference= self.potential(initial_position)- self.potential(final_position)\n",
    "    return PorentialDifference\n",
    "\n",
    "  def gradient(self, observation_position: np.ndarray, delta: float= 0.001)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the derivative of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    gradient= np.zeros_like(observation_position)\n",
    "    for i in range(len(observation_position)):\n",
    "      di= np.zeros_like(observation_position)\n",
    "      di[i, ] = di[i, ]+delta\n",
    "      plusdi= observation_position+ di\n",
    "      minusdi= observation_position- di\n",
    "      gradient[i]= (self.dynamics(plusdi)- self.dynamics(minusdi))[i]/ (2* delta)\n",
    "    return gradient\n",
    "  \n",
    "  def curl(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the curl of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  def divergence(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the divergence of the field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  @enforce_method_typing\n",
    "  def plot_field(self, low_bound= -20, high_bound= 20):\n",
    "    \"\"\"\n",
    "    This funtion plots the 2D electric vector field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    observation_position= np.meshgrid(np.linspace(low_bound, high_bound, 25), \n",
    "                                    np.linspace(low_bound, high_bound, 25))\n",
    "    observation_position= np.stack(observation_position)\n",
    "    xd, yd = self.dynamics(observation_position)\n",
    "    xd = xd / np.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / np.sqrt(xd**2 + yd**2)\n",
    "    color_aara = np.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(observation_position[0],observation_position[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Particle in Field Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(MDPEnvironment): \n",
    "  field: ClassicalField\n",
    "  charged_particle: ClassicalParticle\n",
    "  target: np.ndarray # m\n",
    "  distance_weight: float= 1.0\n",
    "  energy_weight: float= -1.0\n",
    "  terminal_signal_weight: float= -1000.0\n",
    "  current_time: float = 0.0# s\n",
    "  \n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its position, velocity and the Field Strength if experiences at its position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    position: np.ndarray # m\n",
    "    velocity: np.ndarray #m/s\n",
    "    \n",
    "    def vector(self):\n",
    "      return np.concatenate([self.position, self.velocity])\n",
    "    \n",
    "  initial_state: State = None\n",
    "  current_state: State = None\n",
    "\n",
    "  def __post_init__(self):\n",
    "    if self.initial_state is None:\n",
    "        self.initial_state= self.random_state()\n",
    "    self.current_state= self.initial_state\n",
    "\n",
    "  def state_dynamics(self, state: np.ndarray, time: float, control_force: np.ndarray):\n",
    "    dxposition, dyposition = state[2], state[3]\n",
    "    position= np.array([state[0], state[1]])\n",
    "    dxvelocity, dyvelocity = ((self.charged_particle.charge* self.field(position))+control_force)/self.charged_particle.mass\n",
    "\n",
    "    return np.array([dxposition, dyposition, dxvelocity, dyvelocity])\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def TransitionModel(self, State: State, Action: np.ndarray= np.array([0.0, 0.0]), timeInterval:float= 1.0)-> State:\n",
    "    '''Outputs the state of the system after taking an action(applying a constant force for *timeInterval* seconds)'''\n",
    "    Posx, Posy, Velx, Vely= integrate.odeint(self.state_dynamics, State.vector(), [self.current_time, self.current_time+ timeInterval], args=(Action,))[-1]\n",
    "    Currrentposition= np.array([Posx, Posy])\n",
    "    Currentvelocity= np.array([Velx, Vely])\n",
    "    return self.State(Currrentposition, Currentvelocity)\n",
    "  \n",
    "  def RewardModel(self, State: State, Action: np.ndarray, NextState: State, TerminalSignal: bool)-> float:\n",
    "      '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "      DistanceGainedFromtarget= np.linalg.norm(State.position-self.target)- np.linalg.norm(NextState.position-self.target) \n",
    "      EnergyConsumed= self.charged_particle.charge* self.Field.FieldPotential(State.position, NextState.position)\n",
    "      Cost= self.distance_weight* DistanceGainedFromtarget+ self.energy_weight* EnergyConsumed+ self.terminal_signal_weight* TerminalSignal\n",
    "      return Cost\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def IsTerminalCondition(self, State: State)-> bool:\n",
    "      '''This method determines if the state is within the viable learning region of the environment: Constraints'''\n",
    "      WithinXBound= -10. <= State.position[0] <= 10.\n",
    "      WithinYBound= -10. <= State.position[1] <= 10. \n",
    "      WithinvelocityBound= np.linalg.norm(State.velocity) < 10. \n",
    "      if WithinXBound and WithinYBound and WithinvelocityBound: \n",
    "          return False    \n",
    "      else:\n",
    "          return True\n",
    "  \n",
    "  def StateTransition(self, State: State= current_state, Action: np.ndarray= np.array([0.0, 0.0]), timeInterval: float= 1.0):\n",
    "      'Outputs the state of the system after taking an action, the reward ocurring from the transition and the terminal signal'\n",
    "      NextState= self.TransitionModel(State, Action, timeInterval=timeInterval)\n",
    "      TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "      Reward= self.RewardModel(State, Action, NextState, TerminalSignal)\n",
    "      return NextState, Reward, TerminalSignal\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def random_state(self)->State:\n",
    "      '''This method generates a random state within the viable learning region'''\n",
    "      Randomposition= np.array([np.random.uniform(-10., 10.), \n",
    "                                np.random.uniform(-10., 10.)])\n",
    "      Randomvelocity= np.zeros_like(Randomposition)\n",
    "      return self.State(Randomposition, Randomvelocity)\n",
    "\n",
    "  def SampleTrajectory(self, runtime: float, Policy: Optional[Callable]= None, timeStep: int=0.1):\n",
    "    time= [0]\n",
    "    State= self.current_state\n",
    "    StateTrajectory= []\n",
    "    ActionTrajectory= []\n",
    "    while time[-1]<runtime: \n",
    "      StateTrajectory.append(State)\n",
    "      if Policy is Callable:\n",
    "        Action = Policy(State)\n",
    "      else:\n",
    "          Action = np.random.randn(2)\n",
    "      ActionTrajectory.append(Action)\n",
    "      State= self.TransitionModel(State, Action, timeInterval= timeStep) \n",
    "      time.append(time[-1]+timeStep) \n",
    "    return StateTrajectory, ActionTrajectory, time\n",
    "\n",
    "  def PlotTrajectory(self, StateTrajectory, time): \n",
    "      positionPath= [State.position for State in StateTrajectory]\n",
    "      velocityPath= [State.velocity for State in StateTrajectory]\n",
    "      positionTrajectory= np.stack(positionPath).T\n",
    "      velocityTrajectory= np.stack(velocityPath).T\n",
    "      plt.plot(positionTrajectory[0], positionTrajectory[1])\n",
    "      plt.plot(positionTrajectory[0][0], positionTrajectory[1][0], 'ko')\n",
    "      plt.plot(positionTrajectory[0][-1], positionTrajectory[1][-1], 'r*')\n",
    "      plt.xlim(-10,10)\n",
    "      plt.ylim(-10,10)\n",
    "      plt.grid(True)\n",
    "      plt.show()\n",
    "\n",
    "  def TrajectoryValue(self, StateTrajectory: list[State], ActionTrajectory, time)-> float:\n",
    "      Value= 0\n",
    "      timeInterval= (time[-1]-time[0])/len(time)\n",
    "      for State, Action in zip(StateTrajectory, ActionTrajectory):\n",
    "         Value= Value+ (np.linalg.norm(State.position-self.target)+np.linalg.norm(Action))* timeInterval\n",
    "      return Value\n",
    "  \n",
    "  def Reset(self):\n",
    "      self.current_state= self.initial_state\n",
    "      self.current_time= 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeCharge= ClassicalParticle(mass=1.0, charge= -1e-9)\n",
    "positiveCharge= ClassicalParticle(mass=1.0, charge= 1e-9)\n",
    "sources = {\"Particle\": [negativeCharge, positiveCharge],\n",
    "           \"Position\": [np.array([1.0, 1.0]), np.array([-1.0, 1.0])]}\n",
    "testElectricField= ElectrostaticField2D(field_sources=sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ParticleInField.__init__() missing 2 required keyword-only arguments: 'InitialState' and 'CurrentState'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m PositiveChargeInElectricField\u001b[38;5;241m=\u001b[39m \u001b[43mParticleInField\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestElectricField\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharged_particle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositiveCharge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m StateTrajectory, ActionTrajectory, Time\u001b[38;5;241m=\u001b[39m PositiveChargeInElectricField\u001b[38;5;241m.\u001b[39mSampleTrajectory(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      3\u001b[0m Return\u001b[38;5;241m=\u001b[39m PositiveChargeInElectricField\u001b[38;5;241m.\u001b[39mTrajectoryValue(StateTrajectory, ActionTrajectory, Time)\n",
      "\u001b[0;31mTypeError\u001b[0m: ParticleInField.__init__() missing 2 required keyword-only arguments: 'InitialState' and 'CurrentState'"
     ]
    }
   ],
   "source": [
    "PositiveChargeInElectricField= ParticleInField(field=testElectricField, charged_particle=positiveCharge, target=np.array([0.0, 0.0]))\n",
    "StateTrajectory, ActionTrajectory, Time= PositiveChargeInElectricField.SampleTrajectory(50)\n",
    "Return= PositiveChargeInElectricField.TrajectoryValue(StateTrajectory, ActionTrajectory, Time)\n",
    "PositiveChargeInElectricField.PlotTrajectory(StateTrajectory, Time)\n",
    "Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningAgent(MDPController):\n",
    "  MDPEnvironment: MDPEnvironment\n",
    "  Policy: Callable\n",
    "\n",
    "  def Act(self, Observation: np.ndarray)-> np.ndarray:\n",
    "    ...\n",
    "      \n",
    "  def Observe(self)-> np.ndarray:\n",
    "    ...\n",
    "\n",
    "  def Learn(self):\n",
    "    'Improves  the MDPController by updating its models'\n",
    "    ...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
