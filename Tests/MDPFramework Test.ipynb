{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "import scipy.integrate as integrate\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/niyi/Documents/GitHub/Optimal-Control/Tools')\n",
    "from EnforceTyping import EnforceClassTyping, enforce_method_typing\n",
    "from ParticlesandFields import ClassicalField, ClassicalParticle\n",
    "from MDPFramework import MDPEnvironment, MDPController, LearningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ElectrostaticField2D(ClassicalField):\n",
    "  \"\"\"\n",
    "  A class used to represent a 2D Electrostatic Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  field_sources: dict\n",
    "      a formatted string to print out what the animal says\n",
    "  dimensionality: tuple\n",
    "      a tuple of the dimensionality of the field  \n",
    "\n",
    "  Methods\n",
    "  -------\n",
    "  dynamics(self, observation_position: np.ndarray, time: float) -> np.ndarray:\n",
    "      Represents the value of the field at any given point(s) or time. \n",
    "  potential(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential due to the field at a given position and/or time  \n",
    "  potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential difference between two positions at a given time in the vector field   \n",
    "  gradient(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the gradient at a given position and/or time in the vector field \n",
    "  curl(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the curl at a given position and/or time in the vector field \n",
    "  divergence(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the divergence at a given position and/or time in the vector field\n",
    "  \"\"\"\n",
    "  field_sources: dict\n",
    "  dimensionality: tuple = (2,)\n",
    "\n",
    "  def __call__(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      return self.dynamics(observation_position)\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def dynamics(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      \"\"\"\n",
    "      This function outputs the field strength due to field sources experienced at any given point(s) or time. \n",
    "      This determines the physics of the field (a 2D Electricstatic Field in this case)\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric field strength vector at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_field_vector = np.zeros_like(observation_position)\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_field_vector += (displacement_vector * field_source.charge) / displacement_magnitude**3\n",
    "      electric_field_vector = coulomb_constant * electric_field_vector\n",
    "      return np.round(electric_field_vector, 3)  # N/C or V/m\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential(self, observation_position: np.ndarray) -> float:\n",
    "      \"\"\"\n",
    "      Calculate the potential (voltage) at a position in the field.\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric potential at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "  \n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_potential = 0.0\n",
    "\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_potential += field_source.charge / displacement_magnitude\n",
    "\n",
    "      electric_potential = coulomb_constant * electric_potential\n",
    "      return np.round(electric_potential, 3)  # V\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the potential difference between the initial position and the final position in the field.\n",
    "\n",
    "    Args:\n",
    "        initial_position (np.ndarray): The starting position.\n",
    "        final_position (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    assert initial_position.shape == self.dimensionality, \"initial_position has the wrong dimensions\"\n",
    "    assert final_position.shape == self.dimensionality, \"final_position has the wrong dimensions\"\n",
    "    PorentialDifference= self.potential(initial_position)- self.potential(final_position)\n",
    "    return PorentialDifference\n",
    "\n",
    "  def gradient(self, observation_position: np.ndarray, delta: float= 0.001)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the derivative of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    gradient= np.zeros_like(observation_position)\n",
    "    for i in range(len(observation_position)):\n",
    "      di= np.zeros_like(observation_position)\n",
    "      di[i, ] = di[i, ]+delta\n",
    "      plusdi= observation_position+ di\n",
    "      minusdi= observation_position- di\n",
    "      gradient[i]= (self.dynamics(plusdi)- self.dynamics(minusdi))[i]/ (2* delta)\n",
    "    return gradient\n",
    "  \n",
    "  def curl(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the curl of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  def divergence(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the divergence of the field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def plot_field(self, low_bound= -20, high_bound= 20):\n",
    "    \"\"\"\n",
    "    This funtion plots the 2D electric vector field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    observation_position= np.meshgrid(np.linspace(low_bound, high_bound, 25), \n",
    "                                    np.linspace(low_bound, high_bound, 25))\n",
    "    observation_position= np.stack(observation_position)\n",
    "    xd, yd = self.dynamics(observation_position)\n",
    "    xd = xd / np.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / np.sqrt(xd**2 + yd**2)\n",
    "    color_aara = np.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(observation_position[0],observation_position[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Particle in Field Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(MDPEnvironment):\n",
    "  \"\"\"\n",
    "  A class used to represent a particle in a Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  field: ClassicalField\n",
    "    The field that the particle is in \n",
    "  particle: ClassicalParticle\n",
    "    The particle that is in the field\n",
    "  target: np.ndarray \n",
    "    The target position of the particle\n",
    "  distance_weight: float \n",
    "    The weight of the distance between the particle and the target\n",
    "  energy_weight: float \n",
    "    The weight of the energy of the particle\n",
    "  terminal_signal_weight: float \n",
    "    The weight of the terminal signal of the particle\n",
    "  current_time: float \n",
    "    The current time of the system\n",
    "\n",
    "  Methods\n",
    "  ------- \n",
    "  transition_model(self, state: State, action: Any)-> State: \n",
    "    Represents the  \n",
    "  reward_model(self, state: State, action: Any, next_state: State, terminal_signal: bool)-> float:\n",
    "    Represents the reward of the system\n",
    "  is_terminal_condition(self, state: State)-> bool: \n",
    "    Represents the terminal condition of the system\n",
    "  transition_step(self, state: State, action: Any)-> tuple[float, State, bool]: \n",
    "    Represents the transition step of the system\n",
    "  sample_trajectory(self, runtime: float)-> list[State]: \n",
    "    Samples a trajectory of the system\n",
    "  trajectory_value(self, trajectory: list[State])-> float: \n",
    "    Represents the value of the trajectory\n",
    "  reset(self): \n",
    "    Resets the system\n",
    "\n",
    "  \"\"\"\n",
    "  field: ClassicalField\n",
    "  particle: ClassicalParticle\n",
    "  target: np.ndarray # m\n",
    "  distance_weight: float= 1.0\n",
    "  energy_weight: float= -1.0\n",
    "  terminal_signal_weight: float= -1000.0\n",
    "  current_time: float = 0.0# s\n",
    "  state_dims: tuple= None\n",
    "  action_dims: tuple= None\n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its position, velocity and the Field Strength if experiences at its position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    position: np.ndarray # m\n",
    "    velocity: np.ndarray #m/s\n",
    "    \n",
    "    def vector(self):\n",
    "      return np.concatenate([self.position, self.velocity])\n",
    "    \n",
    "  initial_state: State = None\n",
    "  current_state: State = None\n",
    "\n",
    "  def __post_init__(self):\n",
    "    if self.initial_state is None:\n",
    "        self.initial_state= self.random_state()\n",
    "    self.current_state= self.initial_state\n",
    "    self.action_dims= self.field.dimensionality\n",
    "    self.state_dims= self.field.dimensionality * 2\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def state_dynamics(self, state: np.ndarray, time: float, control_force: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the dynamics of the particle's state.\n",
    "\n",
    "    Parameters:\n",
    "    state (np.ndarray): The current state of the particle [x, y, vx, vy].\n",
    "    time (float): The current time.\n",
    "    control_force (np.ndarray): The external control force applied to the particle.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The derivative of the state [vx, vy, ax, ay].\n",
    "    \"\"\"\n",
    "    velocity = state[2:]\n",
    "    acceleration = (self.particle.charge * self.field.dynamics(state[:2]) + control_force) / self.particle.mass\n",
    "    return np.concatenate((velocity, acceleration))\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def transition_model(self, state: State, action: np.ndarray= np.array([0.0, 0.0]), time_interval:float= 0.1)-> State:\n",
    "    \"\"\"\n",
    "    Computes the next state of the system after applying a constant force for a given time interval.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the system.\n",
    "        action (np.ndarray, optional): The constant force to apply. Defaults to [0.0, 0.0].\n",
    "        time_interval (float, optional): The time interval to apply the force for. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        State: The next state of the system.\n",
    "    \"\"\"\n",
    "    t_span = [self.current_time, self.current_time + time_interval]\n",
    "    next_state_vector = integrate.odeint(self.state_dynamics, state.vector(), t_span, args=(action,))[-1]\n",
    "    next_position = next_state_vector[:2]\n",
    "    next_velocity = next_state_vector[2:]\n",
    "    return self.State(next_position, next_velocity)\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def reward_model(self, state: State, action: np.ndarray, next_state: State, terminal_signal: bool) -> float:\n",
    "      \"\"\"\n",
    "      Computes the reward for the agent given a state transition.\n",
    "\n",
    "      The reward is a weighted sum of three components:\n",
    "      1. Distance gained towards the target\n",
    "      2. Energy consumed during the transition\n",
    "      3. Terminal signal (e.g. reaching the target or running out of energy)\n",
    "\n",
    "      Args:\n",
    "          state: The current state of the agent\n",
    "          action: The action taken by the agent\n",
    "          next_state: The resulting state after taking the action\n",
    "          terminal_signal: A boolean indicating whether the episode has terminated\n",
    "\n",
    "      Returns:\n",
    "          float: The reward value\n",
    "      \"\"\"\n",
    "      distance_gained = np.linalg.norm(state.position - self.target) - np.linalg.norm(next_state.position - self.target)\n",
    "      energy_consumed = np.linalg.norm(action)\n",
    "      reward = (\n",
    "          self.distance_weight * distance_gained\n",
    "          + self.energy_weight * energy_consumed\n",
    "          + self.terminal_signal_weight * int(terminal_signal)\n",
    "      )\n",
    "      return reward\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def is_terminal_condition(self, state: State) -> bool:\n",
    "      \"\"\"\n",
    "      Checks if the state is outside the viable learning region of the environment.\n",
    "\n",
    "      Args:\n",
    "          state (State): The current state of the environment.\n",
    "\n",
    "      Returns:\n",
    "          bool: True if the state is outside the viable learning region, False otherwise.\n",
    "      \"\"\"\n",
    "      x_bound = -10.0 <= state.position[0] <= 10.0\n",
    "      y_bound = -10.0 <= state.position[1] <= 10.0\n",
    "      velocity_bound = np.linalg.norm(state.velocity) < 10.0\n",
    "\n",
    "      return not (x_bound and y_bound and velocity_bound)\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def transition_step(\n",
    "      self, \n",
    "      state: State, \n",
    "      action: np.ndarray = np.array([0.0, 0.0]), \n",
    "      time_interval: float = 0.1\n",
    "  ) -> Tuple[State, float, bool]:\n",
    "      \"\"\"\n",
    "      Simulates a single time step of the environment.\n",
    "\n",
    "      Args:\n",
    "          state (State): The current state of the environment. Defaults to current_state.\n",
    "          action (np.ndarray): The action to take in the environment. Defaults to [0.0, 0.0].\n",
    "          time_interval (float): The time interval for the simulation. Defaults to 0.1.\n",
    "\n",
    "      Returns:\n",
    "          Tuple[State, float, bool]: A tuple containing the next state, the reward, and a terminal signal.\n",
    "      \"\"\"\n",
    "      next_state = self.transition_model(state, action, time_interval=time_interval)\n",
    "      terminal_signal = self.is_terminal_condition(next_state)\n",
    "      reward = self.reward_model(state, action, next_state, terminal_signal)\n",
    "      return next_state, reward, terminal_signal\n",
    "\n",
    "  def random_state(self) -> State:\n",
    "      \"\"\"\n",
    "      Generates a random state within the viable learning region.\n",
    "\n",
    "      Returns:\n",
    "          State: A random state within the viable learning region\n",
    "      \"\"\"\n",
    "      position = np.random.uniform(-10.0, 10.0, size=self.field.dimensionality)\n",
    "      velocity = np.zeros_like(position)\n",
    "      return self.State(position, velocity)\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def sample_trajectory(\n",
    "      self, \n",
    "      runtime: float, \n",
    "      initial_state: State = None, \n",
    "      n_steps: int = 200\n",
    "  ) -> Tuple[List[Any], List[np.ndarray], List[float]]:\n",
    "      \"\"\"\n",
    "      Generates a random state trajectory within the viable learning region.\n",
    "\n",
    "      Args:\n",
    "      - runtime (float): The total time for the trajectory in seconds.\n",
    "      - initial_state (State): The initial state of the trajectory. Defaults to current_state.\n",
    "      - n_steps (int): The number of steps in the trajectory. Defaults to 200.\n",
    "\n",
    "      Returns:\n",
    "      - A tuple containing the state trajectory, action trajectory, and time points.\n",
    "      \"\"\"\n",
    "      time_interval = runtime/n_steps\n",
    "      if initial_state == None:\n",
    "         state = self.current_state\n",
    "      else:\n",
    "         state = initial_state\n",
    "      time= 0.0\n",
    "      state_trajectory = []\n",
    "      time_points = np.linspace(time, runtime, n_steps)\n",
    "      return_value= 0.0\n",
    "\n",
    "      for t in time_points:\n",
    "          state_trajectory.append(state)\n",
    "          state, reward, _ = self.transition_step(state, time_interval=time_interval)\n",
    "          return_value += reward\n",
    "      return state_trajectory, return_value, time_points\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def plot_trajectory(self, state_trajectory: list, time: np.ndarray) -> None:\n",
    "      \"\"\"\n",
    "      Plot the trajectory of states over time.\n",
    "\n",
    "      Args:\n",
    "      - state_trajectory: A list of States representing the trajectory.\n",
    "      - time: A list of time points corresponding to each state in the trajectory.\n",
    "\n",
    "      Returns:\n",
    "      - None (plots the trajectory)\n",
    "      \"\"\"\n",
    "      positions = np.array([state.position for state in state_trajectory])\n",
    "      velocities = np.array([state.velocity for state in state_trajectory])\n",
    "\n",
    "      plt.figure(figsize=(8, 8))\n",
    "      plt.plot(positions[:, 0], positions[:, 1], label='Trajectory')\n",
    "      plt.scatter(positions[0, 0], positions[0, 1], c='k', marker='o', label='Start')\n",
    "      plt.scatter(positions[-1, 0], positions[-1, 1], c='r', marker='*', label='End')\n",
    "\n",
    "      plt.xlim(-10, 10)\n",
    "      plt.ylim(-10, 10)\n",
    "      plt.grid(True)\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(8, 8))\n",
    "      plt.plot( time, velocities[:, 0], label='Velocity x')\n",
    "      plt.plot( time, velocities[:, 1], label='Velocity y')\n",
    "      plt.grid(True)\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "  def reset(self) -> None:\n",
    "      \"\"\"\n",
    "      Resets the current state to the initial state and sets the current time to 0.0.\n",
    "      \"\"\"\n",
    "      self.current_state = self.initial_state\n",
    "      self.current_time = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_charge= ClassicalParticle(mass=1e-14, charge= -1e-9)\n",
    "positive_charge= ClassicalParticle(mass=1e-14, charge= 1e-9)\n",
    "sources = {\"Particle\": [negative_charge, positive_charge],\n",
    "           \"Position\": [np.array([1.0, 1.0]), np.array([-1.0, 1.0])]} \n",
    "\n",
    "test_electric_field= ElectrostaticField2D(field_sources=sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_charge_in_electric_field= ParticleInField(field=test_electric_field, \n",
    "                                               particle=positive_charge, \n",
    "                                               target=np.array([0.0, 0.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_trajectory, trajectory_return, time= point_charge_in_electric_field.sample_trajectory(25.0)\n",
    "point_charge_in_electric_field.plot_trajectory(state_trajectory, time)\n",
    "trajectory_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_force(action_dims):\n",
    "    return 1e-11* (2*np.random.ranf(action_dims)-1)\n",
    "class RandomWalker(MDPController):\n",
    "\n",
    "  def act(self, observation: np.ndarray)-> np.ndarray:\n",
    "      action_dims= self.environment.action_dims\n",
    "      action= self.policy(action_dims)\n",
    "      return action\n",
    "  def observe(self, state)-> np.ndarray:\n",
    "      observation= state.vector()\n",
    "      return observation\n",
    "  def sample_route(self, runtime: float, n_steps: int=100):\n",
    "      route= []\n",
    "      route_return= 0.0\n",
    "      time= 0.0\n",
    "      state= self.environment.initial_state\n",
    "      time_points = np.linspace(time, runtime, n_steps)\n",
    "      for _ in time_points:\n",
    "          observation= self.observe(state)\n",
    "          route.append(observation)\n",
    "          action= self.act(observation)\n",
    "          state, reward, _= self.environment.transition_step(state, action)\n",
    "          route_return += reward\n",
    "      return route, route_return\n",
    "  def plot_route(self, route):\n",
    "      route= np.array(route)\n",
    "      plt.plot(route[:, 0], route[:, 1])\n",
    "      plt.show()\n",
    "\n",
    "er= RandomWalker(environment=point_charge_in_electric_field, policy= random_force)\n",
    "a, b= er.sample_route(5)\n",
    "er.plot_route(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
