{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Agent class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch \n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Source(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Position: torch.Tensor # m\n",
    "    Charge: float #C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@EnforceFunctionTyping\n",
    "def ElectricField(FieldSources: list, ObservationPosition: torch.Tensor)->torch.Tensor:\n",
    "    'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "    CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "    for FieldSource in FieldSources:\n",
    "        if type(FieldSource) != Source:\n",
    "            raise TypeError(\"The input is not valid\")\n",
    "    if type(ObservationPosition[0]) != type(ObservationPosition[1]):\n",
    "         raise TypeError(\"Incompatible Reference point data types\")\n",
    "    elif type(ObservationPosition[0]) != torch.Tensor:\n",
    "        raise TypeError(\"Invalid Reference point data type\")\n",
    "    elif ObservationPosition[0].size()!=ObservationPosition[1].size():\n",
    "        raise TypeError(\"Incompatible Reference point dimensions\")\n",
    "    else: \n",
    "        ElectricFieldVector = torch.zeros_like(ObservationPosition)\n",
    "    for FieldSource in FieldSources:\n",
    "        PositionMatrices= torch.stack([torch.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                        torch.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "        DisplacemnetVector = ObservationPosition - PositionMatrices\n",
    "        DisplacementMagnitude = torch.sqrt(DisplacemnetVector[0]**2 +DisplacemnetVector[1]**2)  # Magnitude of the displacement vector\n",
    "        ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacemnetVector\n",
    "    return ElectricFieldVector #N/C or V/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@EnforceFunctionTyping\n",
    "def PlotField(Sources: list, ObservationPosition: torch.Tensor):\n",
    "    'This funtion plots the 2D electric vector field'\n",
    "    xd, yd = ElectricField(Sources, ObservationPosition)\n",
    "    xd = xd / torch.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / torch.sqrt(xd**2 + yd**2)\n",
    "    color_aara = torch.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "       These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    Position: torch.Tensor # m\n",
    "    FieldStrength: torch.Tensor #N/C or V/m\n",
    "    Momentum: torch.Tensor #kg*m/s\n",
    "    def Unwrap(self)->torch.Tensor:\n",
    "        '''This function converts the state parameters to a single tensor for processing. '''\n",
    "        return torch.cat([self.Position, \n",
    "                          self.FieldStrength,\n",
    "                          self.Momentum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accuately predict the expected value given a state-action pair.'''\n",
    "    def __init__(self, layer_sizes: list, layer_activations: list, layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [2 * torch.rand(layer_sizes[x], layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [2 * torch.rand(1, layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "        self.layer_activations_derivative= layer_activations_derivative\n",
    "    def forward(self, StateInput: torch.Tensor, ActionInput: torch.Tensor, full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network'\n",
    "        InputData = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        LayerConnections= []\n",
    "        ActivatedNeuronLayer= [InputData]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections.append(torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]) \n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return ActivatedNeuronLayer[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, ActionInput: torch.Tensor, OptimalReturn: torch.Tensor, loss_derivative: Callable):\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss function and '''\n",
    "        BiasGradient = [torch.zeros_like(b) for b in self.bias]\n",
    "        WeightGradient = [torch.zeros_like(w) for w in self.weights]\n",
    "        zs , ActivatedNeuronLayer  = self.forward(StateInput, ActionInput, full= True)\n",
    "        LayerError = loss_derivative(ActivatedNeuronLayer[-1], OptimalReturn) * self.layer_activations_derivative[-1](zs[-1])\n",
    "        BiasGradient[-1] = LayerError\n",
    "        WeightGradient[-1] = torch.matmul(ActivatedNeuronLayer[-2].t(), LayerError)\n",
    "        if ActivatedNeuronLayer[0].ndim < 2:\n",
    "            ActivatedNeuronLayer[0]= ActivatedNeuronLayer[0].unsqueeze(dim=0)\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            LayerError = torch.matmul(LayerError, self.weights[-l+1].t()) * self.layer_activations_derivative[-l](z)\n",
    "            BiasGradient[-l] = LayerError\n",
    "            WeightGradient[-l] = torch.matmul(ActivatedNeuronLayer[-l-1].t(), LayerError)\n",
    "        return  WeightGradient, BiasGradient\n",
    "    def update_model(self, weight_grad: list, bias_grad: list, learning_rate: float):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Policy Function(Actor) used to predict the best action to take at any given a state.\n",
    "       This policy function is a neural network that will learn to predict actions that lead to better rewards.'''\n",
    "    def __init__(self, layer_sizes: list, layer_activations: list, layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [2 * torch.rand(layer_sizes[x], layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [2 * torch.rand(1, layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "        self.layer_activations_derivative= layer_activations_derivative\n",
    "    def forward(self, StateInput: torch.Tensor, full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters to outputs Action parameters(Force applied on the x and y axis by the agent/controller) predicted by the Main actor network'\n",
    "        LayerConnections= []\n",
    "        ActivatedNeuronLayer= [StateInput]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections.append(torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]) \n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return torch.squeeze(ActivatedNeuronLayer[-1])\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateBatch: torch.Tensor, CriticModel: CriticNetwork)-> torch.Tensor:\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss function, a batch of inputs and targets'''\n",
    "        BiasGradient = [torch.zeros_like(b) for b in self.bias]\n",
    "        WeightGradient = [torch.zeros_like(w) for w in self.weights]\n",
    "        zs , ActivatedNeuronLayer  = self.forward(StateBatch, full= True)\n",
    "        ActorLoss = -torch.mean(CriticModel.forward(StateBatch, ActivatedNeuronLayer[-1].squeeze()))\n",
    "        LayerError = ActorLoss * self.layer_activations_derivative[-1](zs[-1])\n",
    "        BiasGradient[-1] = LayerError\n",
    "        WeightGradient[-1] = torch.matmul(ActivatedNeuronLayer[-2].t(), LayerError)\n",
    "        for l in range(2, len(self.layer_sizes)):\n",
    "            z = zs[-l]\n",
    "            sp = self.layer_activations_derivative[-l](z)\n",
    "            LayerError = torch.matmul(LayerError, self.weights[-l+1].t()) * sp\n",
    "            BiasGradient[-l] = LayerError\n",
    "            WeightGradient[-l] = torch.matmul((ActivatedNeuronLayer[-l-1].unsqueeze(dim=1)), LayerError)\n",
    "        return  WeightGradient, BiasGradient\n",
    "    def update_model(self, weight_grad, bias_grad, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass \n",
    "class ReplayBuffer(EnforceClassTyping):\n",
    "    '''This class represents the Replay buffer which stores state transitions(State, Action, NextState, Reward, Terminal Signal) which will be used to train the Actor and Critic Networks. \n",
    "    The replay buffer'''\n",
    "    BufferSize: int\n",
    "    Buffer: list = None\n",
    "    def __post_init__(self):\n",
    "        if self.Buffer is None:\n",
    "            self.Buffer = []\n",
    "    @EnforceMethodTyping\n",
    "    def AddExperience(self, State: State, Action: torch.Tensor, NextState: State, Reward: float, TerminalState: bool):\n",
    "        '''This method adds a state transition to the replay buffer'''\n",
    "        if len(self.Buffer) < self.BufferSize:\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "        else:\n",
    "            self.Buffer.pop(0)\n",
    "            self.Buffer.append([State, Action, NextState, Reward, TerminalState])\n",
    "    @EnforceMethodTyping\n",
    "    def SampleBuffer(self, BatchSize: int):\n",
    "        '''This method randomly samples the replay buffer to ouput a batches of state transition variables'''\n",
    "        if len(self.Buffer) >= BatchSize:\n",
    "            SampledBatch = random.sample(self.Buffer, BatchSize)\n",
    "            SampledStates= [SampledState[0].Unwrap() for SampledState in SampledBatch]\n",
    "            SampledActions= [SampledAction[1] for SampledAction in SampledBatch]\n",
    "            SampledNextStates= [SampledNextState[2].Unwrap() for SampledNextState in SampledBatch]\n",
    "            SampledRewards= [torch.Tensor([SampledReward[3]]) for SampledReward in SampledBatch]\n",
    "            SampledTerminalSignals= [torch.Tensor([SampledTerminalSignal[4]]) for SampledTerminalSignal in SampledBatch]\n",
    "            StateBatch= torch.stack(SampledStates)\n",
    "            ActionBatch= torch.stack(SampledActions)\n",
    "            NextStateBatch= torch.stack(SampledNextStates)\n",
    "            RewardsBatch= torch.stack(SampledRewards)\n",
    "            TerminalSignalsBatch= torch.stack(SampledTerminalSignals)\n",
    "        else:\n",
    "            raise ValueError('BatchSize too big')\n",
    "        return StateBatch, ActionBatch, NextStateBatch, RewardsBatch, TerminalSignalsBatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent(EnforceClassTyping):\n",
    "    '''This class represents the agent which will interact with the environment to create state state transitions which it will use to learn a good policy and value function.\n",
    "\n",
    "    The Mass and Charge parameters deteremine how the interacts with its environment.\n",
    "    The LearningRate, LossFunction, HiddenLayerSize, and MemorySize parameters determine its learning behaviour.'''\n",
    "    Charge: float\n",
    "    Mass: float\n",
    "    LearningRate: float\n",
    "    MemorySize: int\n",
    "    ActorHiddenLayerSize: list\n",
    "    ActorLayerActivations: list\n",
    "    ActorLayerActivationDerivatives: list\n",
    "    CriticHiddenLayerSize: list\n",
    "    CriticLayerActivations: list\n",
    "    CriticLayerActivationDerivatives: list\n",
    "    CurrentState: State \n",
    "    Memory: ReplayBuffer = field(init=False) \n",
    "    ActorModel: ActorNetwork = field(init=False)\n",
    "    CriticModel: CriticNetwork = field(init=False)\n",
    "    ActorTargetModel: ActorNetwork = field(init=False)\n",
    "    CriticTargetModel: CriticNetwork = field(init=False)\n",
    "    def __post_init__(self):\n",
    "        self.Memory= ReplayBuffer(self.MemorySize)\n",
    "        self.ActorModel= ActorNetwork(self.ActorHiddenLayerSize, self.ActorLayerActivations, self.ActorLayerActivationDerivatives)\n",
    "        self.ActorTargetModel= self.ActorModel\n",
    "        self.CriticModel= CriticNetwork(self.CriticHiddenLayerSize, self.CriticLayerActivations, self.CriticLayerActivationDerivatives)\n",
    "        self.CriticTargetModel= self.CriticModel\n",
    "    def ForceGenerator(self, Action: torch.Tensor)-> torch.Tensor:\n",
    "        ForceVector= Action* 20\n",
    "        return ForceVector\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateCritic(self, StateBatch: torch.Tensor, ActionBatch: torch.Tensor, NextStateBatch: torch.Tensor, RewardBatch: torch.Tensor, TerminalSignalsbatch: torch.Tensor, DiscountRate: float):\n",
    "        'Updates the main critic network parameters by minimizing the difference between the bellman optimal expected return and the expected return predicted by the main critic network'\n",
    "        NextAction= self.ActorTargetModel.forward(NextStateBatch)\n",
    "        BellmanOptimalReturn= RewardBatch+ (1-TerminalSignalsbatch)*DiscountRate*self.CriticTargetModel.forward(NextStateBatch, NextAction)\n",
    "        for i in range(len(StateBatch)):\n",
    "            WeightGradient, BiasGradient= self.CriticModel.compute_gradients(StateBatch[i], ActionBatch[i], BellmanOptimalReturn[i], mse_grad)\n",
    "            self.CriticModel.update_model(WeightGradient, BiasGradient, self.LearningRate)\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateActor(self, StateBatch: torch.Tensor):\n",
    "        'Updates the main actor network parameters by maximizing the Expected Q-value predicted by the main critic network'\n",
    "        for i in range(len(StateBatch)):\n",
    "            WeightGradient, BiasGradient= self.ActorModel.compute_gradients(StateBatch[i], self.CriticModel)\n",
    "            self.ActorModel.update_model(WeightGradient, BiasGradient, self.LearningRate)\n",
    "    @EnforceMethodTyping\n",
    "    def UpdateTargetCritic(self, SoftUpdateRate: float):\n",
    "        'Updates the target critic network parameters by making in it lag behind the main critic network updates'\n",
    "        for i in range(len(self.CriticTargetModel.weights)):\n",
    "            self.CriticTargetModel.weights[i]= self.CriticModel.weights[i] * SoftUpdateRate + self.CriticTargetModel.weights[i] * (1.0 - SoftUpdateRate)\n",
    "    @EnforceMethodTyping \n",
    "    def UpdateTargetActor(self, SoftUpdateRate: float):\n",
    "        'Updates the target actor network parameters by making in it lag behind the main actor network updates'\n",
    "        for i in range(len(self.ActorTargetModel.weights)):\n",
    "            self.ActorTargetModel.weights[i]= self.ActorModel.weights[i] * SoftUpdateRate + self.ActorTargetModel.weights[i] * (1.0 - SoftUpdateRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
