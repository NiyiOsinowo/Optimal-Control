{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This module contains a detailed implementation of the weight normalization algorithm'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from functools import wraps\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReLU:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.relu(x)\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "@dataclass\n",
    "class Tanh:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.tanh(x)\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return 1-x**2\n",
    "@dataclass\n",
    "class Null:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.ones_like(x)\n",
    "def mse_grad(x, y):\n",
    "    return torch.mean(-2*torch.mean(y-x))\n",
    "def mse(x, y):\n",
    "    return torch.mean(torch.mean(((x-y)**2)/2))\n",
    "def LayerNormalization(InputData: torch.Tensor, epsilon= 0.00000000001):\n",
    "    mean = InputData.mean(-1, keepdim = True)\n",
    "    std = InputData.std(-1, keepdim = True, unbiased=False)\n",
    "    NormalizedLayer= (InputData-mean)/(std+epsilon)\n",
    "    return NormalizedLayer\n",
    "def LayerNormalizationDerivative(x: torch.Tensor, epsilon= 0.00000000001):\n",
    "    if x.ndim<2:\n",
    "        x= x.unsqueeze(dim=0)\n",
    "    N = x.shape[1]\n",
    "    I = torch.eye(N)\n",
    "    mean = x.mean(-1, keepdim = True)\n",
    "    std = x.std(-1, keepdim = True, unbiased=False)\n",
    "    return ((N * I - 1) / (N * std + epsilon)) - (( (x - mean)*((x - mean).t())) / (N * std**3 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(8, 10)\n",
    "        self.hidden_layer = nn.Linear(10, 1)\n",
    "        self.weigh= torch._weight_norm\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.input_layer(x))\n",
    "        x = self.hidden_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accurately predict the expected value given a state-action pair.\n",
    "    This model uses weight normalization\n",
    "\n",
    "        Parameters:\n",
    "        - LayerSizes: List of integers representing the number of neurons in each layer.\n",
    "        - LayerActivations: List of strings representing the activation function for each layer.\n",
    "        - NormalizationLayers: List of layers to use normalization. Default is [BatchNormalization]\n",
    "\n",
    "        Attributes:\n",
    "        - WeightParameters:  A list containing all weights parameter matrices.\n",
    "        - WeightCoefficient:  A list containing all weights magnitude matrices.\n",
    "        - Bias:   A list containing all bias vectors (one per hidden layer)\n",
    "        - Gamma:  A list of gamma values used by batch normalization layers\n",
    "        - Beta:   A list of beta values used by batch normalization layers\n",
    "\n",
    "        Methods:\n",
    "        - forward:  Computes the output of the network given an input vector x.\n",
    "        - compute_gradients: Compute the gradients of the loss with respect to the parameters of the network.\n",
    "        - update_model: Update the model using gradient descent on the parameters.'''\n",
    "    def __init__(self, LayerSizes: list[int], LayerActivations: list[Callable], NormalizationLayers: list[bool]= None):\n",
    "        assert len(LayerSizes)-1 == len(LayerActivations)\n",
    "        self.LayerSizes= LayerSizes\n",
    "        self.WeightMagnitudes: list[torch.Tensor]= [torch.randn(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.WeightParameters: list[torch.Tensor]= [torch.randn(LayerSizes[x], LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.Bias: list[torch.Tensor]= [torch.randn(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.LayerActivations= LayerActivations\n",
    "        if  NormalizationLayers is None:\n",
    "            self.NormalizationLayers= [1]*(len(LayerSizes)-2) +[0]\n",
    "        else:\n",
    "            assert len(LayerSizes)-1 == len(NormalizationLayers)\n",
    "            self.NormalizationLayers= NormalizationLayers\n",
    "        self.Gamma= [torch.ones(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.Beta= [torch.zeros(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "    @EnforceMethodTyping\n",
    "    def forward(self, \n",
    "                StateInput: torch.Tensor, \n",
    "                ActionInput: torch.Tensor, \n",
    "                full: bool= False)-> torch.Tensor | tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network\n",
    "\n",
    "        Input:\n",
    "        - StateInput: Tensor of shape (batch_size, state_dimension)\n",
    "        - ActionInput: Tensor of shape (batch_size, action_dimension)\n",
    "        - full: \n",
    "\n",
    "        Output:\n",
    "        - LayerConnections:\n",
    "        - NormalizedLayer:   \n",
    "        - ActivatedLayer: \n",
    "        - MeanShiftedLayer: \n",
    "        - LayerValue:\n",
    "        \"\"\"\n",
    "        InputData: torch.Tensor = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        if InputData.ndim < 2:\n",
    "            InputData= InputData.unsqueeze(dim=0)\n",
    "        LayerConnections=  [torch.zeros(1)]*len(self.WeightParameters)\n",
    "        NormalizedLayer= [torch.zeros(1)]*len(self.WeightParameters)\n",
    "        ActivatedLayer= [torch.zeros(1)]*len(self.WeightParameters)\n",
    "        MeanShiftedLayer= [torch.zeros(1)]*len(self.WeightParameters)\n",
    "        LayerValue= [InputData]\n",
    "        def NormalizedReLULayerForward(LayerConnection: torch.Tensor, \n",
    "                                       ActivationFunction: Callable, \n",
    "                                       Gamma: torch.Tensor, \n",
    "                                       Beta: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Sub-routine to perform forward propagation on the InputData.\n",
    "\n",
    "            Parameters:\n",
    "            - LayerConnection: Input tensor.\n",
    "            - ActivationFunction:  ActivationFunction\n",
    "            - Gamma: Gamma\n",
    "            - Beta:\n",
    "\n",
    "            Returns:\n",
    "            - ActivatedLayer, NormalizedLayer, MeanShiftedLayer.\n",
    "            \"\"\"\n",
    "            ActivatedLayer= ActivationFunction(LayerConnection)\n",
    "            NormalizedLayer= LayerNormalization(ActivatedLayer)\n",
    "            MeanShiftedLayer= (Gamma* NormalizedLayer)+ Beta\n",
    "            return NormalizedLayer, MeanShiftedLayer, ActivatedLayer\n",
    "        def NormalizedLayerForward(LayerConnection: torch.Tensor, \n",
    "                                       ActivationFunction: Callable, \n",
    "                                       Gamma: torch.Tensor, \n",
    "                                       Beta: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Sub-routine to perform forward propagation on the InputData.\n",
    "\n",
    "            Parameters:\n",
    "            - LayerConnection: Input tensor.\n",
    "            - ActivationFunction:  ActivationFunction\n",
    "            - Gamma: Gamma\n",
    "            - Beta:\n",
    "\n",
    "            Returns:\n",
    "            - ActivatedLayer, NormalizedLayer, MeanShiftedLayer.\n",
    "            \"\"\"\n",
    "            NormalizedLayer= LayerNormalization(LayerConnection)\n",
    "            MeanShiftedLayer= (Gamma* NormalizedLayer)+ Beta\n",
    "            ActivatedLayer= ActivationFunction(MeanShiftedLayer)\n",
    "            return NormalizedLayer, MeanShiftedLayer, ActivatedLayer\n",
    "        for i in range(len(self.WeightParameters)):\n",
    "            NormalizedWeight= self.WeightMagnitudes[i]* torch.div(self.WeightParameters[i], torch.norm(self.WeightParameters[i], dim=0, keepdim=True))\n",
    "            LayerConnections[i]= torch.matmul(LayerValue[i], NormalizedWeight) + self.Bias[i]\n",
    "            ActivationFunction: Callable= self.LayerActivations[i]()\n",
    "            if self.NormalizationLayers[i]== 1:\n",
    "                if self.LayerActivations[i]== ReLU:\n",
    "                    NormalizedLayer[i], MeanShiftedLayer[i], ActivatedLayer[i]= NormalizedReLULayerForward(LayerConnections[i], ActivationFunction, self.Gamma[i], self.Beta[i])\n",
    "                    LayerValue.append(MeanShiftedLayer[i])\n",
    "                else:\n",
    "                    NormalizedLayer[i], MeanShiftedLayer[i], ActivatedLayer[i]= NormalizedLayerForward(LayerConnections[i], ActivationFunction, self.Gamma[i], self.Beta[i])\n",
    "                    LayerValue.append(ActivatedLayer[i])\n",
    "            else:\n",
    "                ActivatedLayer[i]= ActivationFunction(LayerConnections[i])\n",
    "                LayerValue.append(ActivatedLayer[i])\n",
    "        if full is False:\n",
    "            return LayerValue[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedLayer, MeanShiftedLayer, NormalizedLayer, LayerValue\n",
    "    @EnforceMethodTyping\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, \n",
    "                          ActionInput: torch.Tensor, \n",
    "                          OptimalReturn: torch.Tensor, \n",
    "                          LossDerivative: Callable):\n",
    "        \"\"\"\n",
    "        This function computes the gradient of the Weights and Biases of the network using the given derivative of a loss functio, input data and target data\n",
    "\n",
    "        Input:\n",
    "        - StateInput:      Tensor of shape (batch_size, state_dimension)\n",
    "        - ActionInput:       Tensor of shape (batch_size, action_dimension)\n",
    "        - OptimalReturn:   Tensor of shape (batch_size).\n",
    "        - LossDerivative:  Function that returns the derivative of loss with respect to network parameters.\n",
    "        Output:\n",
    "        - CummulativeWeightMagnitudesGradient:   Gradient of the network weights with respect to the loss function.\n",
    "        - CummulativeWeightParametersGradient:   Gradient of the network weights with respect to the loss function.\n",
    "        - CummulativeBiasGradient:     Gradient of the network biases with respect to the loss function.\n",
    "        - CummulativeGammaGradient:    Gradient of gamma with respect to the loss function.\n",
    "        - CummulativeBetaGradient:     Gradient of beta with respect to the loss function.\n",
    "        \"\"\"\n",
    "        CummulativeBiasGradient: list[torch.Tensor] = [torch.zeros_like(bias) for bias in self.Bias]\n",
    "        CummulativeWeightMagnitudesGradient: list[torch.Tensor] = [torch.zeros_like(weight) for weight in self.WeightMagnitudes]\n",
    "        CummulativeWeightParametersGradient: list[torch.Tensor] = [torch.zeros_like(weight) for weight in self.WeightParameters]\n",
    "        CummulativeGammaGradient: list[torch.Tensor] = [torch.zeros_like(gamma) for gamma in self.Gamma]\n",
    "        CummulativeBetaGradient: list[torch.Tensor] = [torch.zeros_like(beta) for beta in self.Beta]\n",
    "        NormalizedWeight: list[torch.Tensor]= [g* torch.div(v, torch.norm(v, dim=0, keepdim=True)) for v, g in zip(self.WeightParameters, self.WeightMagnitudes)]\n",
    "        @EnforceFunctionTyping\n",
    "        def NormalizedReLULayerBackward(dEdMS: torch.Tensor, \n",
    "                                        ActivationFunction: Callable, \n",
    "                                        NormalizedLayer: torch.Tensor, \n",
    "                                        Gamma: torch.Tensor, \n",
    "                                        ActivatedLayer: torch.Tensor, \n",
    "                                        LayerConnections: torch.Tensor, \n",
    "                                        LayerValue: torch.Tensor, \n",
    "                                        NormalizedWeight: torch.Tensor,\n",
    "                                        WeightParameters: torch.Tensor,\n",
    "                                        WeightMagnitudes: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a  normalized ReLU layer.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction: \n",
    "                - NormalizedLayer: The output from the previous normalized ReLU layer.\n",
    "                - Gamma: \n",
    "                - ActivatedLayer: \n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight:\n",
    "\n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                - dEdGamma: The error gradient of the current layers gamma parameter.\n",
    "                - dEdBeta: The error gradient of the current layers beta parameter.\n",
    "                \"\"\"\n",
    "                dMSdGamma= NormalizedLayer\n",
    "                dEdBeta=  dEdMS\n",
    "                dEdGamma= torch.mul(dMSdGamma, dEdMS)\n",
    "\n",
    "                dMSdN= Gamma\n",
    "                dEdN= torch.mul(dEdMS, dMSdN)\n",
    "                dNdA= LayerNormalizationDerivative(ActivatedLayer)\n",
    "                dEdA= torch.matmul(dEdN, dNdA)\n",
    "                dAdLC= ActivationFunction.derivative(LayerConnections)\n",
    "                dEdLC= torch.mul(dEdA, dAdLC)\n",
    "                dLCdw= LayerValue.t()\n",
    "                dEdWeight= torch.mul(dEdLC, dLCdw)\n",
    "                dEdb= dEdLC\n",
    "                vnorm= torch.norm(WeightParameters, dim=0, keepdim=True)\n",
    "                dEdg= torch.sum(dEdWeight*WeightParameters, dim=0, keepdim=True)/ vnorm\n",
    "                dEdv= (WeightMagnitudes/ vnorm)*dEdWeight- (((WeightMagnitudes*dEdg)/vnorm**2)* WeightParameters)\n",
    "\n",
    "                LayerInputGradient= NormalizedWeight.t()\n",
    "                ErrorGradient= torch.matmul(dEdLC, LayerInputGradient)\n",
    "                return ErrorGradient, dEdv, dEdg, dEdb, dEdGamma, dEdBeta\n",
    "        def NormalizedLayerBackward(dEdA: torch.Tensor, \n",
    "                                        ActivationFunction: Callable, \n",
    "                                        NormalizedLayer: torch.Tensor, \n",
    "                                        Gamma: torch.Tensor, \n",
    "                                        MeanShiftedLayer: torch.Tensor, \n",
    "                                        LayerConnections: torch.Tensor, \n",
    "                                        LayerValue: torch.Tensor, \n",
    "                                        NormalizedWeight: torch.Tensor,\n",
    "                                        WeightParameters: torch.Tensor,\n",
    "                                        WeightMagnitudes: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a normalized layer in the network.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction:  \n",
    "                - NormalizedLayer: The output from the previous normalized ReLU layer.\n",
    "                - Gamma: \n",
    "                - MeanShiftedLayer: \n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight: \n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                - dEdGamma: The error gradient of the current layers gamma parameter.\n",
    "                - dEdBeta: The error gradient of the current layers beta parameter.\n",
    "                \"\"\"\n",
    "                dAdMS= ActivationFunction.derivative(MeanShiftedLayer)\n",
    "                dEdMS = torch.mul(dEdA, dAdMS)\n",
    "                dMSdGamma= NormalizedLayer\n",
    "                dEdbeta=  dEdMS\n",
    "                dEdGamma= torch.mul(dEdMS, dMSdGamma)\n",
    "\n",
    "                dMSdN= Gamma\n",
    "                dEdN= torch.mul(dEdMS, dMSdN)\n",
    "                dNdLC= LayerNormalizationDerivative(LayerConnections)\n",
    "                dEdLC = torch.matmul(dEdN, dNdLC)\n",
    "                dLCdw= LayerValue.t()\n",
    "                dEdb = dEdLC\n",
    "                dEdWeight = torch.mul(dLCdw, dEdLC)\n",
    "                vnorm= torch.norm(WeightParameters, dim=0, keepdim=True)\n",
    "                dEdg= torch.sum(dEdWeight*WeightParameters, dim=0, keepdim=True)/ vnorm\n",
    "                dEdv= (WeightMagnitudes/ vnorm)*dEdWeight- (((WeightMagnitudes*dEdg)/vnorm**2)* WeightParameters)\n",
    "\n",
    "                LayerInputGradient= NormalizedWeight.t()\n",
    "                ErrorGradient= torch.matmul(dEdLC, LayerInputGradient)\n",
    "                return ErrorGradient, dEdv, dEdg, dEdb, dEdGamma, dEdbeta\n",
    "        @EnforceFunctionTyping\n",
    "        def LayerBackward(ErrorGradient: torch.Tensor, \n",
    "                          ActivationFunction: Callable, \n",
    "                          LayerConnections: torch.Tensor, \n",
    "                          LayerValue: torch.Tensor, \n",
    "                          NormalizedWeight: torch.Tensor,\n",
    "                          WeightParameters: torch.Tensor,\n",
    "                          WeightMagnitudes: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a  single layer in the network.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction:\n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight: \n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                \"\"\"\n",
    "                ActivationGradient= ActivationFunction.derivative(LayerConnections)\n",
    "                LayerErrorGradient = torch.mul(ErrorGradient, ActivationGradient)\n",
    "                LayerWeightGradient= LayerValue.t()\n",
    "                dEdb = LayerErrorGradient\n",
    "                dEdWeight = torch.mul(LayerWeightGradient, LayerErrorGradient)\n",
    "                vnorm= torch.norm(WeightParameters, dim=0, keepdim=True)\n",
    "                dEdg= torch.sum(dEdWeight*WeightParameters, dim=0, keepdim=True)/ vnorm\n",
    "                dEdv= (WeightMagnitudes/ vnorm)*dEdWeight- (((WeightMagnitudes*dEdg)/vnorm**2)* WeightParameters)\n",
    "                LayerInputGradient= NormalizedWeight.t()\n",
    "                ErrorGradient= torch.matmul(LayerErrorGradient, LayerInputGradient)\n",
    "                return ErrorGradient, dEdv, dEdg, dEdb\n",
    "        for State, Action, Return in zip(StateInput, ActionInput, OptimalReturn):\n",
    "            dEdBias = [torch.zeros(1)]*len(self.Bias)\n",
    "            dEdWeightParameters = [torch.zeros(1)]*len(self.WeightParameters)\n",
    "            dEdWeightMagnitudes = [torch.zeros(1)]*len(self.WeightMagnitudes)\n",
    "            dEdGamma = [torch.zeros(1)]*len(self.Gamma)\n",
    "            dEdBeta = [torch.zeros(1)]*len(self.Beta)\n",
    "            LayerConnections, ActivatedLayer, MeanShiftedLayer, NormalizedLayer, LayerValue= self.forward(State, Action, full= True)\n",
    "            Output= LayerValue[-1]\n",
    "            ErrorGradient= torch.tensor([[LossDerivative(Output, Return)]])\n",
    "            for l in range(len(self.WeightParameters)): \n",
    "                ActivationFunction= self.LayerActivations[-l-1]()\n",
    "                if self.NormalizationLayers[-l-1]==1 :\n",
    "                    if self.LayerActivations[-l-1]== ReLU:\n",
    "                        ErrorGradient, dEdWeightParameters[-l-1], dEdWeightMagnitudes[-l-1], dEdBias[-l-1], dEdGamma[-l-1], dEdBeta[-l-1]= NormalizedReLULayerBackward(ErrorGradient, \n",
    "                                                                                                                                      ActivationFunction, \n",
    "                                                                                                                                      NormalizedLayer[-l-1], \n",
    "                                                                                                                                      self.Gamma[-l-1], \n",
    "                                                                                                                                      ActivatedLayer[-l-1], \n",
    "                                                                                                                                      LayerConnections[-l-1], \n",
    "                                                                                                                                      LayerValue[-l-2], \n",
    "                                                                                                                                      NormalizedWeight[-l-1],\n",
    "                                                                                                                                      self.WeightParameters[-l-1] ,\n",
    "                                                                                                                                      self.WeightMagnitudes[-l-1])\n",
    "                    else:\n",
    "                        ErrorGradient, dEdWeightParameters[-l-1], dEdWeightMagnitudes[-l-1], dEdBias[-l-1], dEdGamma[-l-1], dEdBeta[-l-1]= NormalizedLayerBackward(NormalizedWeight, \n",
    "                                                                                                                                      ActivationFunction, \n",
    "                                                                                                                                      NormalizedLayer[-l-1], \n",
    "                                                                                                                                      self.Gamma[-l-1], \n",
    "                                                                                                                                      MeanShiftedLayer[-l-1], \n",
    "                                                                                                                                      LayerConnections[-l-1], \n",
    "                                                                                                                                      LayerValue[-l-2], \n",
    "                                                                                                                                      NormalizedWeight[-l-1],\n",
    "                                                                                                                                      self.WeightParameters[-l-1] ,\n",
    "                                                                                                                                      self.WeightMagnitudes[-l-1])\n",
    "                else:    \n",
    "                    ErrorGradient, dEdWeightParameters[-l-1], dEdWeightMagnitudes[-l-1], dEdBias[-l-1]= LayerBackward(ErrorGradient,\n",
    "                                                                                                                        ActivationFunction, \n",
    "                                                                                                                        LayerConnections[-l-1], \n",
    "                                                                                                                        LayerValue[-l-2], \n",
    "                                                                                                                        NormalizedWeight[-l-1],\n",
    "                                                                                                                        self.WeightParameters[-l-1],\n",
    "                                                                                                                        self.WeightMagnitudes[-l-1])  \n",
    "            CummulativeBiasGradient = [BiasGradient+dEdbias/len(StateInput) for BiasGradient, dEdbias in zip(CummulativeBiasGradient, dEdBias)]\n",
    "            CummulativeWeightParametersGradient = [WeightParameterGradient+dEdweight/len(StateInput) for WeightParameterGradient, dEdweight in zip(CummulativeWeightParametersGradient, dEdWeightParameters)]\n",
    "            CummulativeWeightMagnitudesGradient = [WeightMagnitudeGradient+dEdweight/len(StateInput) for WeightMagnitudeGradient, dEdweight in zip(CummulativeWeightMagnitudesGradient, dEdWeightMagnitudes)]\n",
    "            CummulativeGammaGradient = [GammaGradient+dEdgamma/len(StateInput) for GammaGradient, dEdgamma in zip(CummulativeGammaGradient, dEdGamma)]\n",
    "            CummulativeBetaGradient = [BetaGradient+dEdbeta/len(StateInput) for BetaGradient, dEdbeta in zip(CummulativeBetaGradient, dEdBeta)]\n",
    "        return CummulativeWeightParametersGradient, CummulativeWeightMagnitudesGradient, CummulativeBiasGradient, CummulativeGammaGradient, CummulativeBetaGradient\n",
    "    def update_model(self, \n",
    "                     CummulativeWeightParameterGradient: list[torch.Tensor], \n",
    "                     CummulativeWeightMagnitudeGradient: list[torch.Tensor], \n",
    "                     CummulativeBiasGradient: list[torch.Tensor], \n",
    "                     CummulativeGammaGradient: list[torch.Tensor], \n",
    "                     CummulativeBetaGradient: list[torch.Tensor], \n",
    "                     LearningRate=0.01):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the  neural network using the calculated gradients.\n",
    "        Input:\n",
    "           - CummulativeWeightGradient (list): list of gradient tensors corresponding to each weight in the network.\n",
    "           - CummulativeBiasGradient (list): list of gradient tensors corresponding to each bias in the network.\n",
    "           - CummulativeGammaGradient (list): List of gradient tensors corresponding to gamma parameter for normalization.\n",
    "           - CummulativeBetaGradient (list): List of gradient tensors corresponding to beta parameter for normalization.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.WeightParameters)):\n",
    "            self.WeightParameters[i]= self.WeightParameters[i].clone()- LearningRate * CummulativeWeightParameterGradient[i]\n",
    "            self.WeightMagnitudes[i]= self.WeightMagnitudes[i].clone()- LearningRate * CummulativeWeightMagnitudeGradient[i]\n",
    "            self.Bias[i] = self.Bias[i].clone() - LearningRate * CummulativeBiasGradient[i]\n",
    "            self.Gamma[i] = self.Gamma[i].clone() - LearningRate * CummulativeGammaGradient[i]\n",
    "            self.Beta[i] = self.Beta[i].clone() - LearningRate * CummulativeBetaGradient[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalCriticNetwork:\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accuately predict the expected value given a state-action pair.'''\n",
    "    def __init__(self, layer_sizes: list, \n",
    "                 layer_activations: list, \n",
    "                 layer_activations_derivative: list):\n",
    "        self.layer_sizes= layer_sizes\n",
    "        self.weights= [2 * torch.randn(layer_sizes[x], layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.bias= [2 * torch.randn(1, layer_sizes[x+1])- 1 for x in range(len(layer_sizes)-1)]\n",
    "        self.layer_activations= layer_activations\n",
    "        self.layer_activations_derivative= layer_activations_derivative\n",
    "    def forward(self, StateInput: torch.Tensor, \n",
    "                ActionInput: torch.Tensor, \n",
    "                full: bool= False)-> torch.Tensor:\n",
    "        'Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network'\n",
    "        InputData = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        LayerConnections= []\n",
    "        ActivatedNeuronLayer= [InputData]\n",
    "        for i in range(len(self.weights)):\n",
    "            LayerConnections.append(torch.matmul(ActivatedNeuronLayer[i], self.weights[i]) + self.bias[i]) \n",
    "            ActivatedNeuronLayer.append(self.layer_activations[i](LayerConnections[i]))\n",
    "        if full is False:\n",
    "            return ActivatedNeuronLayer[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedNeuronLayer\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, \n",
    "                          ActionInput: torch.Tensor, \n",
    "                          OptimalReturn: torch.Tensor, \n",
    "                          loss_derivative: Callable):\n",
    "        '''This function computes the gradient of the weights and biases of the network using the given derivative of a loss functio, input data and target data'''\n",
    "        bias_grad = [torch.zeros_like(b) for b in self.bias]\n",
    "        weight_grad = [torch.zeros_like(w) for w in self.weights]\n",
    "        for x1, x2, y in zip(StateInput, ActionInput, OptimalReturn):\n",
    "            dEdb = [0]*len(self.bias)\n",
    "            dEdw = [0]*len(self.weights)\n",
    "            LayerConnections, ActivatedNeuronLayer= self.forward(x1, x2, full= True)\n",
    "            dEdA= torch.tensor([[loss_derivative(ActivatedNeuronLayer[-1], y)]])\n",
    "            if ActivatedNeuronLayer[0].ndim < 2:\n",
    "                ActivatedNeuronLayer[0]= ActivatedNeuronLayer[0].unsqueeze(dim=0)\n",
    "            for l in range(len(self.weights)): \n",
    "                z = LayerConnections[-l-1]     \n",
    "                dAdz= self.layer_activations_derivative[-l-1](z)\n",
    "                dEdz = torch.mul(dEdA, dAdz)\n",
    "                dzdw= ActivatedNeuronLayer[-l-2].t()\n",
    "                dEdb[-l-1] = dEdz\n",
    "                dEdw[-l-1] = torch.mul(dzdw, dEdz)\n",
    "                dzdA= self.weights[-l-1].t()\n",
    "                dEdA= torch.matmul(dEdz, dzdA)\n",
    "            bias_grad = [nb+dnb/len(StateInput) for nb, dnb in zip(bias_grad, dEdb)]\n",
    "            weight_grad = [nw+dnw/len(StateInput) for nw, dnw in zip(weight_grad, dEdw)]\n",
    "        return weight_grad, bias_grad\n",
    "    def update_model(self, weight_grad, bias_grad, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -=  learning_rate * weight_grad[i]\n",
    "            self.bias[i] -=  learning_rate * bias_grad[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateInputs = torch.randn(3, 6)\n",
    "ActionInputs = torch.randn(3, 2)\n",
    "input_data = torch.cat([StateInputs, ActionInputs], dim=1)\n",
    "target_data = torch.rand(3, 1)\n",
    "loss_function = nn.MSELoss()\n",
    "WNCriticModel = CriticNetwork([8, 10, 5, 1], [ReLU, ReLU, Null], [1, 1, 0])\n",
    "CriticModel = NormalCriticNetwork([8, 10, 5, 1], [ReLU, ReLU, Null], [1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz= []\n",
    "for epoch in range(2000):\n",
    "    guess= WNCriticModel.forward(StateInputs, ActionInputs)\n",
    "    loss = loss_function(guess, target_data)\n",
    "    tz.append(loss.detach())\n",
    "    a, b, c, d, e= WNCriticModel.compute_gradients(StateInputs, ActionInputs, target_data, mse_grad)\n",
    "    WNCriticModel.update_model(a, b, c, d, e)\n",
    "plt.plot(tz)\n",
    "zt= WNCriticModel.forward(StateInputs, ActionInputs)\n",
    "print('guess',zt)\n",
    "print('target',target_data)\n",
    "loss_function(zt, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz= []\n",
    "for epoch in range(2000):\n",
    "    guess= CriticModel.forward(StateInputs, ActionInputs)\n",
    "    loss = loss_function(guess, target_data)\n",
    "    tz.append(loss.detach())\n",
    "    a, b, c, d, e= CriticModel.compute_gradients(StateInputs, ActionInputs, target_data, mse_grad)\n",
    "    CriticModel.update_model(a, b, c, d, e)\n",
    "plt.plot(tz)\n",
    "zt= CriticModel.forward(StateInputs, ActionInputs)\n",
    "print('guess',zt)\n",
    "print('target',target_data)\n",
    "loss_function(zt, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightparameter_grad, g_grad, bias_grad, gamma_grad, beta_grad= CriticModel.compute_gradients(StateInputs, ActionInputs, target_data, mse_grad)\n",
    "# print(weightparameter_grad)\n",
    "# print(g_grad)\n",
    "# print(bias_grad)\n",
    "# print(gamma_grad)\n",
    "# beta_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementLearningPractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
