{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This module contains a detailed implementation of the layer normalization algorithm'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "from functools import wraps\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReLU:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.relu(x)\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.where(x > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
    "@dataclass\n",
    "class Tanh:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.tanh(x)\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return 1-x**2\n",
    "@dataclass\n",
    "class Null:\n",
    "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x\n",
    "    def derivative(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.ones_like(x)\n",
    "def mse_grad(x, y):\n",
    "    return torch.mean(-2*torch.mean(y-x))\n",
    "def mse(x, y):\n",
    "    return torch.mean(torch.mean(((x-y)**2)/2))\n",
    "def LayerNormalization(InputData: torch.Tensor, epsilon= 0.00000000001):\n",
    "    mean = InputData.mean(-1, keepdim = True)\n",
    "    std = InputData.std(-1, keepdim = True, unbiased=False)\n",
    "    NormalizedLayer= (InputData-mean)/(std+epsilon)\n",
    "    return NormalizedLayer\n",
    "def LayerNormalizationDerivative(x: torch.Tensor, epsilon= 0.00000000001):\n",
    "    if x.ndim<2:\n",
    "        x= x.unsqueeze(dim=0)\n",
    "    N = x.shape[1]\n",
    "    I = torch.eye(N)\n",
    "    mean = x.mean(-1, keepdim = True)\n",
    "    std = x.std(-1, keepdim = True, unbiased=False)\n",
    "    return ((N * I - 1) / (N * std + epsilon)) - (( (x - mean)*((x - mean).t())) / (N * std**3 + epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(8, 10)\n",
    "        self.normalize_input = nn.LayerNorm(10, elementwise_affine = False)\n",
    "        self.gamma_H1= 2 * torch.rand(1, 10)- 1\n",
    "        self.beta_H1= 2 * torch.rand(1, 10)- 1\n",
    "        self.hidden_layer_1 = nn.Linear(10, 5)\n",
    "        self.normalize_hidden_1 = nn.LayerNorm(5, elementwise_affine = False )\n",
    "        self.gamma_H2= 2 * torch.rand(1, 5)- 1\n",
    "        self.beta_H2= 2 * torch.rand(1, 5)- 1\n",
    "        self.hidden_layer_2 = nn.Linear(5, 1)\n",
    "        self.normalize_hidden_2 = nn.LayerNorm(1, elementwise_affine = False)\n",
    "    def forward(self, x):\n",
    "        a = (self.gamma_H1 * self.normalize_input(torch.relu(self.input_layer(x))))+ self.beta_H1\n",
    "        b = (self.gamma_H2 * self.normalize_hidden_1(torch.relu(self.hidden_layer_1(a))))+ self.beta_H2\n",
    "        c = (self.hidden_layer_2(b))\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(EnforceClassTyping):\n",
    "    '''This object represents the Value Function(Critic) used to estimate the expected value of a state-action pair.\n",
    "    This value function is a neural network that will learn to more accurately predict the expected value given a state-action pair.\n",
    "        Parameters:\n",
    "        - LayerSizes: List of integers representing the number of neurons in each layer.\n",
    "        - LayerActivations: List of strings representing the activation function for each layer.\n",
    "        - NormalizationLayers: List of layers to use normalization. Default is [BatchNormalization]\n",
    "\n",
    "        Attributes:\n",
    "        - Weights:  A list containing all weights matrices (one per hidden layer). The i-th element of this\n",
    "        - Bias:     A list containing all bias vectors (one per hidden layer)\n",
    "        - Gamma:    A list of gamma values used by batch normalization layers\n",
    "        - Beta:     A list of beta values used by batch normalization layers\n",
    "        Methods:\n",
    "        - forward:  Computes the output of the network given an input vector x.\n",
    "        - compute_gradients: Compute the gradients of the loss with respect to the parameters of the network.\n",
    "        - update_model: Update the model using gradient descent on the parameters.'''\n",
    "    def __init__(self, LayerSizes: list[int], LayerActivations: list, NormalizationLayers: list[bool]= None):\n",
    "        assert len(LayerSizes)-1 == len(LayerActivations)\n",
    "        self.LayerSizes= LayerSizes\n",
    "        self.Weights: list[torch.Tensor]= [torch.randn(LayerSizes[x], LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.Bias: list[torch.Tensor]= [torch.randn(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.LayerActivations= LayerActivations\n",
    "        if  NormalizationLayers is None:\n",
    "            self.NormalizationLayers= [1]*(len(LayerSizes)-2) +[0]\n",
    "        else:\n",
    "            assert len(LayerSizes)-1 == len(NormalizationLayers)\n",
    "            self.NormalizationLayers= NormalizationLayers\n",
    "        self.Gamma= [torch.ones(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "        self.Beta= [torch.zeros(1, LayerSizes[x+1]) for x in range(len(LayerSizes)-1)]\n",
    "    @EnforceMethodTyping\n",
    "    def forward(self, \n",
    "                StateInput: torch.Tensor, \n",
    "                ActionInput: torch.Tensor, \n",
    "                full: bool= False)-> torch.Tensor | tuple[list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor], list[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Takes State Parameters and Action Parameters to outputs the expected return of the state-action pair predicted by the Main critic network\n",
    "\n",
    "        Input:\n",
    "        - StateInput:      Tensor of shape (batch_size, state_dimension)\n",
    "        - ActionInput:       Tensor of shape (batch_size, action_dimension)\n",
    "        - full: \n",
    "\n",
    "        Output:\n",
    "        - LayerConnections:\n",
    "        - NormalizedLayer:   \n",
    "        - ActivatedLayer: \n",
    "        - MeanShiftedLayer: \n",
    "        - LayerValue:\n",
    "        \"\"\"\n",
    "        InputData: torch.Tensor = torch.cat([StateInput, ActionInput], dim=StateInput.ndim-1)\n",
    "        if InputData.ndim < 2:\n",
    "            InputData= InputData.unsqueeze(dim=0)\n",
    "        LayerConnections=  [torch.zeros(1)]*len(self.Weights)\n",
    "        NormalizedLayer= [torch.zeros(1)]*len(self.Weights)\n",
    "        ActivatedLayer= [torch.zeros(1)]*len(self.Weights)\n",
    "        MeanShiftedLayer= [torch.zeros(1)]*len(self.Weights)\n",
    "        LayerValue= [InputData]\n",
    "        def NormalizedReLULayerForward(LayerConnection: torch.Tensor, \n",
    "                                       ActivationFunction: Callable, \n",
    "                                       Gamma: torch.Tensor, \n",
    "                                       Beta: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Sub-routine to perform forward propagation on the InputData.\n",
    "\n",
    "            Parameters:\n",
    "            - LayerConnection: Input tensor.\n",
    "            - ActivationFunction:  ActivationFunction\n",
    "            - Gamma: Gamma\n",
    "            - Beta:\n",
    "\n",
    "            Returns:\n",
    "            - ActivatedLayer, NormalizedLayer, MeanShiftedLayer.\n",
    "            \"\"\"\n",
    "            ActivatedLayer= ActivationFunction(LayerConnection)\n",
    "            NormalizedLayer= LayerNormalization(ActivatedLayer)\n",
    "            MeanShiftedLayer= (Gamma* NormalizedLayer)+ Beta\n",
    "            return NormalizedLayer, MeanShiftedLayer, ActivatedLayer\n",
    "        def NormalizedLayerForward(LayerConnection: torch.Tensor, \n",
    "                                       ActivationFunction: Callable, \n",
    "                                       Gamma: torch.Tensor, \n",
    "                                       Beta: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "            \"\"\"\n",
    "            Sub-routine to perform forward propagation on the InputData.\n",
    "\n",
    "            Parameters:\n",
    "            - LayerConnection: Input tensor.\n",
    "            - ActivationFunction:  ActivationFunction\n",
    "            - Gamma: Gamma\n",
    "            - Beta:\n",
    "\n",
    "            Returns:\n",
    "            - ActivatedLayer, NormalizedLayer, MeanShiftedLayer.\n",
    "            \"\"\"\n",
    "            NormalizedLayer= LayerNormalization(LayerConnection)\n",
    "            MeanShiftedLayer= (Gamma* NormalizedLayer)+ Beta\n",
    "            ActivatedLayer= ActivationFunction(MeanShiftedLayer)\n",
    "            return NormalizedLayer, MeanShiftedLayer, ActivatedLayer\n",
    "        for i in range(len(self.Weights)):\n",
    "            LayerConnections[i]= torch.matmul(LayerValue[i], self.Weights[i]) + self.Bias[i]\n",
    "            ActivationFunction: Callable= self.LayerActivations[i]()\n",
    "            if self.NormalizationLayers[i]== 1:\n",
    "                if self.LayerActivations[i]== ReLU:\n",
    "                    NormalizedLayer[i], MeanShiftedLayer[i], ActivatedLayer[i]= NormalizedReLULayerForward(LayerConnections[i], ActivationFunction, self.Gamma[i], self.Beta[i])\n",
    "                    LayerValue.append(MeanShiftedLayer[i])\n",
    "                else:\n",
    "                    NormalizedLayer[i], MeanShiftedLayer[i], ActivatedLayer[i]= NormalizedLayerForward(LayerConnections[i], ActivationFunction, self.Gamma[i], self.Beta[i])\n",
    "                    LayerValue.append(ActivatedLayer[i])\n",
    "            else:\n",
    "                ActivatedLayer[i]= ActivationFunction(LayerConnections[i])\n",
    "                LayerValue.append(ActivatedLayer[i])\n",
    "        if full is False:\n",
    "            return LayerValue[-1]\n",
    "        else:\n",
    "            return LayerConnections, ActivatedLayer, MeanShiftedLayer, NormalizedLayer, LayerValue\n",
    "    @EnforceMethodTyping\n",
    "    def compute_gradients(self, StateInput: torch.Tensor, \n",
    "                          ActionInput: torch.Tensor, \n",
    "                          OptimalReturn: torch.Tensor, \n",
    "                          LossDerivative: Callable):\n",
    "        \"\"\"\n",
    "        This function computes the gradient of the Weights and Biases of the network using the given derivative of a loss functio, input data and target data\n",
    "\n",
    "        Input:\n",
    "        - StateInput:      Tensor of shape (batch_size, state_dimension)\n",
    "        - ActionInput:       Tensor of shape (batch_size, action_dimension)\n",
    "        - OptimalReturn:   Tensor of shape (batch_size).\n",
    "        - LossDerivative:  Function that returns the derivative of loss with respect to network parameters.\n",
    "        Output:\n",
    "        - CummulativeWeightGradient:   Gradient of the network weights with respect to the loss function.\n",
    "        - CummulativeBiasGradient:     Gradient of the network biases with respect to the loss function.\n",
    "        - CummulativeGammaGradient:    Gradient of gamma with respect to the loss function.\n",
    "        - CummulativeBetaGradient:     Gradient of beta with respect to the loss function.\n",
    "        \"\"\"\n",
    "        CummulativeBiasGradient: list[torch.Tensor] = [torch.zeros_like(bias) for bias in self.Bias]\n",
    "        CummulativeWeightGradient: list[torch.Tensor] = [torch.zeros_like(weight) for weight in self.Weights]\n",
    "        CummulativeGammaGradient: list[torch.Tensor] = [torch.zeros_like(gamma) for gamma in self.Gamma]\n",
    "        CummulativeBetaGradient: list[torch.Tensor] = [torch.zeros_like(beta) for beta in self.Beta]\n",
    "        @EnforceFunctionTyping\n",
    "        def NormalizedReLULayerBackward(dEdMS: torch.Tensor, \n",
    "                                        ActivationFunction: Callable, \n",
    "                                        NormalizedLayer: torch.Tensor, \n",
    "                                        Gamma: torch.Tensor, \n",
    "                                        ActivatedLayer: torch.Tensor, \n",
    "                                        LayerConnections: torch.Tensor, \n",
    "                                        LayerValue: torch.Tensor, \n",
    "                                        Weight: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a  normalized ReLU layer.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction:  \n",
    "                - NormalizedLayer: The output from the previous normalized ReLU layer.\n",
    "                - Gamma: \n",
    "                - ActivatedLayer: \n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight: \n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                - dEdGamma: The error gradient of the current layers gamma parameter.\n",
    "                - dEdBeta: The error gradient of the current layers beta parameter.\n",
    "                \"\"\"\n",
    "                dMSdGamma= NormalizedLayer\n",
    "                dEdBeta=  dEdMS\n",
    "                dEdGamma= torch.mul(dMSdGamma, dEdMS)\n",
    "\n",
    "                dMSdN= Gamma\n",
    "                dEdN= torch.mul(dEdMS, dMSdN)\n",
    "                dNdA= LayerNormalizationDerivative(ActivatedLayer)\n",
    "                dEdA= torch.matmul(dEdN, dNdA)\n",
    "                dAdLC= ActivationFunction.derivative(LayerConnections)\n",
    "                dEdLC= torch.mul(dEdA, dAdLC)\n",
    "                dLCdw= LayerValue.t()\n",
    "                dEdWeight= torch.mul(dEdLC, dLCdw)\n",
    "                dEdb= dEdLC\n",
    "\n",
    "                LayerInputGradient= Weight.t()\n",
    "                ErrorGradient= torch.matmul(dEdLC, LayerInputGradient)\n",
    "                return ErrorGradient, dEdWeight, dEdb, dEdGamma, dEdBeta\n",
    "        @EnforceFunctionTyping\n",
    "        def NormalizedLayerBackward(dEdA: torch.Tensor, \n",
    "                                    ActivationFunction: Callable, \n",
    "                                    NormalizedLayer: torch.Tensor, \n",
    "                                    Gamma: torch.Tensor, \n",
    "                                    MeanShiftedLayer: torch.Tensor, \n",
    "                                    LayerConnections: torch.Tensor, \n",
    "                                    LayerValue: torch.Tensor, \n",
    "                                    Weight: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a normalized layer in the network.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction:  \n",
    "                - NormalizedLayer: The output from the previous normalized ReLU layer.\n",
    "                - Gamma: \n",
    "                - MeanShiftedLayer: \n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight: \n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                - dEdGamma: The error gradient of the current layers gamma parameter.\n",
    "                - dEdBeta: The error gradient of the current layers beta parameter.\n",
    "                \"\"\"\n",
    "                dAdMS= ActivationFunction.derivative(MeanShiftedLayer)\n",
    "                dEdMS = torch.mul(dEdA, dAdMS)\n",
    "                dMSdGamma= NormalizedLayer\n",
    "                dEdBeta=  dEdMS\n",
    "                dEdGamma= torch.mul(dEdMS, dMSdGamma)\n",
    "\n",
    "                dMSdN= Gamma\n",
    "                dEdN= torch.mul(dEdMS, dMSdN)\n",
    "                dNdLC= LayerNormalizationDerivative(LayerConnections)\n",
    "                dEdLC = torch.matmul(dEdN, dNdLC)\n",
    "                dLCdw= LayerValue.t()\n",
    "                dEdb = dEdLC\n",
    "                dEdWeight = torch.mul(dLCdw, dEdLC)\n",
    "\n",
    "                LayerInputGradient= Weight.t()\n",
    "                ErrorGradient= torch.matmul(dEdLC, LayerInputGradient)\n",
    "                return ErrorGradient, dEdWeight, dEdb, dEdGamma, dEdBeta\n",
    "        @EnforceFunctionTyping\n",
    "        def LayerBackward(ErrorGradient: torch.Tensor, \n",
    "                          ActivationFunction: Callable, \n",
    "                          LayerConnections: torch.Tensor, \n",
    "                          LayerValue: torch.Tensor, \n",
    "                          Weight: torch.Tensor)-> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                \"\"\"\n",
    "                Sub-routine to perform backward propagation through a  single layer in the network.\n",
    "\n",
    "                Arguments:\n",
    "                - dEdMS: Differential error with respect to the mean shifted layer.\n",
    "                - ActivationFunction:\n",
    "                - LayerConnections: \n",
    "                - LayerValue: \n",
    "                - Weight: \n",
    "                Returns:\n",
    "                - ErrorGradient: The error gradient of the previous layer.\n",
    "                - dEdWeight: The error gradient of the current layer's weights.\n",
    "                - dEdb: The error gradient of the current layer's bias.\n",
    "                \"\"\"\n",
    "                ActivationGradient= ActivationFunction.derivative(LayerConnections)\n",
    "                LayerErrorGradient = torch.mul(ErrorGradient, ActivationGradient)\n",
    "                LayerCummulativeWeightGradient= LayerValue.t()\n",
    "                dEdb = LayerErrorGradient\n",
    "                dEdWeight = torch.mul(LayerCummulativeWeightGradient, LayerErrorGradient)\n",
    "                LayerInputGradient= Weight.t()\n",
    "                ErrorGradient= torch.matmul(LayerErrorGradient, LayerInputGradient)\n",
    "                return ErrorGradient, dEdWeight, dEdb\n",
    "        for State, Action, Return in zip(StateInput, ActionInput, OptimalReturn):\n",
    "            dEdBias = [torch.zeros(1)]*len(self.Bias)\n",
    "            dEdWeight = [torch.zeros(1)]*len(self.Weights)\n",
    "            dEdGamma = [torch.zeros(1)]*len(self.Gamma)\n",
    "            dEdBeta = [torch.zeros(1)]*len(self.Beta)\n",
    "            LayerConnections, ActivatedLayer, MeanShiftedLayer, NormalizedLayer, LayerValue= self.forward(State, Action, full= True)\n",
    "            Output= LayerValue[-1]\n",
    "            ErrorGradient= torch.tensor([[LossDerivative(Output, Return)]])\n",
    "            for l in range(len(self.Weights)): \n",
    "                ActivationFunction= self.LayerActivations[-l-1]()\n",
    "                if self.NormalizationLayers[-l-1]==1 :\n",
    "                    if self.LayerActivations[-l-1]== ReLU:\n",
    "                        ErrorGradient, dEdWeight[-l-1], dEdBias[-l-1], dEdGamma[-l-1], dEdBeta[-l-1]= NormalizedReLULayerBackward(ErrorGradient, \n",
    "                                                                                                                          ActivationFunction, \n",
    "                                                                                                                          NormalizedLayer[-l-1], \n",
    "                                                                                                                          self.Gamma[-l-1], \n",
    "                                                                                                                          ActivatedLayer[-l-1], \n",
    "                                                                                                                          LayerConnections[-l-1], \n",
    "                                                                                                                          LayerValue[-l-2], \n",
    "                                                                                                                          self.Weights[-l-1])\n",
    "                    else:\n",
    "                        ErrorGradient, dEdWeight[-l-1], dEdBias[-l-1], dEdGamma[-l-1], dEdBeta[-l-1]= NormalizedLayerBackward(ErrorGradient, \n",
    "                                                                                                                        ActivationFunction, \n",
    "                                                                                                                        NormalizedLayer[-l-1], \n",
    "                                                                                                                        self.Gamma[-l-1], \n",
    "                                                                                                                        MeanShiftedLayer[-l-1], \n",
    "                                                                                                                        LayerConnections[-l-1], \n",
    "                                                                                                                        LayerValue[-l-2], \n",
    "                                                                                                                        self.Weights[-l-1])\n",
    "                else:    \n",
    "                    ErrorGradient, dEdWeight[-l-1], dEdBias[-l-1]= LayerBackward(ErrorGradient, \n",
    "                                                                                 ActivationFunction, \n",
    "                                                                                 LayerConnections[-l-1], \n",
    "                                                                                 LayerValue[-l-2], \n",
    "                                                                                 self.Weights[-l-1])  \n",
    "            CummulativeBiasGradient = [BiasGradient+dEdbias/len(StateInput) for BiasGradient, dEdbias in zip(CummulativeBiasGradient, dEdBias)]\n",
    "            CummulativeWeightGradient = [WeightGradient+dEdweight/len(StateInput) for WeightGradient, dEdweight in zip(CummulativeWeightGradient, dEdWeight)]\n",
    "            CummulativeGammaGradient = [GammaGradient+dEdgamma/len(StateInput) for GammaGradient, dEdgamma in zip(CummulativeGammaGradient, dEdGamma)]\n",
    "            CummulativeBetaGradient = [BetaGradient+dEdbeta/len(StateInput) for BetaGradient, dEdbeta in zip(CummulativeBetaGradient, dEdBeta)]\n",
    "        return CummulativeWeightGradient, CummulativeBiasGradient, CummulativeGammaGradient, CummulativeBetaGradient\n",
    "    @EnforceMethodTyping\n",
    "    def update_model(self, CummulativeWeightGradient: list[torch.Tensor], CummulativeBiasGradient: list[torch.Tensor], CummulativeGammaGradient: list[torch.Tensor], CummulativeBetaGradient: list[torch.Tensor], LearningRate=0.01):\n",
    "        \"\"\"\n",
    "        Updates the parameters of the  neural network using the calculated gradients.\n",
    "        Input:\n",
    "           - CummulativeWeightGradient (list): list of gradient tensors corresponding to each weight in the network.\n",
    "           - CummulativeBiasGradient (list): list of gradient tensors corresponding to each bias in the network.\n",
    "           - CummulativeGammaGradient (list): List of gradient tensors corresponding to gamma parameter for normalization.\n",
    "           - CummulativeBetaGradient (list): List of gradient tensors corresponding to beta parameter for normalization.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.Weights)):\n",
    "            self.Weights[i]= self.Weights[i].clone()- LearningRate * CummulativeWeightGradient[i]\n",
    "            self.Bias[i] = self.Bias[i].clone() - LearningRate * CummulativeBiasGradient[i]\n",
    "            self.Gamma[i] = self.Gamma[i].clone() - LearningRate * CummulativeGammaGradient[i]\n",
    "            self.Beta[i] = self.Beta[i].clone() - LearningRate * CummulativeBetaGradient[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PytorchCritic = NeuralNetwork()\n",
    "CriticModel = CriticNetwork([8, 10, 5, 1], [ReLU, ReLU, Null], [1, 1, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CriticModel.Weights[0]= PytorchCritic.input_layer.weight.t()\n",
    "CriticModel.Weights[1]= PytorchCritic.hidden_layer_1.weight.t()\n",
    "CriticModel.Weights[2]= PytorchCritic.hidden_layer_2.weight.t()\n",
    "CriticModel.Bias[0]= PytorchCritic.input_layer.bias.t()\n",
    "CriticModel.Bias[1]= PytorchCritic.hidden_layer_1.bias.t()\n",
    "CriticModel.Bias[2]= PytorchCritic.hidden_layer_2.bias.t()\n",
    "CriticModel.Gamma[0]= PytorchCritic.gamma_H1\n",
    "CriticModel.Gamma[1]= PytorchCritic.gamma_H2\n",
    "CriticModel.Beta[0]= PytorchCritic.beta_H1\n",
    "CriticModel.Beta[1]= PytorchCritic.beta_H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StateInputs = torch.randn(5, 6)\n",
    "ActionInputs = torch.randn(5, 2)\n",
    "input_data = torch.cat([StateInputs, ActionInputs], dim=1)\n",
    "target_data = torch.rand(5, 1)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= CriticModel.forward(StateInputs, ActionInputs)\n",
    "y= PytorchCritic.forward(input_data)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pytorch_gradients(input_data, target_data, model, loss_function):\n",
    "    model.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = loss_function(output, target_data)\n",
    "    loss.backward()\n",
    "    gradients = []\n",
    "    for param in model.parameters():\n",
    "        gradients.append(param.grad.clone())\n",
    "    for i, grad in enumerate(gradients):\n",
    "        print(f\"Gradient for parameter {i + 1}:\\n{grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz= []\n",
    "for epoch in range(2000):\n",
    "    guess= CriticModel.forward(StateInputs, ActionInputs)\n",
    "    loss = loss_function(guess, target_data)\n",
    "    tz.append(loss.detach())\n",
    "    a, b, c, d= CriticModel.compute_gradients(StateInputs, ActionInputs, target_data, mse_grad)\n",
    "    CriticModel.update_model(a, b, c, d)\n",
    "plt.plot(tz)\n",
    "zt= CriticModel.forward(StateInputs, ActionInputs)\n",
    "print('guess',zt)\n",
    "print('target',target_data)\n",
    "loss_function(zt, target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients = compute_pytorch_gradients(input_data, target_data, PytorchCritic, loss_function)\n",
    "# a, b, c, d= CriticModel.compute_gradients(StateInputs, ActionInputs, target_data, mse_grad)\n",
    "# print(a)\n",
    "# print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReinforcementLearningPractice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
