{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Introduction](#1)\n",
    "  - [Dynamical Systems](#2)\n",
    "    + [Basic terminology of dynamical systems](#3) \n",
    "    + [Challenges that may arise when modelling a dynamical system](#4)\n",
    "    + [Types of dynamical systems](#5)\n",
    "  - [Control Theory](#5)\n",
    "    + [Basic terminology of control theory](#6)\n",
    "    + [Challenges that may arise when controlling a system](#7)\n",
    "    + [Methods of control](#8)\n",
    "    + [Markov Decision Processes](#6)\n",
    "  - [Optimization Problems](#7)\n",
    "    + [Optimal Control](#8)\n",
    "* [Solving Optimal Control Problems](#9)\n",
    "  - [Analytical/ Planning Methods](#10)\n",
    "    + [Variational Calculus](#11)\n",
    "    + [Minimum Principle](#12)\n",
    "    + [Hamilton-Jacobi-Bellman Equation](#13)\n",
    "  - [Learning Methods](#14)\n",
    "    + [Dynamic Programming](#15)\n",
    "    + [Model-Based Reinforcemnt Learning](#16)\n",
    "    + [Model-Free Reinforment Learning](#17)\n",
    "* [Implementation of Solutions](#18)\n",
    "  - [Case Study 1: Trajectory Optimization](#19)\n",
    "    + [System Design](#20)\n",
    "    + [Control Implementation](#21)\n",
    "  - [Case Study 2: Adaptive Control](#22)\n",
    "    + [System Design](#23)\n",
    "    + [Control Implementation](#24)\n",
    "  - [Case Study 3: Stochastic Control](#25)\n",
    "    + [System Design](#26)\n",
    "    + [Control Implementation](#27)\n",
    "* [Code Design](#28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Introduction <a id =\"1\"></a>\n",
    "This introduction will consist of an overview of the core concepts behind optimal control theory and outline it's connection other fields of knowledge. This introduction will be dedicated to understanding the Optimal Control(OC) problems.\n",
    "\n",
    "To have a better understanding of OC problems I will begin by providing an overview of the core concepts behind optimal control theory including Dynamical Systems, Control Theory, Markov Decision Process and Optimization Problems. This is to help the reader \n",
    "\n",
    "The topics in this section are selected to provide the reader with a broad understanding of some of the concepts that are related to OC. I will start with a few topics about optimization in general and then move on to other methods for solving trajectory optimization problems.\n",
    "\n",
    "## Dynamical Systems <a id =\"2\"></a>\n",
    "Dynamical systems are mathematical models that describe how a system's state changes over time. They are used to study the long-term behavior of complex systems, such as the weather, population growth, and the motion of celestial bodies. Dynamical systems can exhibit a wide range of behaviors, including stability, chaos, and oscillation, depending on the system's parameters and initial conditions. If all the factors driving the change of a dynamical system can be identified, it is called a stochastic system. If none(or lim->0) the factors driving the change of a dynamical system can be identified, it is called a stochastic system.\n",
    "\n",
    "In the topic of Dynamical Systems I will cover \n",
    "1) Basic Terminology of Dynamical Systems\n",
    "2) Challenges that may arise when modelling a dynamical system\n",
    "3) Types of dynamical systems.\n",
    "\n",
    "### Basic terminology of dynamical systems <a id =\"3\"></a>\n",
    "A dynamic system is said to be:\n",
    "\n",
    "Stable for some class of initial states if its solution trajectories do not grow without bound,\n",
    "\n",
    "Unstable (or divergent) if the trajectories grow without bound, and\n",
    "\n",
    "Convergent if the solution trajectories approach a single point.\n",
    "\n",
    "Autonomous system if its dynamics are invariant in time.\n",
    "\n",
    "A **stable point** is a state $x$ such that for some neighborhood\n",
    "of $x$, the ODE is convergent toward $x$. A necessary condition for a\n",
    "point to be stable is that it is an *equilibrium point*.\n",
    "\n",
    "\n",
    "> **Equilibrium point**. For a continuous time dynamical system, a state $x$ such that $\\dot{x} = f(x) = 0$.  For a discrete time dynamical system, a state that satisfies $x = f(x)$.\n",
    "\n",
    "All stable points are equilibria, but the converse is not true, a point can be an equilibrium without being stable.\n",
    "\n",
    "\n",
    "### Challenges that may arise when modelling a dynamical system <a id =\"4\"></a>\n",
    "\n",
    "- Challenges that may arise when modelling a dynamical system:\n",
    "    \n",
    "    - **Unknown dynamics**: The system's dynamics are not well understood\n",
    "\n",
    "    - **High dimensional systems**:\n",
    "    \n",
    "    - **Chaotic/Nonlinear systems**:\n",
    "    \n",
    "    - **Hidden variables/ Partial observability**: Partial observability means that only certain aspects of the state can possibly be measured by the available sensors. For example, a mobile robot with a GPS sensor can only measure position, whereas it may need to model velocity as part of its state. State estimation techniques, such as Kalman filtering and particle filtering, can be used to extrapolate the unobserved components of state to provide reasonable state estimates. With those estimates, there will be some remaining localization error that the controller will still need to handle.\n",
    "    \n",
    "    - **Noise/ Disturbances**\n",
    "    \n",
    "    - **Scale differences/ changes**: Generally speaking, errors can be characterized as being either noisy or systematic. A noisy error is one obeys no obvious pattern each time it is measured. A systematic error is one that does obey a pattern. These deviations fall can also be categorized as **motion uncertainty** and **state uncertainty**. Disturbances are a form of motion uncertainty that cause the state to be moved in unexpected ways at future points in time. For example, wind gusts are very hard to predict in advance, and can move a drone from a desired path. Actuation error occurs when a desired control is not executed faithfully. These errors can be treated as motion uncertainty. Measurement error is a type of state uncertainty where due to sensor noise the state is observed incorrectly. Understanding measurement error is critical for closed-loop controllers which base their behavior on the measured state. Modeling error, means that the true dynamics function differs from the actual dynamics of the system. This is sometimes considered a third class of uncertainty, but could also be treated as state uncertainty.\n",
    "\n",
    "Motion uncertainty can be modeled as a disturbance to the dynamics\n",
    "\\dot{x} = f(x,u) + \\epsilon_d\n",
    " where \\epsilon_{d}(t) in $E_{d}$ is some\n",
    "error. Here $E_d$ is a set of possible disturbances, or a probability\n",
    "distribution over disturbances. Motion uncertainty will cause an\n",
    "open-loop system to \"drift\" from its intended trajectory over time. A\n",
    "properly designed closed-loop controller can regulate the disturbances\n",
    "by choosing controls that drive the system back to intended trajectory.\n",
    "\n",
    "In many cases it is convenient to talk about discrete-time systems in which time is no longer a continuous variable but a discrete quantity  t=0,1,2,â€¦, and the dynamics are specified in the form\n",
    "\n",
    "$x_{t+1}=f(x_{t},u_{t})$\n",
    "\n",
    "Here, the control is allowed to change only at discrete points in time, and the state is only observed at discrete points in time. This more accurately characterizes digital control systems which operate on a given clock frequency. However, in many situations the control frequency is so high that the continuous-time model (2) is appropriate.\n",
    "\n",
    "Usually systems of the form\n",
    "\n",
    "$\\ddot{x}=f(x,\\dot{x},t)$\n",
    "\n",
    "which relate state and controls to accelerations of the state  $\\ddot{x}=\\frac{dx^2}{d^2x}$\n",
    " . This does not seem to satisfy our definition of a dynamic system, since we've never seen a double time derivative. However, we can employ a stacking trick to define a first order system, but of twice the dimension. Let us define the stacked state vector\n",
    "\n",
    "  \\begin{align}\n",
    "    y &= \\begin{pmatrix}\n",
    "           x \\\\\n",
    "           \\dot{x}\n",
    "         \\end{pmatrix}\n",
    "  \\end{align}\n",
    "\n",
    "Then, we can rewrite ( 4 ) in a first-order form as:\n",
    "\n",
    "$\\dot{y}=g(y, u)$(6)\n",
    "where  $g(y, u)=f(x, \\dot{x}, u)$ simply \"unstacks\" the state and velocity from  $y$ . Now all of the machinery of first-order systems can be applied to the second order system. This can also be done for dynamic systems of order 3 and higher, wherein all derivatives are stacked into a single vector.\n",
    "### Types of dynamical systems <a id =\"5\"></a>\n",
    "\n",
    "## Control Theory <a id =\"6\"></a>\n",
    "Control theory is the branch of mathematics that deals with the behavior of dynamical systems. It is concerned\n",
    "with the design of control systems, which are systems that can be controlled to achieve a desired behavior.\n",
    "\n",
    "\n",
    "Feedback/closed-loop control is a control system that adjusts its output based on measured differences between the actual output and the desired output, with the goal of reducing those differences over time.\n",
    "\n",
    "### Basic terminology of control theory <a id =\"7\"></a>\n",
    "\n",
    "### Challenges that may arise when controlling a system <a id =\"8\"></a>\n",
    "\n",
    "### Methods of control <a id =\"9\"></a>\n",
    "\n",
    "### Markov Decision Processes <a id =\"10\"></a>\n",
    "A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making in situations where\n",
    "outcomes are partially random and partially under the control of a decision maker. An MDP is a\n",
    "5-tuple $(S,A,P,R,\\gamma)$, where:\n",
    "\n",
    "- $S$ is a set of states\n",
    "- $A$ is a set of actions\n",
    "- $P(s'|s,a)$ is the transition probability from state $s$ to state $\n",
    "- $R(s,a,s')$ is the reward function\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "The goal of an MDP is to find a policy $\\pi(a|s)$ that maximizes the expected cumulative reward over an infinite horizon.\n",
    "\n",
    "## Optimization Problems <a id =\"11\"></a>\n",
    "Optimization problems are problems that involve finding the best solution among a set of possible solutions. In the\n",
    "context of dynamical systems, optimization problems often involve finding the control inputs that minimize or\n",
    "maximize a cost function over time.\n",
    "\n",
    "### Optimal Control <a id =\"12\"></a>\n",
    "Optimal Control refers to the process of finding the control signals that will drive a dynamic system to a desired state while minimizing or maximizing a given performance criterion, such as time or energy consumption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Optimal Control Problems <a id =\"41\"></a>\n",
    "\n",
    "## Analytical/ Planning Methods <a id =\"42\"></a>\n",
    "Optimal control problems can be solved using analytical methods, such as the Pontryagin's Minimum\n",
    "Principle (PMP), which is a necessary condition for optimality. The PMP states that\n",
    "the Hamiltonian function, which is a combination of the state and control variables, must be\n",
    "minimized with respect to the control variable.\n",
    "\n",
    "### Variational Calculus <a id =\"43\"></a>\n",
    "Variational calculus is a method used to find the optimal solution to a problem by minimizing or maximizing a\n",
    "functional. It is used in optimal control problems where the goal is to find the optimal control input that\n",
    "minimizes or maximizes a performance criterion.\n",
    "\n",
    "### Minimum Principle <a id =\"44\"></a>\n",
    "The minimum principle is a method used to solve optimal control problems. It states that the optimal control input\n",
    "is the one that minimizes the Hamiltonian function, which is a function of the state, control input, and the adjoint variable.\n",
    "The adjoint variable is a function that is used to compute the gradient of the performance criterion with respect\n",
    "to the control input.\n",
    "\n",
    "### Hamilton-Jacobi-Bellman Equation <a id =\"45\"></a>\n",
    "The Hamilton-Jacobi-Bellman (HJB) equation is a partial differential equation that is used to solve optimal control problems. It is a dynamic programming approach that computes the optimal control input by minimizing the performance criterion over a finite horizon.\n",
    "\n",
    "## Learning Methods <a id =\"46\"></a>\n",
    "Learning methods, such as reinforcement learning, can be used to solve optimal control problems. These methods\n",
    "learn the optimal control input by interacting with the environment and receiving feedback in the form of rewards\n",
    "or penalties.\n",
    "\n",
    "### Dynamic Programming <a id =\"47\"></a>\n",
    "Dynamic programming is a method used to solve optimal control problems by breaking down the problem into smaller\n",
    "sub-problems and solving each sub-problem recursively. It is used in reinforcement learning to compute the\n",
    "optimal control input by minimizing the cumulative reward over a finite horizon.\n",
    "\n",
    "### Model-Based Reinforcment Learning <a id =\"48\"></a>\n",
    "Model-based reinforcement learning is a method used to solve optimal control problems by learning a model of the\n",
    "environment and using it to compute the optimal control input. It is used in applications where the environment\n",
    "is complex and difficult to model.\n",
    "\n",
    "### Model-Free Reinforcement Learning <a id =\"49\"></a>\n",
    "Model-free reinforcement learning is a method used to solve optimal control problems by learning the optimal control\n",
    "input directly from experience without learning a model of the environment. It is used in applications where the\n",
    "environment is simple and easy to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Solutions <a id =\"43\"></a>\n",
    "\n",
    "## Case Study 1: Trajectory Optimization <a id =\"43\"></a>\n",
    "\n",
    "### Problem Formulation <a id =\"43\"></a>\n",
    "The problem is to find the optimal trajectory for a spacecraft to travel from Earth to Mars. The spacecraft\n",
    "has a limited amount of fuel, and the trajectory must be optimized to minimize the time of travel.\n",
    "The problem can be formulated as a nonlinear programming problem.\n",
    "\n",
    "\n",
    "![Without fixed end-point.](https://github.com/NiyiOsinowo/Optimal-Control/blob/main/Documentation/Screenshot%202024-08-26%20at%2014.00.10.png)\n",
    "\n",
    "### System Design <a id =\"43\"></a>\n",
    "The system will be designed to optimize the trajectory of a spacecraft. The system will take in the initial\n",
    "position, velocity, and acceleration of the spacecraft, as well as the target position and velocity. The\n",
    "system will then use a numerical optimization algorithm to find the optimal trajectory that minimizes the\n",
    "time of flight.\n",
    "\n",
    "### Control Implementation <a id =\"43\"></a>\n",
    "The control implementation will be based on a model predictive control (MPC) approach. The MPC will\n",
    "use a linear quadratic regulator (LQR) to optimize the control inputs at each time step. The\n",
    "optimization problem will be formulated as a quadratic program, which will be solved using a\n",
    "quadratic programming solver.\n",
    "\n",
    "## Case Study 2: Adaptive Control <a id =\"43\"></a>\n",
    "\n",
    "### Problem Formulation <a id =\"43\"></a>\n",
    "The problem is to design an adaptive control system for a nonlinear system with unknown parameters.\n",
    "The system will be modeled using a nonlinear state-space equation, and the control objective will be to\n",
    "track a reference trajectory.\n",
    "\n",
    "### System Design <a id =\"43\"></a>\n",
    "The system will be designed to adapt to changes in the environment. The system will take in sensor data\n",
    "from the environment and use this data to update the control inputs in real-time. The system will use\n",
    "machine learning algorithms to learn the dynamics of the environment and adapt the control inputs\n",
    "accordingly.\n",
    "\n",
    "### Control Implementation <a id =\"43\"></a>\n",
    "The control implementation will be based on a model reference adaptive control (MRAC) approach. The\n",
    "MRAC will use a least squares estimator to estimate the parameters of the plant model. The\n",
    "estimated parameters will then be used to compute the control inputs using a linear quadratic\n",
    "regulator (LQR).\n",
    "\n",
    "## Case Study 3: Stochastic Control<a id =\"43\"></a>\n",
    "### Problem Formulation <a id =\"43\"></a>\n",
    "The problem is to design a stochastic control system for a system with random disturbances. The system\n",
    "will be modeled using a stochastic state-space equation, and the control objective will be to minimize\n",
    "the expected value of a cost function.\n",
    "\n",
    "### System Design <a id =\"43\"></a>\n",
    "The system will be designed to handle stochastic disturbances. The system will take in sensor data from\n",
    "the environment and use this data to update the control inputs in real-time. The system will use a\n",
    "Kalman filter to estimate the state of the system and a linear quadratic regulator (LQR) to\n",
    "compute the control inputs.\n",
    "\n",
    "### Control Implementation <a id =\"43\"></a>\n",
    "The control implementation will be based on a stochastic linear quadratic regulator (SLQR) approach.\n",
    "The SLQR will use a Kalman filter to estimate the state of the system and a linear quadratic regulator (LQR) to compute the control inputs. The SLQR will also use a stochastic optimization algorithm to optimize the control inputs in real-time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
