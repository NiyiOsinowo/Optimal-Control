{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import scipy.integrate as integrate\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/niyi/Documents/GitHub/Optimal-Control/Tools')\n",
    "from OUNoise import OUNoise\n",
    "from EnforceTyping import EnforceClassTyping, enforce_method_typing, enforce_function_typing\n",
    "from ParticlesandFields import ClassicalField, ClassicalParticle\n",
    "from MDPFramework import MDPEnvironment,  LearningAgent\n",
    "from ActorCriticNetworks import ActorNetwork, CriticNetwork\n",
    "T.Tensor.ndim = property(lambda self: len(self.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ElectrostaticField2D(ClassicalField):\n",
    "  \"\"\"\n",
    "  A class used to represent a 2D Electrostatic Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  field_sources: dict\n",
    "      a formatted string to print out what the animal says\n",
    "  dimensionality: tuple\n",
    "      a tuple of the dimensionality of the field  \n",
    "\n",
    "  Methods\n",
    "  -------\n",
    "  dynamics(self, observation_position: np.ndarray, time: float) -> np.ndarray:\n",
    "      Represents the value of the field at any given point(s) or time. \n",
    "  potential(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential due to the field at a given position and/or time  \n",
    "  potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray, time: float) -> float:\n",
    "      Represents the potential difference between two positions at a given time in the vector field   \n",
    "  gradient(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the gradient at a given position and/or time in the vector field \n",
    "  curl(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the curl at a given position and/or time in the vector field \n",
    "  divergence(self, observation_position: np.ndarray, time: float) -> float:\n",
    "      Represents the divergence at a given position and/or time in the vector field\n",
    "  \"\"\"\n",
    "  field_sources: dict\n",
    "  dimensionality: tuple = (2,)\n",
    "\n",
    "  def __call__(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      return self.dynamics(observation_position)\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def dynamics(self, observation_position: np.ndarray) -> np.ndarray:\n",
    "      \"\"\"\n",
    "      This function outputs the field strength due to field sources experienced at any given point(s) or time. \n",
    "      This determines the physics of the field (a 2D Electricstatic Field in this case)\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric field strength vector at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_field_vector = np.zeros_like(observation_position)\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_field_vector += (displacement_vector * field_source.charge) / displacement_magnitude**3\n",
    "      electric_field_vector = coulomb_constant * electric_field_vector\n",
    "      return np.round(electric_field_vector, 3)  # N/C or V/m\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential(self, observation_position: np.ndarray) -> float:\n",
    "      \"\"\"\n",
    "      Calculate the potential (voltage) at a position in the field.\n",
    "\n",
    "      Args:\n",
    "          observation_position (np.ndarray): The position.\n",
    "\n",
    "      Returns:\n",
    "          np.ndarray: The electric potential at the given position.\n",
    "      \"\"\"\n",
    "      assert len(self.field_sources[\"Particle\"]) == len(self.field_sources[\"Position\"]), \"The length of particles and fields don't match\"\n",
    "      for field_source, _ in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          assert isinstance(field_source, ClassicalParticle),  \"The field source is not a particle\"\n",
    "  \n",
    "      coulomb_constant = 8.9875e9  # N*m^2/C^2\n",
    "      electric_potential = 0.0\n",
    "\n",
    "      for field_source, source_position in zip(self.field_sources[\"Particle\"], self.field_sources[\"Position\"]):\n",
    "          position_matrices = np.broadcast_to(source_position, reversed(observation_position.shape)).T\n",
    "          displacement_vector = observation_position - position_matrices\n",
    "          displacement_magnitude = np.linalg.norm(displacement_vector, axis=0)\n",
    "          electric_potential += field_source.charge / displacement_magnitude\n",
    "\n",
    "      electric_potential = coulomb_constant * electric_potential\n",
    "      return np.round(electric_potential, 3)  # V\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def potential_difference(self, initial_position: np.ndarray, final_position: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the potential difference between the initial position and the final position in the field.\n",
    "\n",
    "    Args:\n",
    "        initial_position (np.ndarray): The starting position.\n",
    "        final_position (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    assert initial_position.shape == self.dimensionality, \"initial_position has the wrong dimensions\"\n",
    "    assert final_position.shape == self.dimensionality, \"final_position has the wrong dimensions\"\n",
    "    PorentialDifference= self.potential(initial_position)- self.potential(final_position)\n",
    "    return PorentialDifference\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def gradient(self, observation_position: np.ndarray, delta: float= 0.001)->np.ndarray:\n",
    "    \"\"\"\n",
    "    This function returns the derivative of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    gradient= np.zeros_like(observation_position)\n",
    "    for i in range(len(observation_position)):\n",
    "      di= np.zeros_like(observation_position)\n",
    "      di[i, ] = di[i, ]+delta\n",
    "      plusdi= observation_position+ di\n",
    "      minusdi= observation_position- di\n",
    "      gradient[i]= (self.dynamics(plusdi)- self.dynamics(minusdi))[i]/ (2* delta)\n",
    "    return gradient\n",
    "  \n",
    "  def curl(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the curl of the field at a given point\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  def divergence(self, observation_position: np.ndarray)-> float:\n",
    "    \"\"\"\n",
    "    This function returns the divergence of the field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    pass\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def plot_field(self, low_bound= -20, high_bound= 20):\n",
    "    \"\"\"\n",
    "    This funtion plots the 2D electric vector field\n",
    "\n",
    "    Args:\n",
    "        InitialPosition (np.ndarray): The starting position.\n",
    "        FinalPosition (np.ndarray): The ending position.\n",
    "        resolution (int, optional): The number of intervals to divide the path into. Defaults to 5000.\n",
    "\n",
    "    Returns:\n",
    "        float: The work required to move from the initial position to the final position.\n",
    "    \"\"\"\n",
    "    observation_position= np.meshgrid(np.linspace(low_bound, high_bound, 25), \n",
    "                                    np.linspace(low_bound, high_bound, 25))\n",
    "    observation_position= np.stack(observation_position)\n",
    "    xd, yd = self.dynamics(observation_position)\n",
    "    xd = xd / np.sqrt(xd**2 + yd**2)\n",
    "    yd = yd / np.sqrt(xd**2 + yd**2)\n",
    "    color_aara = np.sqrt(xd**2+ yd**2)\n",
    "    fig, ax = plt.subplots(1,1)\n",
    "    cp = ax.quiver(observation_position[0],observation_position[1],xd,yd,color_aara)\n",
    "    fig.colorbar(cp)\n",
    "    plt.rcParams['figure.dpi'] = 150\n",
    "    plt.show()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class ParticleInField(MDPEnvironment):\n",
    "  \"\"\"\n",
    "  A class used to represent a particle in a Field\n",
    "\n",
    "  Attributes\n",
    "  ----------\n",
    "  field: ClassicalField\n",
    "    The field that the particle is in \n",
    "  particle: ClassicalParticle\n",
    "    The particle that is in the field\n",
    "  target: np.ndarray \n",
    "    The target position of the particle\n",
    "  distance_weight: float \n",
    "    The weight of the distance between the particle and the target\n",
    "  energy_weight: float \n",
    "    The weight of the energy of the particle\n",
    "  terminal_signal_weight: float \n",
    "    The weight of the terminal signal of the particle\n",
    "  current_time: float \n",
    "    The current time of the system\n",
    "\n",
    "  Methods\n",
    "  ------- \n",
    "  transition_model(self, state: State, action: Any)-> State: \n",
    "    Represents the  \n",
    "  reward_model(self, state: State, action: Any, next_state: State, terminal_signal: bool)-> float:\n",
    "    Represents the reward of the system\n",
    "  is_terminal_condition(self, state: State)-> bool: \n",
    "    Represents the terminal condition of the system\n",
    "  transition_step(self, state: State, action: Any)-> tuple[float, State, bool]: \n",
    "    Represents the transition step of the system\n",
    "  sample_trajectory(self, runtime: float)-> list[State]: \n",
    "    Samples a trajectory of the system\n",
    "  trajectory_value(self, trajectory: list[State])-> float: \n",
    "    Represents the value of the trajectory\n",
    "  reset(self): \n",
    "    Resets the system\n",
    "\n",
    "  \"\"\"\n",
    "  field: ClassicalField\n",
    "  particle: ClassicalParticle\n",
    "  target: np.ndarray # m\n",
    "  distance_weight: float= 1.0\n",
    "  energy_weight: float= -1.0\n",
    "  terminal_signal_weight: float= -1000.0\n",
    "  current_time: float = 0.0# s\n",
    "  state_dims: tuple= None\n",
    "  action_dims: tuple= None\n",
    "  @dataclass \n",
    "  class State(EnforceClassTyping):\n",
    "    '''This class represents the state of the Agent with its position, velocity and the Field Strength if experiences at its position. \n",
    "    These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "    position: np.ndarray # m\n",
    "    velocity: np.ndarray #m/s\n",
    "    \n",
    "    def vector(self):\n",
    "      return np.concatenate([self.position, self.velocity])\n",
    "    \n",
    "  initial_state: State = None\n",
    "  current_state: State = None\n",
    "\n",
    "  def __post_init__(self):\n",
    "    if self.initial_state is None:\n",
    "        self.initial_state= self.random_state()\n",
    "    self.current_state= self.initial_state\n",
    "    self.action_dims= self.field.dimensionality\n",
    "    self.state_dims= self.field.dimensionality * 2\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def state_dynamics(self, state: np.ndarray, time: float, control_force: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the dynamics of the particle's state.\n",
    "\n",
    "    Parameters:\n",
    "    state (np.ndarray): The current state of the particle [x, y, vx, vy].\n",
    "    time (float): The current time.\n",
    "    control_force (np.ndarray): The external control force applied to the particle.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: The derivative of the state [vx, vy, ax, ay].\n",
    "    \"\"\"\n",
    "    velocity = state[2:]\n",
    "    acceleration = (self.particle.charge * self.field.dynamics(state[:2]) + control_force) / self.particle.mass\n",
    "    return np.concatenate((velocity, acceleration))\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def transition_model(self, state: State, action: np.ndarray= np.array([0.0, 0.0]), time_interval:float= 0.1)-> State:\n",
    "    \"\"\"\n",
    "    Computes the next state of the system after applying a constant force for a given time interval.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state of the system.\n",
    "        action (np.ndarray, optional): The constant force to apply. Defaults to [0.0, 0.0].\n",
    "        time_interval (float, optional): The time interval to apply the force for. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        State: The next state of the system.\n",
    "    \"\"\"\n",
    "    t_span = [self.current_time, self.current_time + time_interval]\n",
    "    next_state_vector = integrate.odeint(self.state_dynamics, state.vector(), t_span, args=(action,))[-1]\n",
    "    next_position = next_state_vector[:2]\n",
    "    next_velocity = next_state_vector[2:]\n",
    "    return self.State(next_position, next_velocity)\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def reward_model(self, state: State, action: np.ndarray, next_state: State, terminal_signal: bool) -> float:\n",
    "      \"\"\"\n",
    "      Computes the reward for the agent given a state transition.\n",
    "\n",
    "      The reward is a weighted sum of three components:\n",
    "      1. Distance gained towards the target\n",
    "      2. Energy consumed during the transition\n",
    "      3. Terminal signal (e.g. reaching the target or running out of energy)\n",
    "\n",
    "      Args:\n",
    "          state: The current state of the agent\n",
    "          action: The action taken by the agent\n",
    "          next_state: The resulting state after taking the action\n",
    "          terminal_signal: A boolean indicating whether the episode has terminated\n",
    "\n",
    "      Returns:\n",
    "          float: The reward value\n",
    "      \"\"\"\n",
    "      distance_gained = np.linalg.norm(state.position - self.target) - np.linalg.norm(next_state.position - self.target)\n",
    "      energy_consumed = np.linalg.norm(action)\n",
    "      reward = (\n",
    "          self.distance_weight * distance_gained\n",
    "          + self.energy_weight * energy_consumed\n",
    "          + self.terminal_signal_weight * int(terminal_signal)\n",
    "      )\n",
    "      return reward\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def is_terminal_condition(self, state: State) -> bool:\n",
    "      \"\"\"\n",
    "      Checks if the state is outside the viable learning region of the environment.\n",
    "\n",
    "      Args:\n",
    "          state (State): The current state of the environment.\n",
    "\n",
    "      Returns:\n",
    "          bool: True if the state is outside the viable learning region, False otherwise.\n",
    "      \"\"\"\n",
    "      x_bound = -10.0 <= state.position[0] <= 10.0\n",
    "      y_bound = -10.0 <= state.position[1] <= 10.0\n",
    "      velocity_bound = np.linalg.norm(state.velocity) < 10.0\n",
    "\n",
    "      return not (x_bound and y_bound and velocity_bound)\n",
    "  \n",
    "  @enforce_method_typing\n",
    "  def transition_step(\n",
    "      self, \n",
    "      state: State, \n",
    "      action: np.ndarray = np.array([0.0, 0.0]), \n",
    "      time_interval: float = 0.1\n",
    "  ) -> Tuple[State, float, bool]:\n",
    "      \"\"\"\n",
    "      Simulates a single time step of the environment.\n",
    "\n",
    "      Args:\n",
    "          state (State): The current state of the environment. Defaults to current_state.\n",
    "          action (np.ndarray): The action to take in the environment. Defaults to [0.0, 0.0].\n",
    "          time_interval (float): The time interval for the simulation. Defaults to 0.1.\n",
    "\n",
    "      Returns:\n",
    "          Tuple[State, float, bool]: A tuple containing the next state, the reward, and a terminal signal.\n",
    "      \"\"\"\n",
    "      next_state = self.transition_model(state, action, time_interval=time_interval)\n",
    "      terminal_signal = self.is_terminal_condition(next_state)\n",
    "      reward = self.reward_model(state, action, next_state, terminal_signal)\n",
    "      return next_state, reward, terminal_signal\n",
    "\n",
    "  def random_state(self) -> State:\n",
    "      \"\"\"\n",
    "      Generates a random state within the viable learning region.\n",
    "\n",
    "      Returns:\n",
    "          State: A random state within the viable learning region\n",
    "      \"\"\"\n",
    "      position = np.random.uniform(-10.0, 10.0, size=self.field.dimensionality)\n",
    "      velocity = np.zeros_like(position)\n",
    "      return self.State(position, velocity)\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def sample_trajectory(\n",
    "      self, \n",
    "      runtime: float, \n",
    "      initial_state: State = None, \n",
    "      n_steps: int = 200\n",
    "  ) -> Tuple[List[Any], List[np.ndarray], List[float]]:\n",
    "      \"\"\"\n",
    "      Generates a random state trajectory within the viable learning region.\n",
    "\n",
    "      Args:\n",
    "      - runtime (float): The total time for the trajectory in seconds.\n",
    "      - initial_state (State): The initial state of the trajectory. Defaults to current_state.\n",
    "      - n_steps (int): The number of steps in the trajectory. Defaults to 200.\n",
    "\n",
    "      Returns:\n",
    "      - A tuple containing the state trajectory, action trajectory, and time points.\n",
    "      \"\"\"\n",
    "      time_interval = runtime/n_steps\n",
    "      if initial_state == None:\n",
    "         state = self.current_state\n",
    "      else:\n",
    "         state = initial_state\n",
    "      time= 0.0\n",
    "      state_trajectory = []\n",
    "      time_points = np.linspace(time, runtime, n_steps)\n",
    "      return_value= 0.0\n",
    "\n",
    "      for t in time_points:\n",
    "          state_trajectory.append(state)\n",
    "          state, reward, _ = self.transition_step(state, time_interval=time_interval)\n",
    "          return_value += reward\n",
    "      return state_trajectory, return_value, time_points\n",
    "\n",
    "  @enforce_method_typing\n",
    "  def plot_trajectory(self, state_trajectory: list, time: np.ndarray) -> None:\n",
    "      \"\"\"\n",
    "      Plot the trajectory of states over time.\n",
    "\n",
    "      Args:\n",
    "      - state_trajectory: A list of States representing the trajectory.\n",
    "      - time: A list of time points corresponding to each state in the trajectory.\n",
    "\n",
    "      Returns:\n",
    "      - None (plots the trajectory)\n",
    "      \"\"\"\n",
    "      positions = np.array([state.position for state in state_trajectory])\n",
    "      velocities = np.array([state.velocity for state in state_trajectory])\n",
    "\n",
    "      plt.figure(figsize=(8, 8))\n",
    "      plt.plot(positions[:, 0], positions[:, 1], label='Trajectory')\n",
    "      plt.scatter(positions[0, 0], positions[0, 1], c='k', marker='o', label='Start')\n",
    "      plt.scatter(positions[-1, 0], positions[-1, 1], c='r', marker='*', label='End')\n",
    "\n",
    "      plt.xlim(-10, 10)\n",
    "      plt.ylim(-10, 10)\n",
    "      plt.grid(True)\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure(figsize=(8, 8))\n",
    "      plt.plot( time, velocities[:, 0], label='Velocity x')\n",
    "      plt.plot( time, velocities[:, 1], label='Velocity y')\n",
    "      plt.grid(True)\n",
    "      plt.legend()\n",
    "      plt.show()\n",
    "\n",
    "  def reset(self) -> None:\n",
    "      \"\"\"\n",
    "      Resets the current state to the initial state and sets the current time to 0.0.\n",
    "      \"\"\"\n",
    "      self.current_state = self.initial_state\n",
    "      self.current_time = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DDPGAgent(LearningAgent):\n",
    "\n",
    "    def __init__(self, \n",
    "                 environment: MDPEnvironment, \n",
    "                 actor_layers: tuple,\n",
    "                 critic_layers: tuple,\n",
    "                 actor_activations: tuple, \n",
    "                 critic_activations: tuple,\n",
    "                 observation_size: int, \n",
    "                 action_size: int,\n",
    "                 actor_learning_rate: float,\n",
    "                 critic_learning_rate: float,\n",
    "                 soft_update_rate: float,\n",
    "                 ControlInterval: float = 0.5,\n",
    "                 discount_rate: float =0.99,\n",
    "                 max_size: int= 1000,\n",
    "                 batch_size: int= 6):\n",
    "        self.environment= environment\n",
    "        self.policy: ActorNetwork = ActorNetwork(observation_size, action_size, actor_layers, actor_activations, 'DDPGMainActor', actor_learning_rate)\n",
    "        self.value_estimator = CriticNetwork(observation_size, action_size, critic_layers, critic_activations, 'DDPGMainCritic', critic_learning_rate)\n",
    "        self.target_policy = ActorNetwork(observation_size, action_size, actor_layers, actor_activations, 'DDPGTargetActor', actor_learning_rate)\n",
    "        self.target_value_estimator = CriticNetwork(observation_size, action_size, critic_layers, critic_activations, 'DDPGTargetCritic', critic_learning_rate)\n",
    "        for target_param, param in zip(self.target_policy.parameters(), self.policy.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.target_value_estimator.parameters(), self.value_estimator.parameters()):\n",
    "            target_param.data.copy_(param.data) \n",
    "        self.memory = deque(maxlen=max_size)\n",
    "        self.soft_update_rate= soft_update_rate\n",
    "        self.ControlInterval= ControlInterval\n",
    "        self.discount_rate= discount_rate\n",
    "        self.batch_size= batch_size\n",
    "        self.noise = OUNoise(mu=np.zeros(action_size))\n",
    "        self.ControlInterval= self.ControlInterval# Action duration= conrol interval\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def observe(self, state= None):\n",
    "        if state is None:\n",
    "          state= self.environment.current_state   \n",
    "        observation= T.Tensor(state.vector())\n",
    "        return observation\n",
    "  \n",
    "    def act(self, observation: T.Tensor, with_noise: bool= True):\n",
    "        self.policy.eval()\n",
    "        observation = observation.to(self.policy.device)\n",
    "        action = self.policy.forward(observation).to(self.policy.device)\n",
    "        noise= T.tensor(self.noise(), dtype=T.float)\n",
    "        noisy_action = (1e-7* (action + noise)).to(self.policy.device)\n",
    "        if with_noise:\n",
    "            return noisy_action.cpu().detach()\n",
    "        else:\n",
    "            return action.cpu().detach()\n",
    "     \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        observations, actions, next_observations, rewards, dones = zip(*batch)\n",
    "\n",
    "        state = T.stack(observations).to(self.value_estimator.device)\n",
    "        action = T.stack(actions).to(self.value_estimator.device)\n",
    "        reward = T.tensor(rewards, dtype=T.float).unsqueeze(1).to(self.value_estimator.device)\n",
    "        new_state = T.stack(next_observations).to(self.value_estimator.device)\n",
    "        done = T.tensor(dones, dtype=T.float).unsqueeze(1).to(self.value_estimator.device)\n",
    "        \n",
    "        self.target_policy.eval()\n",
    "        self.target_value_estimator.eval()\n",
    "        self.value_estimator.eval()\n",
    "        \n",
    "        target_actions = self.target_policy.forward(new_state)\n",
    "        Critic_value_ = self.target_value_estimator.forward(new_state, target_actions) \n",
    "        q_expected = self.value_estimator.forward(state, action)\n",
    "        q_targets = reward + self.discount_rate * Critic_value_ * (1 - done)\n",
    "\n",
    "        Critic_loss = nn.MSELoss()(q_expected, q_targets.detach())\n",
    "        self.value_estimator.train()\n",
    "        self.value_estimator.optimizer.zero_grad()\n",
    "        Critic_loss.backward()\n",
    "        self.value_estimator.optimizer.step()\n",
    "\n",
    "        self.policy.eval()\n",
    "        self.value_estimator.eval()\n",
    "\n",
    "        mu = self.policy.forward(state)\n",
    "        Actor_loss = -self.value_estimator.forward(state, mu)\n",
    "\n",
    "        Actor_loss = T.mean(Actor_loss)\n",
    "        self.policy.train()\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        Actor_loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def sample_route(self, runtime: float, n_steps: int=100):\n",
    "        route= []\n",
    "        route_return= 0.0\n",
    "        state= self.environment.initial_state\n",
    "        initial_time= 0.0\n",
    "        time_points = np.linspace(initial_time, runtime, n_steps)\n",
    "        for _ in time_points:\n",
    "            observation= self.observe(state)\n",
    "            route.append(observation)\n",
    "            action= self.act(observation)\n",
    "            state, reward, _= self.environment.transition_step(state, action)\n",
    "            route_return += reward\n",
    "        return route, route_return\n",
    "    \n",
    "    def plot_route(self, RunDuration: float):\n",
    "        Path= self.Run(RunDuration)\n",
    "        Path= T.stack(Path)\n",
    "        Path= Path.transpose(dim0=0, dim1=1)\n",
    "        # print(Path)\n",
    "        t=  T.arange(0, RunDuration)\n",
    "        plt.plot(Path[0], Path[1])\n",
    "        plt.plot(Path[0][0], Path[1][0], 'ko')\n",
    "        plt.plot(Path[0][-1], Path[1][-1], 'r*')\n",
    "        plt.xlim(-10,10)\n",
    "        plt.ylim(-10,10)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    def update_network_parameters(self, SoftUpdateRate=1):\n",
    "        if SoftUpdateRate is None:\n",
    "            SoftUpdateRate = self.soft_update_rate\n",
    "\n",
    "        Critic_state_dict = dict(self.value_estimator.named_parameters())\n",
    "        Actor_state_dict = dict(self.policy.named_parameters())\n",
    "        TargetCritic_dict = dict(self.target_value_estimator.named_parameters())\n",
    "        TargetActor_dict = dict(self.target_policy.named_parameters())\n",
    "\n",
    "        for name in Critic_state_dict:\n",
    "            Critic_state_dict[name] = SoftUpdateRate*Critic_state_dict[name].clone() + (1-SoftUpdateRate)*TargetCritic_dict[name].clone()\n",
    "        self.target_value_estimator.load_state_dict(Critic_state_dict)\n",
    "\n",
    "        for name in Actor_state_dict:\n",
    "            Actor_state_dict[name] = SoftUpdateRate*Actor_state_dict[name].clone() + (1-SoftUpdateRate)*TargetActor_dict[name].clone()\n",
    "        self.target_policy.load_state_dict(Actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        TargetActor_params = self.TargetActor.named_parameters()\n",
    "        TargetCritic_params = self.TargetCritic.named_parameters()\n",
    "\n",
    "        Critic_state_dict = dict(TargetCritic_params)\n",
    "        Actor_state_dict = dict(TargetActor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.Actor.named_parameters():\n",
    "            print(name, T.equal(param, Actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.Critic.named_parameters():\n",
    "            print(name, T.equal(param, Critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.policy.save_checkpoint()\n",
    "        self.target_policy.save_checkpoint()\n",
    "        self.value_estimator.save_checkpoint()\n",
    "        self.target_value_estimator.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.policy.load_checkpoint()\n",
    "        self.target_policy.load_checkpoint()\n",
    "        self.value_estimator.load_checkpoint()\n",
    "        self.target_value_estimator.load_checkpoint()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DDPGAlgorithm(environment: MDPEnvironment, agent: DDPGAgent, n_episodes: int, episode_duration: int):\n",
    "    return_history = []\n",
    "    for _ in range(n_episodes):\n",
    "        environment.reset()\n",
    "        terminal_signal = False\n",
    "        episode_return = 0\n",
    "        for _ in range(episode_duration):\n",
    "            observation= agent.observe()\n",
    "            action = agent.act(observation) \n",
    "            new_state, reward, terminal_signal= environment.transition_step(environment.current_state, np.array(action), agent.ControlInterval) \n",
    "            agent.memory.append((observation, action, agent.observe(new_state), reward, int(terminal_signal)))\n",
    "            agent.learn()\n",
    "            episode_return += reward\n",
    "            environment.current_state = new_state\n",
    "        return_history.append(episode_return)\n",
    "    plt.plot(return_history)\n",
    "    return return_history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_charge= ClassicalParticle(mass=1.0, charge= -1e-9)\n",
    "positive_charge= ClassicalParticle(mass=1.0, charge= 1e-9)\n",
    "sources = {\"Particle\": [negative_charge, positive_charge],\n",
    "           \"Position\": [np.array([1.0, 1.0]), np.array([-1.0, 1.0])]}\n",
    "test_electric_field= ElectrostaticField2D(field_sources=sources)\n",
    "point_charge_in_electric_field= ParticleInField(field=test_electric_field, \n",
    "                                               particle=positive_charge, \n",
    "                                               target=np.array([0.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor_layer= (10, 5)\n",
    "test_critic_layer= (10, 5)\n",
    "test_actor_activations= (nn.ReLU(), nn.ReLU())\n",
    "test_critic_activations= (nn.ReLU(), nn.ReLU())\n",
    "test_agent = DDPGAgent(point_charge_in_electric_field,\n",
    "                       actor_layers=test_actor_layer, \n",
    "                       critic_layers=test_critic_layer,\n",
    "                       actor_activations= test_actor_activations,\n",
    "                       critic_activations= test_critic_activations,\n",
    "                       observation_size= 4,\n",
    "                       action_size= 2,\n",
    "                       actor_learning_rate= 0.01,\n",
    "                       critic_learning_rate= 0.01,\n",
    "                       soft_update_rate=0.01)\n",
    "# test_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state= test_agent.observe()\n",
    "test_action= test_agent.act(test_state)\n",
    "test_value= test_agent.value_estimator.forward(test_state, test_action)\n",
    "test_state, test_action, test_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DDPGAlgorithm(point_charge_in_electric_field, test_agent, 20, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
