{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This repository contains a detailed implementation of the Reinforcement Learning Enviroment class'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import *\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Any, Callable, Dict, List, Tuple, Union, Optional\n",
    "from functools import wraps\n",
    "import os\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque, namedtuple\n",
    "T.Tensor.ndim = property(lambda self: len(self.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EnforceClassTyping:\n",
    "    def __post_init__(self):\n",
    "        for (name, field_type) in self.__annotations__.items():\n",
    "            if not isinstance(self.__dict__[name], field_type):\n",
    "                current_type = type(self.__dict__[name])\n",
    "                raise TypeError(f\"The field `{name}` was assigned by `{current_type}` instead of `{field_type}`\")\n",
    "        # print(\"Check is passed successfully\")\n",
    "def EnforceMethodTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for class mathods'\n",
    "    arg_annotations = func.__annotations__\n",
    "    if not arg_annotations:\n",
    "        return func\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> Any:\n",
    "        for arg, annotation in zip(args, arg_annotations.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for argument {arg}, got {type(arg)}.\")\n",
    "\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in arg_annotations:\n",
    "                annotation = arg_annotations[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for keyword argument {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(self, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "def EnforceFunctionTyping(func: Callable) -> Callable:\n",
    "    'Enforces type annotation/hints for other functions'\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Check positional arguments\n",
    "        for arg, annotation in zip(args, func.__annotations__.values()):\n",
    "            if not isinstance(arg, annotation):\n",
    "                raise TypeError(f\"Expected {annotation} for {arg}, got {type(arg)}.\")\n",
    "\n",
    "        # Check keyword arguments\n",
    "        for arg_name, arg_value in kwargs.items():\n",
    "            if arg_name in func.__annotations__:\n",
    "                annotation = func.__annotations__[arg_name]\n",
    "                if not isinstance(arg_value, annotation):\n",
    "                    raise TypeError(f\"Expected {annotation} for {arg_name}, got {type(arg_value)}.\")\n",
    "\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ornstein Uhlenbeck Noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise(object):\n",
    "    def __init__(self, mu, sigma=0.15, theta=.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "            self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(\n",
    "                                                            self.mu, self.sigma)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Particle(EnforceClassTyping):\n",
    "    'This class represents the electric field sources with its position in the field(Position) and the magnitude of the source(Charge)'\n",
    "    Mass: float # kg\n",
    "    Charge: float #C\n",
    "    Position: T.Tensor # m\n",
    "    Velocity: T.Tensor # m/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Particle Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Electron= Particle(Mass=9.11e-8, Charge= -1.6e-9, Position=T.tensor([1.0, 0.0]), Velocity=T.tensor([0.0, 0.0]))\n",
    "Proton= Particle(Mass=9.11e-8, Charge= 1.6e-9, Position=T.tensor([-1.0, 0.0]), Velocity=T.tensor([0.0, 0.0]))\n",
    "Source= [Electron]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Field:\n",
    "    Dimensions: int\n",
    "    FieldHighBound: list[float]\n",
    "    FieldLowBound: list[float]\n",
    "    def __post_init__(self):\n",
    "        assert  len(self.FieldHighBound) == self.Dimensions| 1, \"Length of high bound and dimensions do not match\"\n",
    "        assert  len(self.FieldLowBound) == self.Dimensions| 1, \"Length of low bound and dimensions do not match\"\n",
    "    @abstractmethod\n",
    "    def FieldStrength(self, ObservationPosition: T.Tensor)-> T.Tensor:\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def FieldPotential(self, ObservationPosition: T.Tensor)-> float:\n",
    "        pass\n",
    "\n",
    "class HomogenousField(Field):\n",
    "    def FieldStrength(self, ObservationPosition: T.Tensor)-> T.Tensor:\n",
    "        return  T.zeros((ObservationPosition.shape[0], self.Dimensions), dtype=T.float64)\n",
    "    def FieldPotential(self, ObservationPosition: T.Tensor)-> float:\n",
    "        return  0.0\n",
    "\n",
    "@dataclass\n",
    "class LJField:\n",
    "    FieldSources: list[Particle]\n",
    "    FieldHighBound: float\n",
    "    FieldLowBound: float\n",
    "    def __call__(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        return self.ElectricFieldStrength(ObservationPosition)\n",
    "    @EnforceMethodTyping\n",
    "    def ElectricFieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "        CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "        for FieldSource in self.FieldSources:\n",
    "            if type(FieldSource) != Particle:\n",
    "                raise TypeError(\"The input is not valid\")\n",
    "        assert type(ObservationPosition) == T.Tensor, \"Invalid Reference point data type\"\n",
    "        ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "        for FieldSource in self.FieldSources:\n",
    "            PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                            T.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "            DisplacementVector = ObservationPosition - PositionMatrices\n",
    "            DisplacementMagnitude = T.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "            ElectricFieldVector += ((FieldSource.Charge) / DisplacementMagnitude**3 * DisplacementVector) - ((FieldSource.Charge) / DisplacementMagnitude**6 * DisplacementVector)\n",
    "        ElectricFieldVector= CoulombConstant *ElectricFieldVector\n",
    "        return ElectricFieldVector #N/C or V/m\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ForceFieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def PlotField(self):\n",
    "        'This funtion plots the 2D electric vector field'\n",
    "        ObservationPosition= T.meshgrid(T.linspace(self.FieldLowBound, self.FieldHighBound, 40), \n",
    "                                        T.linspace(self.FieldLowBound, self.FieldHighBound, 40))\n",
    "        ObservationPosition= T.stack(ObservationPosition)\n",
    "        xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "        xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "        yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "        color_aara = T.sqrt(xd**2+ yd**2)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "        fig.colorbar(cp)\n",
    "        plt.rcParams['figure.dpi'] = 250\n",
    "        plt.show()\n",
    "\n",
    "@dataclass\n",
    "class ElectricField:\n",
    "    FieldSources: list[Particle]\n",
    "    FieldHighBound: float\n",
    "    FieldLowBound: float\n",
    "    def __call__(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        return self.ElectricFieldStrength(ObservationPosition)\n",
    "    @EnforceMethodTyping\n",
    "    def ElectricFieldStrength(self, ObservationPosition: T.Tensor)->T.Tensor:\n",
    "        'This function takes a list of sources and outputs the field strength experienced at any given point(s). This determines the physics of the field(an electric field in this case)'\n",
    "        CoulombConstant = 8.9875e9 #N*m^2/C^2\n",
    "        for FieldSource in self.FieldSources:\n",
    "            if type(FieldSource) != Particle:\n",
    "                raise TypeError(\"The input is not valid\")\n",
    "        assert type(ObservationPosition) == T.Tensor, \"Invalid Reference point data type\"\n",
    "        ElectricFieldVector = T.zeros_like(ObservationPosition)\n",
    "        for FieldSource in self.FieldSources:\n",
    "            PositionMatrices= T.stack([T.ones_like(ObservationPosition[0])* FieldSource.Position[0].item(), \n",
    "                                            T.ones_like(ObservationPosition[1])* FieldSource.Position[1].item()])\n",
    "            DisplacementVector = ObservationPosition - PositionMatrices\n",
    "            DisplacementMagnitude = T.sqrt(DisplacementVector[0]**2 +DisplacementVector[1]**2)\n",
    "            ElectricFieldVector += (CoulombConstant * FieldSource.Charge) / DisplacementMagnitude**3 * DisplacementVector\n",
    "        return ElectricFieldVector #N/C or V/m\n",
    "    @EnforceMethodTyping\n",
    "    def WorkDoneAgainstField(self, InitialPosition: T.Tensor, FinalPosition: T.Tensor, resolution: int= 5000)-> float:\n",
    "        '''This method determines the amount of work required to get one position to another in the field'''\n",
    "        XInterval= (FinalPosition[0] - InitialPosition[0]) / resolution\n",
    "        YInterval= (FinalPosition[1] - InitialPosition[1]) / resolution\n",
    "        XPositions = [InitialPosition[0] + i * XInterval for i in range(resolution + 1)]\n",
    "        YPositions = [InitialPosition[1] + i * YInterval for i in range(resolution + 1)]\n",
    "        WorkDone = 0\n",
    "        for i in range(resolution):\n",
    "            PositionFieldStrength = self.ElectricFieldStrength(T.Tensor([XPositions[i], YPositions[i]]))\n",
    "            WorkDone += - (PositionFieldStrength[0]*XInterval + PositionFieldStrength[1]*YInterval)\n",
    "        return WorkDone\n",
    "    @EnforceMethodTyping\n",
    "    def PlotField(self):\n",
    "        'This funtion plots the 2D electric vector field'\n",
    "        ObservationPosition= T.meshgrid(T.linspace(self.FieldLowBound, self.FieldHighBound, 50), \n",
    "                                        T.linspace(self.FieldLowBound, self.FieldHighBound, 50))\n",
    "        ObservationPosition= T.stack(ObservationPosition)\n",
    "        xd, yd = self.ElectricFieldStrength(ObservationPosition)\n",
    "        xd = xd / T.sqrt(xd**2 + yd**2)\n",
    "        yd = yd / T.sqrt(xd**2 + yd**2)\n",
    "        color_aara = T.sqrt(xd**2+ yd**2)\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "        cp = ax.quiver(ObservationPosition[0],ObservationPosition[1],xd,yd,color_aara)\n",
    "        fig.colorbar(cp)\n",
    "        plt.rcParams['figure.dpi'] = 250\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Field Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElectricField1= ElectricField(Source, 10.0, -10.0)\n",
    "# ElectricField1.PlotField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should obey newtons laws in Homogenous vector field \n",
    "@dataclass\n",
    "class Environment:\n",
    "    @dataclass\n",
    "    class State:\n",
    "        pass\n",
    "    InitialState: State \n",
    "    CurrentState: State \n",
    " \n",
    "    def __post_init__(self):\n",
    "        pass\n",
    " \n",
    "    @abstractmethod\n",
    "    def TransitionModel(self, State: State, Action)-> State:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def RewardModel(self, State: State, Action, NextState: State, TerminalSignal: bool)-> float:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def IsTerminalCondition(self, State: State)-> bool:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def StateTransition(self, State: State, Action)-> tuple[float, State, bool]:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def Run(self, RunDuration: float)-> list[State]:\n",
    "        NotImplementedError (\"Subclasses should implement this method\")\n",
    "        pass\n",
    "    \n",
    "@dataclass\n",
    "class ParticleInField(EnforceClassTyping):\n",
    "    '''This class represents the environment the agent will learn from. \n",
    "    \n",
    "    The UppperBoundX, LowerBoundX, UpperBoundY, and LowerBoundY determine the dimensions of the viable learning region of the environment.\n",
    "    The FieldType determines the physics/dynamics of the environment\n",
    "    The FieldSources shape the field '''\n",
    "    Field: ElectricField\n",
    "    ChargedParticle: Particle\n",
    "    Target: T.Tensor\n",
    "    DistanceWeight: float= 0.5\n",
    "    EnergyWeight: float= 0.5\n",
    "    TerminalSignalWeight: float= 0.5\n",
    "    @dataclass \n",
    "    class State(EnforceClassTyping):\n",
    "        '''This class represents the state of the Agent with its Position, Momentum and the Field Strength if experiences at its Position. \n",
    "        These are parameters the agent is able to observe, they uniquely define the state of the agent.'''\n",
    "        Position: T.Tensor # m\n",
    "        Momentum: T.Tensor #kg*m/s\n",
    "        Time: float # s\n",
    "\n",
    "        # def __add__(self, other):\n",
    "        #     Position = self.Position + other.Position\n",
    "        #     Momentum = self.Momentum + other.Momentum\n",
    "        #     Time = self.Time + other.Time\n",
    "        #     return self(Position, Momentum, Time)\n",
    "    InitialState: State = None\n",
    "    CurrentState: State = None\n",
    "    def __post_init__(self):\n",
    "        if self.InitialState is None:\n",
    "            self.InitialState= self.RandomState()\n",
    "        self.CurrentState= self.InitialState\n",
    "\n",
    "    # def StateDynamics(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0])):\n",
    "    #     PositionDynamics= State.Momentum/ self.ChargedParticle.Mass\n",
    "    #     MomentumDynamics= (self.ChargedParticle.Charge* self.Field(State.Position))+Action\n",
    "    #     TimeDynamics= 1.0\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def TransitionModel(self, State: State, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval:float= 1, Resolution: int=100)-> State:\n",
    "        '''This function determines how the state of the system changes after a given period given the agents state and parameters'''\n",
    "        CurrentMomentum= State.Momentum\n",
    "        CurrrentPosition= State.Position\n",
    "        TimeTaken= 0\n",
    "        for _ in range(Resolution):\n",
    "            CurrentMomentum = CurrentMomentum + ((self.ChargedParticle.Charge* self.Field(CurrrentPosition))+Action)*(TimeInterval/Resolution)\n",
    "            CurrrentPosition= CurrrentPosition+ (CurrentMomentum/ self.ChargedParticle.Mass)*(TimeInterval/Resolution)\n",
    "            TimeTaken+= (TimeInterval/Resolution)\n",
    "        CurrentTime= State.Time+ TimeTaken\n",
    "        return self.State(CurrrentPosition, CurrentMomentum, CurrentTime)\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def IsTerminalCondition(self, State: State)-> bool:\n",
    "        '''This method determines if a position is within the viable learning region of the environment'''\n",
    "        WithinXBound= self.Field.FieldLowBound <= State.Position[0] <= self.Field.FieldHighBound\n",
    "        WithinYBound= self.Field.FieldLowBound <= State.Position[1] <= self.Field.FieldHighBound\n",
    "        if WithinXBound and WithinYBound:\n",
    "            return False    \n",
    "        else:\n",
    "            return True\n",
    "    \n",
    "    def RewardModel(self, State: State, Action , NextState: State, TerminalSignal: bool)-> float:\n",
    "        '''This method determines how the agent is rewarded given a state transition. The reward determines the behaviour the agent should learn(i.e getting to the target and using the least amount of energy).'''\n",
    "        DistanceGainedFromTarget= T.norm(State.Position-self.Target)- T.norm(NextState.Position-self.Target) \n",
    "        EnergyConsumed= self.Field.WorkDoneAgainstField(State.Position, NextState.Position)\n",
    "        Cost= self.DistanceWeight* DistanceGainedFromTarget+ self.EnergyWeight* EnergyConsumed+ self.TerminalSignalWeight* TerminalSignal+ 1.0\n",
    "        return -Cost.item()\n",
    "        \n",
    "    def Step(self, State: State= CurrentState, Action: T.Tensor= T.tensor([0.0, 0.0]), TimeInterval: float= 1):\n",
    "        NextState= self.TransitionModel(State, Action, TimeInterval)\n",
    "        TerminalSignal= self.IsTerminalCondition(NextState) \n",
    "        Reward= self.RewardModel(State, Action, NextState, TerminalSignal)\n",
    "        return NextState, Reward, TerminalSignal\n",
    "    \n",
    "    @EnforceMethodTyping\n",
    "    def RandomState(self)->State:\n",
    "        '''This method generates a random state within the viable learning region'''\n",
    "        RandomPosition= T.Tensor([np.random.uniform(self.Field.FieldLowBound, self.Field.FieldHighBound), \n",
    "                                  np.random.uniform(self.Field.FieldLowBound, self.Field.FieldHighBound)])\n",
    "        RandomMomentum= T.zeros_like(RandomPosition)\n",
    "        return self.State(RandomPosition, RandomMomentum, 0.0)\n",
    "\n",
    "    def Run(self, RunDuration: float, Resolution: int=100):\n",
    "        Path= []\n",
    "        State= self.CurrentState\n",
    "        Time= 0\n",
    "        for _ in range(Resolution):\n",
    "            Path.append(State.Position)\n",
    "            State= self.TransitionModel(State, 1e-7* T.randn(2))\n",
    "            Time += (RunDuration/Resolution)\n",
    "        return Path\n",
    "    \n",
    "    def Lagrangian(self):\n",
    "        pass\n",
    "    \n",
    "    def PlotRun(self, RunDuration: float):\n",
    "        Path= self.Run(RunDuration)\n",
    "        Path= T.stack(Path)\n",
    "        Path= Path.transpose(dim0=0, dim1=1)\n",
    "        # print(Path)\n",
    "        t=  T.arange(0, RunDuration)\n",
    "        plt.plot(Path[0], Path[1])\n",
    "        plt.plot(Path[0][0], Path[1][0], 'ko')\n",
    "        plt.plot(Path[0][-1], Path[1][-1], 'r*')\n",
    "        plt.xlim(-10,10)\n",
    "        plt.ylim(-10,10)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    def Reset(self):\n",
    "        self.CurrentState= self.InitialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Environment1= ParticleInField(ElectricField1, Proton, T.tensor([0.0, 0.0]))\n",
    "Environment1.PlotRun(20)\n",
    "# -(Environment.ChargedParticle.Charge* Environment.Field(Environment.CurrentState.Position))\n",
    "# Environment.Step(Environment.CurrentState,T.tensor([1e-9, 1e-9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__() \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims+n_actions, fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = T.cat([state, action], dim=-1)\n",
    "        x = T.relu(self.bn1(self.fc1(x)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Critic Network Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N= CriticNetwork(0.02, 5, 10, 15, 2, name='tb')\n",
    "\n",
    "# bt= T.tensor([[-1.4355, -0.7806,  0.3042,  1.1601, -0.1184]])\n",
    "# at= T.tensor([[0.8233, 0.8126]])\n",
    "# for target_param in N.parameters():\n",
    "#     print(target_param) \n",
    "# bt, m(bt), N(bt)\n",
    "# N.load_checkpoint()\n",
    "# bt, at, N(bt, at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, state_dims, fc1_dims, fc2_dims, n_actions, name, chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,name+'_ddpg')\n",
    "\n",
    "        self.fc1 = T.nn.utils.parametrizations.weight_norm(nn.Linear(state_dims , fc1_dims)) \n",
    "        self.bn1 = nn.LayerNorm(fc1_dims)\n",
    "        self.fc2 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc1_dims, fc2_dims))\n",
    "        self.bn2 = nn.LayerNorm(fc2_dims)\n",
    "        self.fc3 = T.nn.utils.parametrizations.weight_norm(nn.Linear(fc2_dims, n_actions))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = T.relu(self.bn1(self.fc1(state)))\n",
    "        x = T.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actor Network Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nt= ActorNetwork(0.02, 5, 10, 15, 2, name='btt')\n",
    "\n",
    "# btt= T.tensor([[-0.6552,  0.0852,  2.0087, -0.6352,  0.4445]])\n",
    "# att= T.randn(1, 2)\n",
    "\n",
    "# Nt.load_checkpoint()\n",
    "# btt, Nt(btt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Agent1:\n",
    "    AgentEnvironment: Environment\n",
    "\n",
    "    def __post_init__(self):\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def Act(self, Observation: T.Tensor)-> T.Tensor:\n",
    "        NotImplementedError (\"Subclasses must implement the `Act` method\")\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def Observe(self)-> T.Tensor:\n",
    "        NotImplementedError (\"Subclasses must implement the `Act` method\")\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def Learn(self):\n",
    "        'Improves  the agent by updating its models'\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def LearningAlgorithm(self):\n",
    "        pass\n",
    "    \n",
    "class Agent(object):\n",
    "    def __init__(self, lr_Actor, lr_Critic, ObservationDimensions, SoftUpdateRate, LearningEnvironment,\n",
    "                 gamma=0.99, n_actions=2, max_size=1000, layer1_size=20, layer2_size=15, batch_size=16, ControlInterval= 0.5):\n",
    "        self.Actor = ActorNetwork(lr_Actor, ObservationDimensions, layer1_size, layer2_size, n_actions=n_actions, name='Actor')\n",
    "        self.Critic = CriticNetwork(lr_Critic, ObservationDimensions, layer1_size, layer2_size, n_actions=n_actions, name='Critic')\n",
    "        self.TargetActor = ActorNetwork(lr_Actor, ObservationDimensions, layer1_size, layer2_size, n_actions=n_actions, name='TargetActor')\n",
    "        self.TargetCritic = CriticNetwork(lr_Critic, ObservationDimensions, layer1_size, layer2_size, n_actions=n_actions, name='TargetCritic')\n",
    "        for target_param, param in zip(self.TargetActor.parameters(), self.Actor.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        for target_param, param in zip(self.TargetCritic.parameters(), self.Critic.parameters()):\n",
    "            target_param.data.copy_(param.data) \n",
    "        self.memory = deque(maxlen=max_size)\n",
    "        self.gamma = gamma\n",
    "        self.SoftUpdateRate = SoftUpdateRate\n",
    "        self.batch_size = batch_size\n",
    "        self.LearningEnvironment: ParticleInField= LearningEnvironment\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "        self.ControlInterval= ControlInterval# Acttion duration= conrol interval\n",
    "        self.update_network_parameters(SoftUpdateRate=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.Actor.eval()\n",
    "        observation = T.tensor(observation, dtype=T.float).to(self.Actor.device)\n",
    "        mu = self.Actor.forward(observation).to(self.Actor.device)\n",
    "        mu_prime = 1e-7* (mu + T.tensor(self.noise(), dtype=T.float).to(self.Actor.device))\n",
    "        self.Actor.train()\n",
    "        return mu_prime.cpu().detach()\n",
    "\n",
    "    def Observe(self, State= None):\n",
    "        if State is None:\n",
    "          State= self.LearningEnvironment.CurrentState   \n",
    "        if type(State) == tuple:\n",
    "            Observation= [0]*len(State)\n",
    "            for i in range(len(State)):\n",
    "              Observation[i]= self.Observe(State[i])\n",
    "            Observation= T.stack(Observation)\n",
    "        else:\n",
    "            Observation= T.cat([State.Position,\n",
    "                                State.Momentum])\n",
    "        return Observation\n",
    "    \n",
    "    def DDPGAlgorithm(self):\n",
    "        score_history = []\n",
    "        for i in range(10):\n",
    "            self.LearningEnvironment.CurrentState = self.LearningEnvironment.InitialState\n",
    "            IsDone = False\n",
    "            score = 0\n",
    "            for _ in range(50):\n",
    "                Observation= self.Observe()\n",
    "                Action = self.choose_action(Observation) \n",
    "                new_state, Reward, IsDone= self.LearningEnvironment.Step(self.LearningEnvironment.CurrentState, Action, self.ControlInterval) \n",
    "                self.memory.append((self.LearningEnvironment.CurrentState, Action, new_state, Reward, int(IsDone)))\n",
    "                self.learn()\n",
    "                score += Reward\n",
    "                # print(Reward)\n",
    "                self.LearningEnvironment.CurrentState = new_state\n",
    "            score_history.append(score)\n",
    "        plt.plot(score_history)\n",
    "        return score_history\n",
    "        \n",
    "    def learn(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, next_states, rewards, dones = zip(*batch)\n",
    "\n",
    "        state = self.Observe(states).to(self.Critic.device)\n",
    "        action = T.stack(actions).to(self.Critic.device)\n",
    "        reward = T.tensor(rewards, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "        new_state = self.Observe(next_states).to(self.Critic.device)\n",
    "        done = T.tensor(dones, dtype=T.float).unsqueeze(1).to(self.Critic.device)\n",
    "        \n",
    "        self.TargetActor.eval()\n",
    "        self.TargetCritic.eval()\n",
    "        self.Critic.eval()\n",
    "        \n",
    "        target_actions = self.TargetActor.forward(new_state)\n",
    "        Critic_value_ = self.TargetCritic.forward(new_state, target_actions) \n",
    "        q_expected = self.Critic.forward(state, action)\n",
    "        q_targets = reward + self.gamma * Critic_value_ * (1 - done)\n",
    "\n",
    "        Critic_loss = nn.MSELoss()(q_expected, q_targets.detach())\n",
    "        self.Critic.train()\n",
    "        self.Critic.optimizer.zero_grad()\n",
    "        Critic_loss.backward()\n",
    "        self.Critic.optimizer.step()\n",
    "\n",
    "        self.Actor.eval()\n",
    "        self.Critic.eval()\n",
    "\n",
    "        mu = self.Actor.forward(state)\n",
    "        Actor_loss = -self.Critic.forward(state, mu)\n",
    "\n",
    "        Actor_loss = T.mean(Actor_loss)\n",
    "        self.Actor.train()\n",
    "        self.Actor.optimizer.zero_grad()\n",
    "        Actor_loss.backward()\n",
    "        self.Actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def Run(self, RunDuration: float, Resolution: int=100):\n",
    "        Path= []\n",
    "        State= self.LearningEnvironment.InitialState\n",
    "        Time= 0\n",
    "        for _ in range(Resolution):\n",
    "            Path.append(State.Position)\n",
    "            action= self.choose_action(self.Observe(State))\n",
    "            State= self.LearningEnvironment.TransitionModel(State, action)\n",
    "            Time += (RunDuration/Resolution)\n",
    "        return Path\n",
    "    def PlotRun(self, RunDuration: float):\n",
    "        Path= self.Run(RunDuration)\n",
    "        Path= T.stack(Path)\n",
    "        Path= Path.transpose(dim0=0, dim1=1)\n",
    "        # print(Path)\n",
    "        t=  T.arange(0, RunDuration)\n",
    "        plt.plot(Path[0], Path[1])\n",
    "        plt.plot(Path[0][0], Path[1][0], 'ko')\n",
    "        plt.plot(Path[0][-1], Path[1][-1], 'r*')\n",
    "        plt.xlim(-10,10)\n",
    "        plt.ylim(-10,10)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    def update_network_parameters(self, SoftUpdateRate=None):\n",
    "        if SoftUpdateRate is None:\n",
    "            SoftUpdateRate = self.SoftUpdateRate\n",
    "\n",
    "        Critic_state_dict = dict(self.Critic.named_parameters())\n",
    "        Actor_state_dict = dict(self.Actor.named_parameters())\n",
    "        TargetCritic_dict = dict(self.TargetCritic.named_parameters())\n",
    "        TargetActor_dict = dict(self.TargetActor.named_parameters())\n",
    "\n",
    "        for name in Critic_state_dict:\n",
    "            Critic_state_dict[name] = SoftUpdateRate*Critic_state_dict[name].clone() + (1-SoftUpdateRate)*TargetCritic_dict[name].clone()\n",
    "        self.TargetCritic.load_state_dict(Critic_state_dict)\n",
    "\n",
    "        for name in Actor_state_dict:\n",
    "            Actor_state_dict[name] = SoftUpdateRate*Actor_state_dict[name].clone() + (1-SoftUpdateRate)*TargetActor_dict[name].clone()\n",
    "        self.TargetActor.load_state_dict(Actor_state_dict)\n",
    "\n",
    "        \"\"\"\n",
    "        #Verify that the copy assignment worked correctly\n",
    "        TargetActor_params = self.TargetActor.named_parameters()\n",
    "        TargetCritic_params = self.TargetCritic.named_parameters()\n",
    "\n",
    "        Critic_state_dict = dict(TargetCritic_params)\n",
    "        Actor_state_dict = dict(TargetActor_params)\n",
    "        print('\\nActor Networks', tau)\n",
    "        for name, param in self.Actor.named_parameters():\n",
    "            print(name, T.equal(param, Actor_state_dict[name]))\n",
    "        print('\\nCritic Networks', tau)\n",
    "        for name, param in self.Critic.named_parameters():\n",
    "            print(name, T.equal(param, Critic_state_dict[name]))\n",
    "        input()\n",
    "        \"\"\"\n",
    "    \n",
    "    def save_models(self):\n",
    "        self.Actor.save_checkpoint()\n",
    "        self.TargetActor.save_checkpoint()\n",
    "        self.Critic.save_checkpoint()\n",
    "        self.TargetCritic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.Actor.load_checkpoint()\n",
    "        self.TargetActor.load_checkpoint()\n",
    "        self.Critic.load_checkpoint()\n",
    "        self.TargetCritic.load_checkpoint()\n",
    " \n",
    "agent = Agent(0.0025, 0.0025, 4, 0.001, Environment1)\n",
    "agent.DDPGAlgorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.PlotRun(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
